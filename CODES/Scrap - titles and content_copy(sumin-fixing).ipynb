{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688e2d1c",
   "metadata": {},
   "source": [
    "# Scrap algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8b434",
   "metadata": {},
   "source": [
    "## 1. Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e94e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the contents in the url\n",
    "#### input: url\n",
    "#### output: content (type: str)\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "###### The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 3)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50f83f",
   "metadata": {},
   "source": [
    "## 2. Auto scrap of the top n search results from different search engines\n",
    "What we scrap:\n",
    "* Title\n",
    "* Link\n",
    "* Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "315fd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n can be changed to test small examples\n",
    "n = 20\n",
    "# For some search engines, it gives less than 20 articles in one page. In that case, it only scraps the first page for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22335323",
   "metadata": {},
   "source": [
    "###  2-1. Google News (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "697b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from google news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "    date_elements = soup.find_all('time', class_='hvbAAd')\n",
    "\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(20):\n",
    "        title = search_results[i].find('a', class_ = 'JtKRv').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i]['datetime'][:10]    \n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3715f7c",
   "metadata": {},
   "source": [
    "#### 2-1-1. Testing Google scrap algorithm\n",
    "#### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f10f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d4f4e",
   "metadata": {},
   "source": [
    "#### Testing with queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "190c2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "1. Freaky Oregon Fish Is Bigger Than Shaquille O'Neal - Videos from The Weather Channel\n",
      "https://news.google.com/articles/CBMiZWh0dHBzOi8vd2VhdGhlci5jb20vbmF0dXJlL3dpbGQtYW5pbWFscy92aWRlby9yYXJlbHktaWRlbnRpZmllZC1uZXdseS1kaXNjb3ZlcmVkLWZpc2gtZm91bmQtaW4tb3JlZ29u0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-11\n",
      " June 13, 2024 This type of sunfish was only recently discovered to even exist. Now, scientists say one of the largest examples of the rarely identified species has washed up on an Oregon beach. Now Playing\n",
      "\n",
      "2. Rare 7-foot fish washed ashore on Oregon's coast garners worldwide attention\n",
      "https://news.google.com/articles/CBMiXWh0dHBzOi8vYWJjbmV3cy5nby5jb20vV2VpcmQvd2lyZVN0b3J5L3JhcmUtNy1mb290LWZpc2gtd2FzaGVkLWFzaG9yZS1vcmVnb25zLWNvYXN0LTExMDkzODgyMdIBYWh0dHBzOi8vYWJjbmV3cy5nby5jb20vYW1wL1dlaXJkL3dpcmVTdG9yeS9yYXJlLTctZm9vdC1maXNoLXdhc2hlZC1hc2hvcmUtb3JlZ29ucy1jb2FzdC0xMTA5Mzg4MjE?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-07\n",
      " A massive rare fish thought to only live in temperate waters in the southern hemisphere has washed up on Oregon's northern coast GEARHART, Ore. -- A massive rare fish thought to only live in temperate waters in the southern hemisphere has washed up on Oregon's northern coast, drawing crowds of curious onlookers intrigued by the unusual sight. The 7.3-foot (2.2 meter) hoodwinker sunfish first appeared on the beach in Gearhart on Monday, the Seaside Aquarium said in a media release. It was still on the beach on Friday and may remain there for weeks, the aquarium said, as it is difficult for scavengers to puncture its tough skin. \n",
      "\n",
      "3. A fish washed ashore in Oregon. It was a recently discovered species.\n",
      "https://news.google.com/articles/CBMibmh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS9uYXRpb24vMjAyNC8wNi8xMC9maXNoLXdhc2hlZC1hc2hvcmUtb3JlZ29uLWl0LXdhcy1yZWNlbnRseS1kaXNjb3ZlcmVkLXNwZWNpZXMv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-11\n",
      " The rarely seen species of sunfish is called the hoodwinker sunfish, alluding to its ability to fool people. Seaside Aquarium thought a common sunfish had washed up on the Oregon coast last week and posted photos of its 2,000-pound carcass on social media. But a New Zealand-based marine biologist who saw the photos suspected they had stumbled upon something far more rare — a hoodwinker sunfish. The aquarium’s mistake was understandable. It’s one people and scientists made for more than a century, until it was identified as a different species. In fact, it’s why marine biologist Marianne Nyegaard had given it a name alluding to its ability to fool people. Last week, she came across the Seaside, Ore.-based aquarium’s photos and contacted employees there to let them know she believed they had found one of the tricksters.\n",
      "\n",
      "4. See the Rare, 2000-Pound Hoodwinker Sunfish That Washed Ashore in Oregon\n",
      "https://news.google.com/articles/CBMifGh0dHBzOi8vd3d3LnNtaXRoc29uaWFubWFnLmNvbS9zbWFydC1uZXdzL3NlZS10aGUtcmFyZS0yMDAwLXBvdW5kLWhvb2R3aW5rZXItc3VuZmlzaC10aGF0LXdhc2hlZC1hc2hvcmUtaW4tb3JlZ29uLTE4MDk4NDUzMy_SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-12\n",
      " Sections The species was only described in 2017 after “hiding in plain sight” for nearly three centuries \n",
      "Sarah Kuta\n",
      "\n",
      "\n",
      "5. How to catch a $1 million fish — or maybe just watch. Your guide to Big Rock tourney.\n",
      "https://news.google.com/articles/CBMiSmh0dHBzOi8vY2Euc3BvcnRzLnlhaG9vLmNvbS9uZXdzL2NhdGNoLTEtbWlsbGlvbi1maXNoLW1heWJlLTE0MjgwMTE5MC5odG1s0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      " For most of the Triangle, a good day fishing means pulling a 5-pound bass out of Jordan Lake or a fat catfish from the Neuse River. But three hours to the southeast, anglers at the Big Rock Blue Marlin Tournament are vying for billfish so big that anything under 400 pounds goes back in the ocean. The 66th annual tournament draws thousands of gawkers and generates a year’s worth of fish stories, but remains unknown to much of the Triangle’s freshly arrived population.\n",
      "\n",
      "6. No licence required for Dads to fish this weekend in Ontario\n",
      "https://news.google.com/articles/CBMiZ2h0dHBzOi8vd3d3LnN1ZGJ1cnkuY29tL2xvY2FsLW5ld3Mvbm8tbGljZW5jZS1yZXF1aXJlZC1mb3ItZGFkcy10by1maXNoLXRoaXMtd2Vla2VuZC1pbi1vbnRhcmlvLTkwNjkyMjbSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-12\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "7. Free fishing program expanding to Georgina, Mississauga | News\n",
      "https://news.google.com/articles/CBMiqAFodHRwczovL3d3dy50b3JvbnRvLmNvbS9uZXdzL2hvb2tlZC1vbi1maXNoaW5nLXByb3ZpbmNpYWwtZnJlZS1sZWFybi10by1maXNoLXByb2dyYW0tZXhwYW5kaW5nLXRvLWdlb3JnaW5hLW1pc3Npc3NhdWdhL2FydGljbGVfMDhkYTQ5ZWYtNWQ0Zi01MmYzLTg2MzMtOWU2M2MzNWQ4OTgxLmh0bWzSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "  Reporter The Ontario government is making it easier and more accessible for people to learn how to fish.\n",
      "\n",
      "8. Victoria, P.E.I., fish farm scales back expansion plans after residents voice concerns\n",
      "https://news.google.com/articles/CBMiYWh0dHBzOi8vd3d3LmNiYy5jYS9uZXdzL2NhbmFkYS9wcmluY2UtZWR3YXJkLWlzbGFuZC9wZWktdmljdG9yaWEtZmlzaC1mYXJtLWFtYXItc2VhZm9vZC0xLjcyMzEyMjTSASBodHRwczovL3d3dy5jYmMuY2EvYW1wLzEuNzIzMTIyNA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-11\n",
      " After consulting with residents of the seaside community of Victoria, P.E.I., aquaculture company Amar Seafood has reduced the proposed expansion of its fish farm operation in the rural municipality, for now. Amar's Prince Edward Island CEO Scott Travers said the farm, which hatches and grows wolffish and halibut, will need to produce an additional 600 tonnes of fish to be economically viable. The company, which is headquartered in Norway, had initially submitted a rezoning request that would see the plant grow to three times its current size, and had purchased 50 acres of land — 40 of which Travers said is usable — for the expansion.\n",
      "\n",
      "9. Glenavy River: Hundreds of fish threatened by pollution\n",
      "https://news.google.com/articles/CBMiLmh0dHBzOi8vd3d3LmJiYy5jb20vbmV3cy9hcnRpY2xlcy9jNG5uODVqZHgxNW_SATJodHRwczovL3d3dy5iYmMuY29tL25ld3MvYXJ0aWNsZXMvYzRubjg1amR4MTVvLmFtcA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      " Anglers are concerned that hundreds of fish could have been killed by pollution in the Glenavy River, County Antrim.  The Northern Ireland Environment Agency (NIEA) received a report on Wednesday that several miles of the river - part of the Lough Neagh catchment - was polluted and that there were fish in distress and struggling to breathe.  About 30 dead fish have been identified, but there are concerns that the number could reach 200 or 300 at that particular stretch of river. \n",
      "\n",
      "10. Province Expands Learn To Fish Program\n",
      "https://news.google.com/articles/CBMiTWh0dHBzOi8vd3d3Lmthd2FydGhhNDExLmNhLzIwMjQvMDYvMTMvcHJvdmluY2UtZXhwYW5kcy1sZWFybi10by1maXNoLXByb2dyYW0v0gFTaHR0cHM6Ly93d3cua2F3YXJ0aGE0MTEuY2EvMjAyNC8wNi8xMy9wcm92aW5jZS1leHBhbmRzLWxlYXJuLXRvLWZpc2gtcHJvZ3JhbS8_YW1wPTE?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      " ONTARIO-The Ontario government is expanding its Learn to Fish program, increasing the number of locations and mobile sessions offered and providing 2,700 new spots. “Fishing is a wonderful activity and long-standing tradition in Ontario and I’m glad to be offering more opportunities – many of them in urban areas – for people to be able to take part in the Learn to Fish program,” said Graydon Smith, Minister of Natural Resources. “Expansion of this program creates more opportunities for people to get out and explore Ontario’s many lakes and rivers and maybe even find a new hobby.” Beginning this year, the Learn to Fish program is expanding to include Lakefront Promenade and Lake Aquitaine in Mississauga and Sibbald Point Provincial Park in Georgina.\n",
      "\n",
      "11. Ontario getting more people “hooked” on fishing\n",
      "https://news.google.com/articles/CBMiL2h0dHBzOi8vd3d3LmR1cmhhbXJhZGlvbmV3cy5jb20vYXJjaGl2ZXMvMTg1MTQx0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "12. Free fishing program expanding to Georgina, Mississauga\n",
      "https://news.google.com/articles/CBMiqwFodHRwczovL3d3dy55b3JrcmVnaW9uLmNvbS9uZXdzL2hvb2tlZC1vbi1maXNoaW5nLXByb3ZpbmNpYWwtZnJlZS1sZWFybi10by1maXNoLXByb2dyYW0tZXhwYW5kaW5nLXRvLWdlb3JnaW5hLW1pc3Npc3NhdWdhL2FydGljbGVfYjAwYjg0MzQtNDQ4NC01NjBiLWE1MDQtZGY1NDAzNjVhYmVjLmh0bWzSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Newsletters Newsletters \n",
      "\n",
      "13. Sault Tribe holds first ever open house at Walleye Fish Hatchery\n",
      "https://news.google.com/articles/CBMiZ2h0dHBzOi8vd3d3LnNvb2xlYWRlci5jb20vYm9sZC9zYXVsdC10cmliZS1ob2xkcy1maXJzdC1ldmVyLW9wZW4taG91c2UtYXQtd2FsbGV5ZS1maXNoLWhhdGNoZXJ5LTkwNzg3MznSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      " Sign In Register For the first time in its history, the Sault Ste. Marie Tribe of Chippewa Indians opened its walleye hatchery to the public in an open house Wednesday evening.\n",
      "\n",
      "14. Free family fishing this weekend for Father's Day in Ontario\n",
      "https://news.google.com/articles/CBMiigFodHRwczovL3d3dy50b3JvbnRvLmNvbS9uZXdzL2ZyZWUtZmFtaWx5LWZpc2hpbmctdGhpcy13ZWVrZW5kLWZvci1mYXRoZXJzLWRheS1pbi1vbnRhcmlvL2FydGljbGVfNDUyMWJjNjYtNDdhNy01YTkyLTljZDItZGFmNGUzZGM1NmJlLmh0bWzSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "  This Father's Day weekend, the province is hosting a free family fishing event June 15 and 16. If you don't have big plans this weekend, take time to drop a line, with another free family fishing weekend June 15 and 16 in Ontario.\n",
      "\n",
      "15. Fish for Free this Weekend\n",
      "https://news.google.com/articles/CBMiP2h0dHBzOi8vd3d3Lm1pbHRvbm5vdy5jYS8yMDI0LzA2LzEzL2Zpc2gtZm9yLWZyZWUtdGhpcy13ZWVrZW5kL9IBAA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "16. Number of dead fish found in Cardigan River now tops 300\n",
      "https://news.google.com/articles/CBMiVGh0dHBzOi8vd3d3LmNiYy5jYS9uZXdzL2NhbmFkYS9wcmluY2UtZWR3YXJkLWlzbGFuZC9wZWktY2FyZGlnYW4tZmlzaC1raWxsLTEuNzIzMDA4NNIBIGh0dHBzOi8vd3d3LmNiYy5jYS9hbXAvMS43MjMwMDg0?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-10\n",
      " After searching 800 metres of river over the past few days, provincial officials say the total number of dead fish in P.E.I.'s Cardigan River now sits at over 300. The cause of the fish kill in the eastern Prince Edward Island river is still uncertain, the province said. According to a statement from the Department of Environment, Energy and Climate Action, the dead species include brook trout, rainbow trout, juvenile salmon and stickleback.\n",
      "\n",
      "17. Federal court rejects review bid for renewing licences for B.C. fish farms\n",
      "https://news.google.com/articles/CBMiWmh0dHBzOi8vZ2xvYmFsbmV3cy5jYS9uZXdzLzEwNTU0NjYzL2ZlZGVyYWwtY291cnQtcmVqZWN0cy1yZW5ld2luZy1saWNlbmNlcy1iYy1maXNoLWZhcm1zL9IBXmh0dHBzOi8vZ2xvYmFsbmV3cy5jYS9uZXdzLzEwNTU0NjYzL2ZlZGVyYWwtY291cnQtcmVqZWN0cy1yZW5ld2luZy1saWNlbmNlcy1iYy1maXNoLWZhcm1zL2FtcC8?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-08\n",
      " Instructions: Want to discuss? Please read our Commenting Policy first. \n",
      "\t\t\t\tIf you get Global News from Instagram or Facebook - that will be changing. Find out how you can still connect with us.\t\t\t\n",
      "\n",
      "18. Paleontologists identify a new fossil fish genus\n",
      "https://news.google.com/articles/CBMiRGh0dHBzOi8vcGh5cy5vcmcvbmV3cy8yMDI0LTA2LXBhbGVvbnRvbG9naXN0cy1mb3NzaWwtZmlzaC1nZW51cy5odG1s0gFDaHR0cHM6Ly9waHlzLm9yZy9uZXdzLzIwMjQtMDYtcGFsZW9udG9sb2dpc3RzLWZvc3NpbC1maXNoLWdlbnVzLmFtcA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "Failed to retrieve content. Status code: 400\n",
      "None\n",
      "\n",
      "19. Father's Day-weekend fishing derby returns to Cultus Lake\n",
      "https://news.google.com/articles/CBMiXWh0dHA6Ly93d3cuYWJieW5ld3MuY29tL2hvbWUvZmF0aGVycy1kYXktd2Vla2VuZC1maXNoaW5nLWRlcmJ5LXJldHVybnMtdG8tY3VsdHVzLWxha2UtNzM4NDEwNNIBAA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      "  Sign In  Subscribe Now Hundreds of anglers will once again line the shores of Cultus Lake to kickoff Father's Day weekend. \n",
      "\n",
      "20. How AI Anchors The Future Of Fishing & Charts The Course To Sustainability\n",
      "https://news.google.com/articles/CBMifWh0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvbmVpbHNhaG90YS8yMDI0LzA2LzEzL2hvdy1haS1hbmNob3JzLXRoZS1mdXR1cmUtb2YtZmlzaGluZy0tY2hhcnRzLXRoZS1jb3Vyc2UtdG8tc3VzdGFpbmFiaWxpdHkv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "2024-06-13\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = \"https://news.google.com\" + result['link'][1:]\n",
    "    print(link)\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775bf9f",
   "metadata": {},
   "source": [
    "### 2-2. Yahoo News (working)\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f86e8b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "1. Watch: ‘Jubilee’ Draws Millions of Fish into Gulf Coast Shallows, Where They’re Ripe for Gigging\n",
      "https://www.yahoo.com/news/watch-jubilee-draws-millions-fish-165758884.html?fr=sycsrp_catchall\n",
      "2 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "2. 21-year-old Georgia woman breaks decades-old fishing record\n",
      "https://sports.yahoo.com/21-old-georgia-woman-breaks-184022927.html?fr=sycsrp_catchall\n",
      "19 minutes ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "3. Magnet fishing couple finds safe containing $100,000 in New York park\n",
      "https://www.yahoo.com/news/magnet-fishing-couple-finds-safe-175003779.html?fr=sycsrp_catchall\n",
      "1 hour ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "4. Ohioans can fish for free across the state this weekend\n",
      "https://www.10tv.com/article/news/local/ohio/ohio-free-fishing-weekend/530-e64ad69f-4740-4abe-ac9c-da9421cafd92\n",
      "2 days ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "5. Busy day for Butler County deputy who fished truck out of pond, responded to multi-vehicle crash\n",
      "https://www.yahoo.com/news/busy-day-butler-county-deputy-184100888.html?fr=sycsrp_catchall\n",
      "18 minutes ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "6. Commercial lobster sector concerned about out-of-season fishing in St. Marys Bay\n",
      "https://www.yahoo.com/news/commercial-lobster-sector-concerned-season-153956695.html?fr=sycsrp_catchall\n",
      "3 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "7. The tinned fish trend is finding its way onto Connecticut menus: 'Not your grandmother's tinned...\n",
      "https://www.newstimes.com/food/article/tinned-fish-ct-restaurants-sardines-seafood-menus-19481399.php\n",
      "4 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "8. How to catch a $1 million fish — or maybe just watch. Your guide to Big Rock tourney.\n",
      "https://www.aol.com/catch-1-million-fish-maybe-142801441.html\n",
      "5 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "9. First fishing trip a real winner | Northwest Arkansas Democrat-Gazette\n",
      "https://www.nwaonline.com/news/2024/jun/13/first-fishing-trip-a-real-winner/\n",
      "12 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n",
      "10. Glenavy: Fish die after river fills with 'brown effluent'\n",
      "https://www.bbc.com/news/videos/ce77j5w163do\n",
      "6 hours ago\n",
      " The future of fishing is more than robot fisher-people and smart refineries. In the vast, blue expanse of our planet's oceans, a revolution is quietly unfolding, poised to redefine the ancient practice of fishing for the modern era. This revolution, powered by artificial intelligence (AI), is not just transforming how we harvest the seas; it's ensuring we do so sustainably, preserving our marine ecosystems for generations to come. As we delve into this fascinating journey, we uncover the innovative ways in which AI is becoming the cornerstone of sustainable fishing, offering a beacon of hope for the future of our oceans. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Yahoo news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form\n",
    "########################################################\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "    date_elements = soup.find_all('span', class_='fc-2nd s-time mr-8')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(len(search_results)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840a379",
   "metadata": {},
   "source": [
    "### 2-3. Microsoft Bing News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff75838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Microsoft Bing news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form like Google\n",
    "########################################################\n",
    "\n",
    "def scrape_bing_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://www.bing.com/news/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='news-card')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(len(search_results)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_bing_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39d97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0001364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbfc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93ee6f70",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa51c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clotilde's function for relevance evaluation\n",
    "# This function searches for the number of keywords in the given text (var: results)\n",
    "\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b9bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383998d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e77293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6708322e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a393e2c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad0434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce1786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4a6a1b",
   "metadata": {},
   "source": [
    "### From the original file. ---- no need to read below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc9c7e",
   "metadata": {},
   "source": [
    "### From GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c163870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['ocean', 'sea', 'crime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba9e92",
   "metadata": {},
   "source": [
    "### Modified (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43014e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: vessel fish crime\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_to_word(soup):\n",
    "    content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "#    print(content)\n",
    "    if content is not []:\n",
    "        for paragraph in content:\n",
    "            print(paragraph)\n",
    "            text = paragraph.get_text(separator='\\n')\n",
    "            print(text)\n",
    "            text = clean_text(text)\n",
    "            #text_word = text.split()\n",
    "            return text.split()    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://news.google.com/search?q={query}&hl=en-CA&gl=CA&ceid=CA%3Aen\"\n",
    "    \n",
    "#    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "\n",
    "    \n",
    "    \n",
    "def scrape_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        word_text = text_to_word(soup)\n",
    "        content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "        content = soup.get_text()\n",
    "        return word_text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results[:5], start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print(\"   Content:\")\n",
    "        content = scrape_content(result['link'])\n",
    "        if content:\n",
    "            print(content[:500])  # Print the first 500 characters of the content\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "text = 'Hi my name is: sumin\", this-is to test text cleaning!'\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14427bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea002656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "https://news.google.com/search?q=fish\n",
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:10]:  # Scraping top 10 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815733cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
