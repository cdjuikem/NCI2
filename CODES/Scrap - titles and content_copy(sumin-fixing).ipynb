{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07b0004",
   "metadata": {},
   "source": [
    "Thoughts on evaluating the search engine results:\n",
    "\n",
    "* I think we need some data to train our algorithm, especially to identify keywords and features of relevant/irrelevant articles. We can easily collect information from news search! (or Sogol and Sana may be able to provide us some information)\n",
    "\n",
    "### Variables (to determine the best/worst search engine):\n",
    "1. The number of relevant articles among top 20 results\n",
    "2. how much specific information the relevant articles contain (general info only or specific info included)\n",
    "3. dates of the articles (how old the articles are)\n",
    "4. \n",
    "\n",
    "\n",
    "### Techniques for preliminary screen (removing irrelevent articles):\n",
    "1. keyword evaluation\n",
    "    * Frequency of the keywords OR how many keywords included (Clotilde's function)\n",
    "    * Naive Bayes model (similar to spam email detection, we can detect irrelevant articles) - but this needs the keyword statistics (probability) information / training data\n",
    "    \n",
    "2. Clustering:\n",
    "    * Cluster the articles with similarity and sort out irrelevant articles - also need some training data\n",
    "    * Non-numerical data need to be carefully pre-processed to obtain the clusters we want. For example, usually the words should be mapped to numerical values (maybe there's some python library does the job?)\n",
    "\n",
    "\n",
    "### Next thing to think about / questions:\n",
    "1. How to screen 'biased' information\n",
    "    * Penalize certain keywords such as politics, (we need to consider different things)\n",
    "2. Specific information (Vessel, captain's name, where it happend, what happened)\n",
    "    * How to identify them in the articles\n",
    "    * How to measure/compare the amount of information in an article numerically\n",
    "3. About regional biases, are we actually focus on Vancouver/North West America? Or anywhere in the world?\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03d5cf",
   "metadata": {},
   "source": [
    "### Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e0019eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "# The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 3)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec703152",
   "metadata": {},
   "source": [
    "###  Titles and links of the top 20 search results from Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ad922a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:2]:  # Scraping top 20 results\n",
    "        title = result.find('a', class_ = 'JtKRv').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119cc8ce",
   "metadata": {},
   "source": [
    "### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44e709f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eef7542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish crime\n",
      "1. UPDATE: Arrest made in theft of tropical fish and cash\n",
      "https://news.google.com/articles/CBMiWGh0dHBzOi8vd3d3Lm15YmFuY3JvZnRub3cuY29tLzYwODI5L25ld3MvYXJyZXN0LW1hZGUtaW4tdGhlZnQtb2YtdHJvcGljYWwtZmlzaC1hbmQtY2FzaC_SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "content: \n",
      " Upper Ottawa Valley OPP say that with public support and the help of the Community Street Crime Unit, they apprehended and arrested a male from Whitewater region on April 19th.   Police say they located the tropical fish which were then returned to their owner.  A Beachburg resident is facing Criminal Code (CC) charges, including theft under $5000 and possession of property obtained by crime.\n",
      "\n",
      "2. Stolen tropical fish returned to Ottawa Valley restaurant\n",
      "https://news.google.com/articles/CBMiTWh0dHBzOi8vY2EubmV3cy55YWhvby5jb20vc3RvbGVuLXRyb3BpY2FsLWZpc2gtcmV0dXJuZWQtb3R0YXdhLTIwNDY1NjUzNC5odG1s0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "content: \n",
      " Ontario Provincial Police (OPP) have found and returned tropical fish stolen from an Ottawa Valley restaurant last week. The fish were taken from the Beachburg Restaurant in an overnight break-in on April 11, OPP said. An undisclosed amount of cash was also stolen. The owner of the restaurant told CBC after the theft that she wasn't sure how many fish were taken from the restaurant's aquarium, but that there were \"a lot of them\" including clownfish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = \"https://news.google.com\" + result['link'][1:]\n",
    "    print(link)\n",
    "    print(\"content: \")\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cab07",
   "metadata": {},
   "source": [
    "### Scraping titles and links of the top 20 search results from Yahoo News\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1dd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:20]:  # Scraping top 20 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cff18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f49aaa7",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clotilde's function for relevance evaluation\n",
    "# This function searches for the number of keywords in the given text (var: results)\n",
    "\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2f0d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1564019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8213c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617e4977",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba816eee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb46a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4e642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c53f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba0d2ca",
   "metadata": {},
   "source": [
    "### From GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['ocean', 'sea', 'crime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba9e92",
   "metadata": {},
   "source": [
    "### Modified (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43014e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: vessel fish crime\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_to_word(soup):\n",
    "    content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "#    print(content)\n",
    "    if content is not []:\n",
    "        for paragraph in content:\n",
    "            print(paragraph)\n",
    "            text = paragraph.get_text(separator='\\n')\n",
    "            print(text)\n",
    "            text = clean_text(text)\n",
    "            #text_word = text.split()\n",
    "            return text.split()    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://news.google.com/search?q={query}&hl=en-CA&gl=CA&ceid=CA%3Aen\"\n",
    "    \n",
    "#    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "\n",
    "    \n",
    "    \n",
    "def scrape_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        word_text = text_to_word(soup)\n",
    "        content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "        content = soup.get_text()\n",
    "        return word_text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results[:5], start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print(\"   Content:\")\n",
    "        content = scrape_content(result['link'])\n",
    "        if content:\n",
    "            print(content[:500])  # Print the first 500 characters of the content\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "text = 'Hi my name is: sumin\", this-is to test text cleaning!'\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3b463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "678751e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "https://news.google.com/search?q=fish\n",
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:10]:  # Scraping top 10 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bfc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
