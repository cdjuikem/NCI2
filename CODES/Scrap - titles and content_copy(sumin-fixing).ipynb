{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02e06e55",
   "metadata": {},
   "source": [
    "Our thoughts on evaluating the search engine results:\n",
    "\n",
    "* I think we need some data to train our algorithm, especially to identify keywords and features of relevant/irrelevant articles. We can easily collect information from news search! (or Sogol and Sana may be able to provide us some information)\n",
    "\n",
    "### Variables (to determine the best/worst search engine):\n",
    "1. The number of relevant articles among top 20 results\n",
    "    * Give weights for the position of the article? (1st result has more weight and 20th result has less)\n",
    "2. how much specific information the relevant articles contain (general info only or specific info included)\n",
    "    * how well the search engines do for Broad / Specific prompts\n",
    "    * misreporting / underreporting any difference in results?\n",
    "3. dates of the articles (how old the articles are)\n",
    "4. how many different incidents show up (not every article is about the same incident)\n",
    "\n",
    "\n",
    "### Techniques for preliminary screen (removing irrelevent articles):\n",
    "1. keyword evaluation\n",
    "    * Frequency of the keywords OR how many keywords included (Clotilde's function) in the title and content (maybe not the whole content if it's long, we can sample some part for efficiency) - Sana's doc also mentioned Term Frequency-Inverse Document Frequency Algorithm (https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/)\n",
    "    * Naive Bayes model (similar to spam email detection, we can detect irrelevant articles) - but this needs the keyword statistics (probability) information / training data\n",
    "    \n",
    "2. Clustering\n",
    "    * Cluster the articles with similarity and sort out irrelevant articles - also need some training data\n",
    "    * Non-numerical data need to be carefully pre-processed to obtain the clusters we want. For example, usually the words should be mapped to numerical values (maybe there's some python library does the job?)\n",
    "    * From Sana's file, it seems like some engines (if not all) make clusters for their results so they can show different results. We can make our own clusters to gather relevant information.\n",
    "\n",
    "\n",
    "### Next thing to think about / questions:\n",
    "1. How to screen 'biased' information\n",
    "    * Penalize certain keywords such as politics, (we need to consider different things)\n",
    "2. Specific information (Vessel, captain's name, where it happend, what happened)\n",
    "    * How to identify them in the articles\n",
    "    * How to measure/compare the amount of information in an article numerically\n",
    "3. About regional biases, are we actually focus on Vancouver/North West America? Or anywhere in the world?\n",
    "4. Can we get certain information like click rate / scroll speed / time on page? (not sure if we would need them though)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What to do (June 12):\n",
    "Sumin: exploring clustering / how to identify keywords\n",
    "\n",
    "Clotilde: searching for previous works\n",
    "\n",
    "Irushi: reviewing the code / think about variables\n",
    "\n",
    "Hiva: searching about optimization\n",
    "\n",
    "Common:\n",
    "1. Subvariables for relevance of the top 20 results with the Sogol's queries (no need to use all 100 of them for now)\n",
    "    * how to measure how much broad / specific information the articles have.\n",
    "2. How to choose keywords (we need to collect some data from articles) - Sogol has some data?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8b434",
   "metadata": {},
   "source": [
    "### Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e94e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "# The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 3)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50f83f",
   "metadata": {},
   "source": [
    "###  Titles and links of the top 20 search results from Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:20]:  # Scraping top 20 results\n",
    "        title = result.find('a', class_ = 'JtKRv').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3715f7c",
   "metadata": {},
   "source": [
    "### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f10f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190c2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: vessel haddock\n",
      "1. Are Russian trawlers targeting small fish?\n",
      "https://news.google.com/articles/CBMiQmh0dHBzOi8vZmlza2VyZm9ydW0uY29tL2FyZS1ydXNzaWFuLXRyYXdsZXJzLXRhcmdldGluZy1zbWFsbC1maXNoL9IBAA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "content: \n",
      " ANNONCER Norwegian vessel operators’ federation Fiskebåt is concerned that Russian vessels are fishing for small fish in the Russian zone of the Barents Sea – which would constitute a serious violation of the fisheries agreement between Norway and Russia. In a letter to the Ministry of Fisheries, Fiskebåt has requested that the Norwegian authorities to contact their Russian colleagues to clarify measures to protect small fish in the Russian zone.\n",
      "\n",
      "2. UK fishing group doubles profit in 2022 despite 'abject failure' of post-Brexit quota negotiations\n",
      "https://news.google.com/articles/CBMikAFodHRwczovL3d3dy5pbnRyYWZpc2guY29tL3doaXRlZmlzaC91ay1maXNoaW5nLWdyb3VwLWRvdWJsZXMtcHJvZml0LWluLTIwMjItZGVzcGl0ZS1hYmplY3QtZmFpbHVyZS1vZi1wb3N0LWJyZXhpdC1xdW90YS1uZWdvdGlhdGlvbnMvMi0xLTE1MzE2NzHSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "content: \n",
      " The fishing opportunities available to the UK catching sector remain as 'more than a disappointment', said the owner and operator of the UK's last distant-water whitefish trawler, Kirkella. IntraFish supplies the breaking news and insight to inform better business decisions throughout the value chain, from the sea to the supermarket shelf. Our global team of experts are focused on the needs of industry professionals, delivering news at speed and explaining why it matters – unrivalled intelligence to help you thrive within a rapidly changing global industry.\n",
      " IntraFish is part of DN Media Group. To read more about DN Media Group,  click here.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = \"https://news.google.com\" + result['link'][1:]\n",
    "    print(link)\n",
    "    print(\"content: \")\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775bf9f",
   "metadata": {},
   "source": [
    "### Scraping titles and links of the top 20 search results from Yahoo News\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:20]:  # Scraping top 20 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af807320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93ee6f70",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa51c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clotilde's function for relevance evaluation\n",
    "# This function searches for the number of keywords in the given text (var: results)\n",
    "\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b9bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383998d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e77293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6708322e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a393e2c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad0434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce1786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4a6a1b",
   "metadata": {},
   "source": [
    "### From the original file. ---- no need to read below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc9c7e",
   "metadata": {},
   "source": [
    "### From GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c163870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['ocean', 'sea', 'crime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba9e92",
   "metadata": {},
   "source": [
    "### Modified (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43014e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: vessel fish crime\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_to_word(soup):\n",
    "    content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "#    print(content)\n",
    "    if content is not []:\n",
    "        for paragraph in content:\n",
    "            print(paragraph)\n",
    "            text = paragraph.get_text(separator='\\n')\n",
    "            print(text)\n",
    "            text = clean_text(text)\n",
    "            #text_word = text.split()\n",
    "            return text.split()    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://news.google.com/search?q={query}&hl=en-CA&gl=CA&ceid=CA%3Aen\"\n",
    "    \n",
    "#    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "\n",
    "    \n",
    "    \n",
    "def scrape_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        word_text = text_to_word(soup)\n",
    "        content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "        content = soup.get_text()\n",
    "        return word_text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results[:5], start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print(\"   Content:\")\n",
    "        content = scrape_content(result['link'])\n",
    "        if content:\n",
    "            print(content[:500])  # Print the first 500 characters of the content\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "text = 'Hi my name is: sumin\", this-is to test text cleaning!'\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14427bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea002656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "https://news.google.com/search?q=fish\n",
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:10]:  # Scraping top 10 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815733cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
