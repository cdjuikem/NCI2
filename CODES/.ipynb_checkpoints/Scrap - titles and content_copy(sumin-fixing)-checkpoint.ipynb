{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ff2e6d",
   "metadata": {},
   "source": [
    "Thoughts on evaluating the search engine results:\n",
    "\n",
    "* I think we need some data to train our algorithm, especially to identify keywords and features of relevant/irrelevant articles. We can easily collect information from news search.\n",
    "\n",
    "### Variables (to determine the best/worst search engine):\n",
    "1. The number of relevant articles among top 20 results\n",
    "2. how much specific information the relevant articles contain (general info only or specific info included)\n",
    "3. dates of the articles (how old the articles are)\n",
    "5. \n",
    "\n",
    "\n",
    "### Techniques for preliminary screen (removing irrelevent articles):\n",
    "1. keyword evaluation\n",
    "    * Frequency of the keywords OR how many keywords included (Clotilde's function)\n",
    "    * Naive Bayes model (similar to spam email detection, we can detect irrelevant articles) - but this needs the keyword statistics (probability) information / training data\n",
    "    \n",
    "2. Clustering:\n",
    "    * Cluster the articles with similarity and sort out irrelevant articles - also need some training data\n",
    "    * Non-numerical data need to be carefully pre-processed to obtain the clusters we want. For example, usually the words should be mapped to numerical values (maybe there's some python library does the job?)\n",
    "\n",
    "\n",
    "### Next thing to think about:\n",
    "1. How to screen 'biased' information\n",
    "    * Penalize certain keywords such as politics, (we need to consider different things)\n",
    "2. How to identify specific information (Vessel, captain's name, where it happend, what happened)\n",
    "3. About regional biases, are we actually focus on Vancouver/North West America? Or just in general?\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1aac45",
   "metadata": {},
   "source": [
    "### Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d839480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "        for paragraph in paragraphs:\n",
    "            content += \" \" + paragraph.text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42173e88",
   "metadata": {},
   "source": [
    "###  Titles and links of the top 20 search results from Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1d28718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:2]:  # Scraping top 20 results\n",
    "        title = result.find('a', class_ = 'JtKRv').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d3965",
   "metadata": {},
   "source": [
    "### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5bc9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1893b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: \n",
      "1. No bath, no laundry: Calgarians do their part to cut water usage as pipe rupture continues to cause shortages\n",
      "https://news.google.com/articles/CBMiemh0dHBzOi8vd3d3LnRoZWdsb2JlYW5kbWFpbC5jb20vY2FuYWRhL2FsYmVydGEvYXJ0aWNsZS1jYWxnYXJpYW5zLXBhc3Mtb24tYmF0aHMtbGF1bmRyeS1hcy13YXRlci1tYWluLXJlcGFpcnMtdG8tbGFzdC1hbGwv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "\n",
      " Crews work on repairs to a ruptured water main in Calgary on June 7.Jeff McIntosh/The Canadian Press Alexander Aubichon says the last time he had a bath was never. On Monday afternoon, the five-year-old was testing his new-found ability to pump himself higher on a swing. He jumped off and landed on a bed of pebbles in a playground at the Bowness Seniors’ Centre in Calgary. “A pipe burst,” Alexander explained. He is doing his part to conserve water after a major piece of infrastructure ruptured in Calgary last Wednesday, and officials are uncertain when it will be fixed – putting the city at risk of running out of treated water. Alexander’s mother, Rebekah Mahar, said that was the last day he and his little brother bathed. “It seems like they haven’t noticed,” she said. “We’re probably going to have to do a bath with just a short amount of water tonight. It is kind of getting to that point.” The Aubichon-Mahar family members are champion water savers, exceeding Calgary Mayor Jyoti Gondek’s request that residents trim indoor water consumption by 25 per cent. The city wants to ensure it can distribute water to everyone in the city, while keeping enough on hand for emergencies, such as a fire. “If we don’t stick to using less water, the reality is that we may run out of water. You could end up turning on a tap and nothing will come out,” Ms. Gondek said Monday afternoon. “That is the reality. It is not the scene out of a movie anymore. “I don’t say this to scare people, but I think it is incredibly important for us to understand how serious the situation is right now.” Calgary and its neighbouring cities slurped up 440 million litres of water on Saturday and 457 million litres on Sunday, compared with the 580 million litres they typically consume this time of year, Ms. Gondek said – thanking residents for using water judiciously. Demand on Sunday outstripped supply at times, she said, but the city started Monday morning with about 620 million litres of treated water available. Nancy Mackay, Calgary’s water services director, on Monday said the city was still “assessing the condition of the pipe and removing the damage.” Ms. Gondek on Sunday said she and the city could have done a better job of providing information in the early days of the emergency. She resolved to update residents twice daily and has provided concrete examples of how citizens can cut down on water use. The busted pipe, known as the Bearspaw South Feedermain, normally services water for 60 per cent of the city, as well as residents in Airdrie, Chestermere and Strathmore. Its diameter varies from 1.5 metres to about 1.95 metres. The broken feedermain is in Bowness, a community in Calgary’s northwest, which was under a boil water advisory until Monday. Ms. Mahar said that a layer of sediment remained after the boiling water cooled, so her family used it for dishes and washing their hands and faces. The family used a bike trailer and a 20-litre container to haul potable water from a filling station back to their home. Roughly 10,770 people live in Bowness, compared with about 1.3 million across Calgary, according to the 2021 census. Alberta Health Services lifted the boil water advisory Monday evening. The emergency prompted Stage 4 restrictions, meaning residents and businesses must not use water for outdoor use. Officials suggest residents limit showers to three minutes, flush the toilet fewer times, and only turn on dishwashers and washing machines when they are fully loaded. Calgary on Saturday said the repairs could take five to seven days. On Monday, Ms. Mackay said officials do not have an updated timeline because they are still analyzing the situation. Sarah St. Martin is a teacher at the Bowmont Community Preschool, where pupils now spend their entire day outside to limit the temptation to use water. The school, which is in Bowness, asked parents to pack bottles of drinking water for their kids, she said. One of Ms. St. Martin’s colleagues brings a jug of water for handwashing. About 19 kids attended the preschool Monday morning, and 11 went in the afternoon. The water restrictions means the kids are unable to care for their garden boxes and the outdoor mud kitchen is closed until further notice. At home, Ms. St. Martin’s two stepchildren, both teenagers, have to choose between showering or laundry. Ms. Mahar, Alexander’s mom, is still holding out on washing clothes: “I have five loads of laundry waiting,” she said in the park. And Alexander has plans for when he can finally take a proper bath again, too: “We have bath bombs,” he said. \n",
      "    The Globe would like to hear from the Calgarians who are being asked to conserve water as officials continue work to repair a critical water main. How are you handling the conservation efforts? What are you doing to tangibly limit your use? Share your experience below, or by sending an e-mail to audience@globeandmail.com.\n",
      "   \n",
      "      The information from this form will only be used for journalistic\n",
      "      purposes, though not all responses will necessarily be published. The\n",
      "      Globe and Mail may contact you if someone would like to interview you for\n",
      "      a story.\n",
      "      Report an editorial error Report a technical issue Editorial code of conduct Authors and topics you follow will be added to your personal news feed in Following. © Copyright 2024 The Globe and Mail Inc. All rights reserved. Andrew Saunders, President and CEO\n",
      "2. Canada Border Services Agency workers reach tentative deal: union\n",
      "https://news.google.com/articles/CBMiSmh0dHBzOi8vdG9yb250by5jaXR5bmV3cy5jYS8yMDI0LzA2LzExL2Nic2EtYm9yZGVyLXdvcmtlcnMtdGVudGF0aXZlLWRlYWwv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen\n",
      "\n",
      " Toronto \n",
      "\t\t\tBy Patricia D'Cunha\t\t Posted June 11, 2024 11:29 am. Last Updated June 11, 2024 12:58 pm. More than 9,000 Canada Border Services Agency (CBSA) workers have reached a tentative agreement, their union says. Workers were initially set to start job action on June 7, but it was postponed after an agreement was reached to extend mediation talks. The union then set a new strike deadline for Friday. “Our bargaining team has been working around the clock to secure the best contract for our members,” Sharon DeSousa, national president of Public Service Alliance of Canada, stated in a release. “This is a well-deserved victory for our members at CBSA who safeguard our nation’s borders and ensure the safety and security of all Canadians.” The key issues for bargaining included pay parity with other law enforcement agencies, remote work options, and pension benefits. “This tentative agreement demonstrates that the best agreements are always reached at the bargaining table. Border Services employees are critical to the safety and security of our borders and this tentative agreement recognizes the importance of that work while remaining reasonable for taxpayers,” Treasury Board President Anita Anand said in a statement. The union said details of the tentative deal will be released once it shared with members on Thursday. Members still need to vote to ratify the deal. The government said 90 per cent of front-line border officers are designated as essential, which means if workers did strike they wouldn’t have been able to walk off the job. However, they could work-to-rule, which could have brought trade to a standstill and cause hours of delays for travellers trying to cross the border into Canada. A similar strike three years ago nearly brought commercial border traffic to a standstill and caused major delays across the country. With files from Michael Talbot, CityNews; and The Canadian Press One person has been killed after a crash involving three dump trucks on the Gardiner Expressway on Tuesday afternoon.\n",
      "\n",
      "\n",
      "\n",
      "All lanes of the westbound Gardiner at York Street remain closed while an investigation... \n",
      "\n",
      "\t\t\t\t\t\t\t1h ago\t\t\t\t\t\t\n",
      " A man who admitted to killing a woman by dousing her with lighter fluid and setting her on fire onboard a TTC bus at Kipling Subway Station in June 2022 has been found not criminally responsible due to... \n",
      "\n",
      "\t\t\t\t\t\t\t2h ago\t\t\t\t\t\t\n",
      " Three people, including a 15-year-old, have been injured after a reported fight near Jane Street and Lawrence Avenue West.\n",
      "\n",
      "\n",
      "\n",
      "Police say they were called to the intersection just before 3:30 p.m. to reports... \n",
      "\n",
      "\t\t\t\t\t\t\t20m ago\t\t\t\t\t\t\n",
      " Walking along the corner of Bathurst and Wilson Streets in Toronto is like a trip back to the Philippines for many Filipinos in the GTA.\n",
      "\n",
      "\n",
      "\n",
      "Hearing the language spoken openly, noticing the aroma of Filipino... \n",
      "\n",
      "\t\t\t\t\t\t\t6m ago\t\t\t\t\t\t\n",
      " One person has been killed after a crash involving three dump trucks on the Gardiner Expressway on Tuesday afternoon.\n",
      "\n",
      "\n",
      "\n",
      "All lanes of the westbound Gardiner at York Street remain closed while an investigation... \n",
      "\n",
      "\t\t\t\t\t\t\t1h ago\t\t\t\t\t\t\n",
      " A man who admitted to killing a woman by dousing her with lighter fluid and setting her on fire onboard a TTC bus at Kipling Subway Station in June 2022 has been found not criminally responsible due to... \n",
      "\n",
      "\t\t\t\t\t\t\t2h ago\t\t\t\t\t\t\n",
      " Three people, including a 15-year-old, have been injured after a reported fight near Jane Street and Lawrence Avenue West.\n",
      "\n",
      "\n",
      "\n",
      "Police say they were called to the intersection just before 3:30 p.m. to reports... \n",
      "\n",
      "\t\t\t\t\t\t\t20m ago\t\t\t\t\t\t\n",
      " Walking along the corner of Bathurst and Wilson Streets in Toronto is like a trip back to the Philippines for many Filipinos in the GTA.\n",
      "\n",
      "\n",
      "\n",
      "Hearing the language spoken openly, noticing the aroma of Filipino... \n",
      "\n",
      "\t\t\t\t\t\t\t6m ago\t\t\t\t\t\t\n",
      " Mississauga has a new mayor, and her name is Carolyn Parrish. She won Monday night's byelection in convincing fashion to become the city's third mayor in nearly 50 years. Our Michelle Mackey recaps an historic night in Mississauga \n",
      "\n",
      "\t\t\t\t\t\t\t18h ago\t\t\t\t\t\t\n",
      " A new outreach program is underway to help homeless and other vulnerable people in the heart of the downtown core.  Shauna Hunt explains the new pilot project.  \n",
      "\n",
      "\t\t\t\t\t\t\t19h ago\t\t\t\t\t\t\n",
      " Veteran politician Carolyn Parrish celebrates her mayoral byelection win, thanking supporters and the citizens of Mississauga for trusting her to become their next leader. \n",
      "\n",
      "\t\t\t\t\t\t\t20h ago\t\t\t\t\t\t\n",
      " Alvin Tedjo says he will remain on city council after he finished second in the Mississauga mayoral byelection behind Carolyn Parrish. \n",
      "\n",
      "\t\t\t\t\t\t\t20h ago\t\t\t\t\t\t\n",
      " Country Queen Shania Twain is joining Canada's Got Talent as a Judge and Tracey Moore and Cheryl Hickey will host a new program for Canadians. Lindsay Dunn has the story.  \n",
      "\n",
      "\t\t\t\t\t\t\t22h ago\t\t\t\t\t\t\n",
      " Now New and Improved! Watch CityNews, listen to 680 NewsRadio live anytime and get up-to-the-minute breaking-news alerts, traffic, weather and video from CityNews Toronto anywhere you are – across all Android and iOS devices.\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = \"https://news.google.com\" + result['link'][1:]\n",
    "    print(link)\n",
    "    print()\n",
    "    print(scrape_content(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd92f06",
   "metadata": {},
   "source": [
    "### Scraping titles and links of the top 20 search results from Yahoo News\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa49060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:20]:  # Scraping top 20 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c24f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4de1a2a6",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Clotilde's function for relevance evaluation\n",
    "# This function searches for the number of keywords in the given text (var: results)\n",
    "\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f48ec9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3da8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d363150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8d0a14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85c3ec46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5eaf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a83e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209ab7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb6b47f",
   "metadata": {},
   "source": [
    "### From GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['ocean', 'sea', 'crime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba9e92",
   "metadata": {},
   "source": [
    "### Modified (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43014e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: vessel fish crime\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_to_word(soup):\n",
    "    content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "#    print(content)\n",
    "    if content is not []:\n",
    "        for paragraph in content:\n",
    "            print(paragraph)\n",
    "            text = paragraph.get_text(separator='\\n')\n",
    "            print(text)\n",
    "            text = clean_text(text)\n",
    "            #text_word = text.split()\n",
    "            return text.split()    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://news.google.com/search?q={query}&hl=en-CA&gl=CA&ceid=CA%3Aen\"\n",
    "    \n",
    "#    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "\n",
    "    \n",
    "    \n",
    "def scrape_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        word_text = text_to_word(soup)\n",
    "        content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "        content = soup.get_text()\n",
    "        return word_text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results[:5], start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print(\"   Content:\")\n",
    "        content = scrape_content(result['link'])\n",
    "        if content:\n",
    "            print(content[:500])  # Print the first 500 characters of the content\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd66536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^-a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "text = 'Hi my name is: sumin\", this-is to test text cleaning!'\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acdf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a53786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fish\n",
      "https://news.google.com/search?q=fish\n",
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    print(response)\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    results = []\n",
    "    for result in search_results[:10]:  # Scraping top 10 results\n",
    "        title = result.find('h4').text\n",
    "        link = result.find('a')['href']\n",
    "        results.append({'title': title, 'link': link})\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66f1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
