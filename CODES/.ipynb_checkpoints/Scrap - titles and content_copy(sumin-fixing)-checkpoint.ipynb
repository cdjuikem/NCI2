{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4462c347",
   "metadata": {},
   "source": [
    "# Scrap functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8b434",
   "metadata": {},
   "source": [
    "## 1. Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e94e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the contents in the url\n",
    "#### input: url\n",
    "#### output: content (type: str)\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#print(soup)\n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "\n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "###### The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 10)):\n",
    "### For the whole content:\n",
    "#        for i in range(len(paragraphs)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "scrape_content('https://www.msn.com/en-ca/lifestyle/other/experts-urge-people-to-fish-and-eat-crab-species-putting-entire-fishing-industry-at-risk-an-animal-of-unacceptable-intelligence/ar-BB1na7wc?ocid=BingNewsSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50f83f",
   "metadata": {},
   "source": [
    "## 2. Auto scrap of the top 20 search results from different search engines\n",
    "Search Engines that we use:\n",
    "* 2-1. Google (news tab)\n",
    "* 2-2. Yahoo (news tab)\n",
    "* 2-3. Bing (news tab)\n",
    "* 2-4. Maritime Executive (content scraping is not authorized.- producing 403 errors)\n",
    "\n",
    "What we scrap:\n",
    "* Title\n",
    "* Link\n",
    "* Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efd8d8",
   "metadata": {},
   "source": [
    "###  2-1. Google search in the news tab (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "697b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from google news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://www.google.com/search?q={query}&tbm=nws\"\n",
    "    print(url)\n",
    "#    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='Gx5Zad fP1Qef xpd EtOod pkphOe')\n",
    "    date_elements = soup.find_all('span', class_='r0bn4c rQMQod')\n",
    "\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 5)):\n",
    "        title = search_results[i].find('a').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        #link = 'https://news.google.com' + link[1:]\n",
    "        date = date_elements[i].text   \n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3715f7c",
   "metadata": {},
   "source": [
    "#### 2-1-1. Testing Google scrap algorithm\n",
    "#### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66f10f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])\n",
    "    \n",
    "queries = queries[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477367a",
   "metadata": {},
   "source": [
    "#### Testing with queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b7f5625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fishing\n",
      "https://www.google.com/search?q=fishing&tbm=nws\n",
      "1. Inside the Slimy, Smelly, Secretive World of Glass-Eel FishingThe New YorkerEach spring, hundreds of millions of baby eels swarm the waterways of coastal Maine. Soaring global demand incited an era of jackpot payouts...11 hours ago\n",
      "https://www.newyorker.com/magazine/2024/06/24/inside-the-slimy-smelly-secretive-world-of-glass-eel-fishing&sa=U&ved=2ahUKEwj_tLrLyuOGAxXRCjQIHT7eDMUQxfQBegQIBBAC&usg=AOvVaw06NpDYqEjrhSW4jGt_oa5F\n",
      "11 hours ago\n",
      " Find anything you save across the site in your account  By Paige Williams The Sargasso Sea, a warm, calm expanse of the North Atlantic Ocean, is bordered not by land but by four strong currents—a gyre. Vast mats of prickly brown seaweed float so thickly on the windless surface that Christopher Columbus worried about his ships getting stuck. The biodiverse sanctuary within and beneath the sargassum produces Anguilla rostrata, the American eel. Each female lays some eight million eggs. The eggs hatch as ribbonlike larvae that drift to the Gulf Stream, which carries them to the continental shelf. By the time they reach Maine, the larvae have transformed into swimmers about the length of an index finger, with the circumference of a bean sprout and the translucence of a jellyfish. Hence their nickname, glass eels, also known as elvers. The glass eel is barely visible, but for a dark stripe—its developing backbone—and a couple of chia seeds for eyes. “Ghosts on the water,” a Maine fisherman once called them. Travelling almost as one, like a swarm or a murmuration, glass eels enter tidal rivers and push upstream, pursuing the scent of freshwater until, ideally, they reach a pond and commence a long, tranquil life of bottom-feeding. Elvers mature into adults two to three feet in length, with the girth and the coloring of a slimy bicycle tire. Then, one distant autumn, on some unknown cue, they return to the Sargasso, where they spawn and die. Maine has thirty-five hundred miles of coastline, including coves, inlets, and bays, plus hundreds of tidal rivers, thousands of streams, and what has been described as “an ungodly amount of brooks.” Hundreds of millions of glass eels arrive each spring, as the waters warm. Four hundred and twenty-five licensed elvermen are allowed to harvest slightly more than seven thousand five hundred pounds of them during a strictly regulated fishing season, which runs from late March to early June. Four Native American tribes may legally fish another two thousand or so pounds, with more than half of that amount designated for the Passamaquoddy, who have lived in Maine and eastern Canada for some twelve thousand years. Maine is the only state with a major elver fishery. South Carolina has a small one (ten licensed elvermen), but everywhere else, in an effort to preserve the species, elver fishing is a federal crime. The elvermen sell their catch to state-licensed buyers, who in turn sell to customers in Asia. The baby eels are shipped live, mostly to Hong Kong, in clear plastic bags of water and pure oxygen, like a sophisticated twist on pet-store goldfish. They live in carefully tended tanks and ponds at aquaculture farms until they are big enough to be eaten. Japan alone annually consumes at least a hundred thousand tons of freshwater eel, unagi, which is widely enjoyed kabayaki style—butterflied, marinated, and grilled. The American eel became a valuable commodity as overfishing, poaching, and other forms of human interference led to the decline of similar species in Japan (Anguilla japonica) and Europe (Anguilla anguilla). Those species are now red-listed as, respectively, endangered and critically endangered. The U.S. has not declared the American eel endangered, and fishermen want to keep it that way. In March, 2011, just before elver season started in Maine, a tsunami in Japan decimated aquaculture ponds, driving the price of American glass eels from about two hundred dollars per pound to nearly nine hundred by the season’s closing day. The next year, the price reached one thousand eight hundred and sixty-nine dollars per pound, and soon topped two thousand. National Fisherman calls glass eels “likely the most valuable fish in the United States on a per-pound basis.” A recent issue of Marine Policy cited “unprecedented demand” for American eel. Only lobster outranks it in Maine. During a favorable market and a hard elver run, a Mainer may earn a hundred thousand dollars in a single haul. Each license holder is assigned a quota, ranging from four pounds to more than a hundred, based partly on seniority. Even the lowest quota insures a payout of six thousand dollars if the price per pound breaches fifteen hundred, which happens with some regularity. Maine is the only place in the country where a kid can become eligible for an elver license at fifteen and win a shot at making more money overnight, swinging a net, than slinging years’ worth of burgers. Elvermen have sent their children to college on eels, and have used the income to improve their homes, their businesses, their boobs. This year, more than forty-five hundred Mainers applied for sixteen available licenses. One frosty evening in April, an elverman named Sam Glass turned onto a dead-end road in the state’s northernmost coastal region, Down East, and parked beside a stream. The water was about thirty feet wide, with boulders across it and trees on the other side. The stream feeds West Bay, which leads to the Atlantic, whose tide swells and then shrinks the river’s volume every twelve hours. Glass, a tall, reserved fifty-year-old with dark, curly hair and a trim beard, pulled five hand-chopped maple poles from the bed of his pickup truck and carried them down the riverbank. Next, he fetched a plastic bucket, nylon cord, coils of rope, two boat anchors, and a fyke net. Unfurled, the net, made of pale, fine-gauge mesh, resembled a Chinese lantern trailed by two oversized streamers, or a mutant sea creature with a barrel-shaped head. Link copied\n",
      "\n",
      "2. Ontario offering more opportunities for people to go fishingwindsornewstoday.caWith Father's Day weekend in full swing, the province of Ontario is hoping people will take advantage by offering more chances to fish.2 days ago\n",
      "https://windsornewstoday.ca/windsor/news/2024/06/15/ontario-offering-more-opportunities-for-people-to-go-fishing&sa=U&ved=2ahUKEwj_tLrLyuOGAxXRCjQIHT7eDMUQxfQBegQICRAC&usg=AOvVaw0b3_WG5vETxNx8q6P3edK2\n",
      "2 days ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n",
      "3. Man on fishing trip drowns trying to retrieve his keys from a lake. Companion tried to save himABC News - Breaking News, Latest News and VideosAuthorities say a man drowned in a lake in upstate New York after his keys fell into the water and he tried to retrieve them.6 hours ago\n",
      "https://abcnews.go.com/US/wireStory/man-fishing-trip-drowns-retrieve-keys-lake-companion-111186132&sa=U&ved=2ahUKEwj_tLrLyuOGAxXRCjQIHT7eDMUQxfQBegQIBRAC&usg=AOvVaw3o147dTNokcbRq2AUmgpsE\n",
      "6 hours ago\n",
      " Authorities say a man drowned in a lake in upstate New York after his keys fell into the water and he tried to retrieve them HASTINGS, N.Y. -- A man drowned in a lake in upstate New York after his keys fell into the water and he tried to retrieve them, police said. Anthony Davis, 44, was fishing with two other men in Oneida Lake in the town of Hastings just before 10 a.m. Sunday when Davis' keys fell in the water, Lt. Andrew Bucher, of the Oswego County Sheriff's Office, said in a news release. Davis went into the water to try to get his keys but could not make it back to land, Bucher said. One of the other men, Wattie Cappers, 42, went in after Davis but also was unable to get back to land. Personnel from several local police and fire departments arrived and tried to rescue the two men, Bucher said. Davis was pulled from the water by a diver from the Brewerton Fire Department and taken to a hospital, where he was pronounced dead. Cappers was rescued by Oswego sheriff's deputies and was stable at the hospital. 24/7 coverage of breaking news and live events\n",
      "\n",
      "4. One of Earth's biggest freshwater fish is bouncing back, a rare 'win win'National GeographicGiant freshwater fish are among the most endangered animals on Earth. But in the lush waterways of the Amazon, one leviathan is swimming...3 days ago\n",
      "https://www.nationalgeographic.com/animals/article/arapaima-brazil-amazon-conservation-fishing&sa=U&ved=2ahUKEwj_tLrLyuOGAxXRCjQIHT7eDMUQxfQBegQIAxAC&usg=AOvVaw21a6PcZKYM8dQ2wOjkwV1F\n",
      "3 days ago\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n",
      "5. Man walking his dog stumbles across unusual pits on beach, leading to unexpected discoveryFox NewsBritish officials recently announced the discovery of centuries-old fishing bait tanks that were cut into beachrock. posing a historical and...18 hours ago\n",
      "https://www.foxnews.com/lifestyle/man-walking-his-dog-stumbles-unusual-pits-beach-leading-unexpected-discovery&sa=U&ved=2ahUKEwj_tLrLyuOGAxXRCjQIHT7eDMUQxfQBegQICBAC&usg=AOvVaw2ZefnrxPOwcNkaJT0GRmgs\n",
      "18 hours ago\n",
      "Failed to retrieve content. Status code: 404\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = result['link'][7:]\n",
    "    print(link)\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f22b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8775bf9f",
   "metadata": {},
   "source": [
    "### 2-2. Yahoo search in the news tab (working)\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Yahoo news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form\n",
    "########################################################\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "    date_elements = soup.find_all('span', class_='fc-2nd s-time mr-8')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 5)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445576b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    ### NOT print(scrape_content(link))\n",
    "    print(scrape_content(result['link']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a1626",
   "metadata": {},
   "source": [
    "###### Clotilde's data frame function (copied from Global code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443eeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Google for query: Vessel caught misreporting catch amount\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Sea Shepherd Global', 'Link': 'https://news.google.com/articles/CBMiQWh0dHBzOi8vd3d3LnNlYXNoZXBoZXJkZ2xvYmFsLm9yZy9sYXRlc3QtbmV3cy9jb21iYXQtaXV1LWZpc2hpbmcv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Wednesday, 05 Jun, 2024 Sea Shepherd Global stands at the forefront of the fight against Illegal, Unreported, and Unregulated (IUU) fishing, deploying innovative strategies and international collaborations to protect marine biodiversity.\\xa0 Illegal, Unreported, and Unregulated (IUU) fishing refers to fishing activities that do not comply with national, regional, or international fisheries conservation and management laws and regulations. These activities are conducted by vessels in various ways, including:'}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'New rules tighten controls on EU…', 'Link': 'https://news.google.com/articles/CBMiZGh0dHBzOi8vZWpmb3VuZGF0aW9uLm9yZy9uZXdzLW1lZGlhL25ldy1ydWxlcy10aWdodGVuLWNvbnRyb2xzLW9uLWV1LXZlc3NlbHMtdG8tcHJldmVudC1taXNyZXBvcnRpbmfSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Today, the Environmental Justice Foundation commends the European Commission for approving new rules that require stricter controls for landings by EU fishing vessels, providing new tools to prevent significant misreporting of unsorted catches when landing in selected ports, including those in third countries. It is welcome that the new rules require advanced and stricter control tools, such as CCTV to monitor landings, and set minimum benchmarks for the rates of inspection on trans-shipments. If properly implemented, this can increase transparency and accuracy in reporting by EU fleets that catch a large number of species, including those that have been overfished, such as yellowfin tuna in the Indian Ocean. Sean Parramore, Senior EU Advocacy Officer at the Environmental Justice Foundation, said: \"Stricter control measures on landings by EU vessels that have more leeway to misreport their catches are critical to prevent hidden overfishing. In the long run, everyone loses if we open the door to fish fraud. Fish stocks will collapse and the EU\\'s credibility in its efforts to promote sustainable fisheries on the world stage will be severely undermined. These new rules can help prevent that.“'}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'N.S. boat captain, 2 companies fined $125K for fisheries violations', 'Link': 'https://news.google.com/articles/CBMiYGh0dHBzOi8vd3d3LmNiYy5jYS9uZXdzL2NhbmFkYS9ub3ZhLXNjb3RpYS9uLXMtYm9hdC1jYXB0YWluLWZpbmVkLWZpc2hlcmllcy12aW9sYXRpb25zLTEuNjk2NTE5ONIBIGh0dHBzOi8vd3d3LmNiYy5jYS9hbXAvMS42OTY1MTk4?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' A boat captain from Sambro, N.S., with a history of fishery convictions has been fined $60,000 and banned from fishing for six months for five violations that included a secret, middle-of-the-night offload of halibut. The case involved misreporting of halibut, hake and cod catch from trips on board the fishing boat Ivy Lew between May 2019 and June 2020. Casey Henneberry, 40, and ALS Fisheries and Law Fisheries were found guilty last October by Halifax provincial court\\xa0Judge\\xa0Elizabeth Buckle.'}\n",
      "Failed to retrieve content. Status code: 403\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Move the IUU Fight Up the Food Chain | Proceedings - November 2023 Vol. 149/11/1,449', 'Link': 'https://news.google.com/articles/CBMiUmh0dHBzOi8vd3d3LnVzbmkub3JnL21hZ2F6aW5lcy9wcm9jZWVkaW5ncy8yMDIzL25vdmVtYmVyL21vdmUtaXV1LWZpZ2h0LWZvb2QtY2hhaW7SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': None}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Surprise! Media is misreporting the source of a Dutch cargo ship fire', 'Link': 'https://news.google.com/articles/CBMiZGh0dHBzOi8vZWxlY3RyZWsuY28vMjAyMy8wNy8yNi9zdXJwcmlzZS1tZWRpYS1pcy1taXNyZXBvcnRpbmctdGhlLXNvdXJjZS1vZi1hLWR1dGNoLWNhcmdvLXNoaXAtZmlyZS_SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Early this morning, the Fremantle Highway, a vehicle carrying cargo ship, caught fire in the North Sea, off the coast of Ameland in the Netherlands. The fire has killed one person on board and injured several more, though all 23 crew members have at this point been evacuated from the ship. Currently, the fire is still burning and the cause of the fire is unknown, according to the Dutch Coast Guard, which is carrying out the firefighting operation. But media reports, seeming to all crib from the same misquote, would have you think otherwise. The Fremantle Highway is a “roll-on/roll-off,” or “RoRo,” ship, a vehicle carrier designed for cars to drive on and off of it in loading and unloading. According to early reports, it was carrying 2,832 gas-powered cars, complete with a large amount of volatile energy stored in their collective gas tanks (some is needed to drive on and off of the ship) and in the tank of the ship itself, and 25 electric cars, from Germany to Egypt (those numbers have now been updated to 3,783 cars, including 498 EVs – which shows the peril of reporting on things like this with incomplete information).'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     25\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuery\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[0;32m     26\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngine\u001b[39m\u001b[38;5;124m'\u001b[39m: engine_name,\n\u001b[0;32m     27\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo results\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m             })\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;28mlen\u001b[39m(data)])\n\u001b[1;32m---> 32\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adding a delay to be respectful of website policies\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#### Data frame:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "search_engines = {\n",
    "    'Google': scrape_google_news,\n",
    "    'Yahoo': scrape_yahoo_news,\n",
    "}\n",
    "\n",
    "data = []\n",
    "for query in queries:\n",
    "    for engine_name, scrape_function in search_engines.items():\n",
    "        print(f\"Scraping {engine_name} for query: {query}\")\n",
    "        results = scrape_function(query)\n",
    "        \n",
    "        if results:\n",
    "            for result in results[:len(results)]:  # Limiting to the top 20 results per engine\n",
    "                data.append({\n",
    "                    'Query': query,\n",
    "                    'Engine': engine_name,\n",
    "                    'Title': result['title'],\n",
    "                    'Link': result['link'],\n",
    "                    'Content': scrape_content(result['link'])\n",
    "                })\n",
    "                print(data[len(data)-1])\n",
    "\n",
    "        else:\n",
    "            data.append({\n",
    "                'Query': query,\n",
    "                'Engine': engine_name,\n",
    "                'Title': 'No results'\n",
    "            })\n",
    "            print(data[len(data)])\n",
    "\n",
    "        \n",
    "        time.sleep(1)  # Adding a delay to be respectful of website policies\n",
    "\n",
    "#### Data frame:\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b4064",
   "metadata": {},
   "source": [
    "### 2-3. Microsoft Bing search in news tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49840bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: fishing\n",
      "https://www.bing.com/news/search?q=fishing\n",
      "1. Experts urge people to fish and eat crab species putting entire fishing industry at risk: 'An animal of unacceptable intelligence'\n",
      "https://www.msn.com/en-ca/lifestyle/other/experts-urge-people-to-fish-and-eat-crab-species-putting-entire-fishing-industry-at-risk-an-animal-of-unacceptable-intelligence/ar-BB1na7wc?ocid=BingNewsSearch\n",
      "1 hour ago\n",
      "\n",
      "\n",
      "2. Video shows the Houthis attack a merchant ship with a naval drone seemingly disguised as a slow fishing boat\n",
      "https://www.msn.com/en-us/news/world/video-shows-the-houthis-attack-a-merchant-ship-with-a-naval-drone-seemingly-disguised-as-a-slow-fishing-boat/ar-BB1ooEEY?ocid=BingNewsSearch\n",
      "1 hour ago\n",
      "\n",
      "\n",
      "3. Commercial lobster sector concerned about out-of-season fishing in St. Marys Bay\n",
      "https://www.msn.com/en-ca/news/other/commercial-lobster-sector-concerned-about-out-of-season-fishing-in-st-marys-bay/ar-BB1oav0d?ocid=BingNewsSearch\n",
      "4 days ago\n",
      "\n",
      "\n",
      "4. Free Fishing For Father's Day Weekend In Ontario\n",
      "https://www.ptbocanada.com/journal/free-fishing-for-fathers-day-weekend-in-ontario\n",
      "3 days ago\n",
      " The last annual free fishing event this year is Family Fishing Week (June 29 to July 7) to celebrate Canada Day. Photo courtesy of CAMCAST.  Upcoming Events sponsored by PTBOCANADA is a website about Peterborough, Ontario, Canada. Copyright ©2010-2022, PTBOCANADA Media Inc. All rights reserved.\n",
      "\n",
      "5. Free fishing to be offered across the province\n",
      "https://www.country1053.ca/2024/06/13/fishing-draft/\n",
      "4 days ago\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "6. Fishing Outreach Program Helps Veterans ‘Cast Off' Stress: \"I Love Being Of Service!\"\n",
      "https://www.msn.com/en-us/news/us/fishing-outreach-program-helps-veterans-cast-off-stress-i-love-being-of-service/ar-BB1ooyhe?ocid=BingNewsSearch\n",
      "1 hour ago\n",
      "\n",
      "\n",
      "7. Fishing fun on Father's Day in Salmon Arm\n",
      "https://www.saobserver.net/community/fishing-fun-on-fathers-day-in-salmon-arm-7392527\n",
      "5 hours ago\n",
      "  Sign In  Subscribe Now Despite the clouds convening over the Shuswap, it didn’t rain in the parade of the annual Salmon Arm Kids Fishing Derby.  Taking place on Father’s Day at the Salmon Arm Wharf, children from one-year-old to 12 years old, gathered to try their hand at casting a line out into the water in hopes of reeling back in a big one.  The day had a great number of fishing prizes to be one including: •First Fish which went to Lincoln Robinson. •Biggest fish:  •Who caught the most fish: Five kids were tied and in the running for third place, with the heaviest all together fish weight being the deciding factor. •Hidden weight went to Ivan Ivanov and Ruby Filipski. \n",
      "\n",
      "8. PETA calls out Stoney Creek dads thinking about going fishing\n",
      "https://torontosun.com/news/local-news/peta-calls-out-stoney-creek-dads-thinking-about-going-fishing\n",
      "4 days ago\n",
      "Failed to retrieve content. Status code: 403\n",
      "None\n",
      "\n",
      "9. Charlotte man who went fishing with friends drowns at Jordan Lake\n",
      "https://www.msn.com/en-us/news/crime/charlotte-man-who-went-fishing-with-friends-drowns-at-jordan-lake/ar-BB1omYcc?ocid=BingNewsSearch\n",
      "9 hours ago\n",
      "\n",
      "\n",
      "10. Man on fishing trip drowns trying to retrieve his keys from a lake. Companion tried to save him\n",
      "https://www.msn.com/he-il/news/other/man-on-fishing-trip-drowns-trying-to-retrieve-his-keys-from-a-lake-companion-tried-to-save-him/ar-BB1onM7s?ocid=BingNewsSearch\n",
      "6 hours ago\n",
      "\n",
      "\n",
      "11. Ontario Getting More People Hooked On Fishing\n",
      "https://muskoka411.com/ontario-getting-more-people-hooked-on-fishing/\n",
      "2 days ago\n",
      "  Get the daily inside scoop\n",
      "right in your inbox.\n",
      " Email address:\n",
      "\n",
      " \n",
      " Sign me up for the newsletter!\n",
      "  Yes! I’d like to receive emails from Muskoka 411Yes, I’d like to receive email from Muskoka411's partners\n",
      " You can unsubscribe at any time, learn more at our Privacy Policy page\n",
      " Δ The Ontario government is making it easier and more accessible for people to learn how to fish. This summer, the province is expanding its Learn to Fish program, increasing the number of locations and mobile sessions offered and providing 2,700 new spots. “Fishing is a wonderful activity and long-standing tradition in Ontario and I’m glad to be offering more opportunities – many of them in urban areas – for people to be able to take part in the Learn to Fish program,” said Graydon Smith, Minister of Natural Resources. “Expansion of this program creates more opportunities for people to get out and explore Ontario’s many lakes and rivers and maybe even find a new hobby.”\n",
      "\n",
      "12. Hooked on Fishing: Provincial free Learn to Fish program expanding to Georgina, Mississauga\n",
      "https://www.toronto.com/news/hooked-on-fishing-provincial-free-learn-to-fish-program-expanding-to-georgina-mississauga/article_08da49ef-5d4f-52f3-8633-9e63c35d8981.html\n",
      "4 days ago\n",
      "  Reporter The Ontario government is making it easier and more accessible for people to learn how to fish. Ontario is reeling in more fishing enthusiasts. Beginning this summer, the province is expanding its free Learn to Fish program to include Lakefront Promenade and Lake Aquitaine in Mississauga and Sibbald Point Provincial Park in Georgina. “Fishing is a wonderful activity and long-standing tradition in Ontario and I’m glad to be offering more opportunities — many of them in urban areas — for people to be able to take part in the Learn to Fish program,” said Graydon Smith, minister of natural resources, in a news release. “Expansion of this program creates more opportunities for people to get out and explore Ontario’s many lakes and rivers and maybe even find a new hobby.” Additionally, the Learn to Fish mobile unit will be travelling to 11 locations this year — including provincial parks, conservation areas and events. With the latest expansion of locations and mobile sessions, the program that runs from mid-June to late August now offers 2,700 new fishing spots. In total, the Learn to Fish program will engage more than 12,000 new anglers at 20 locations across the province this summer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Microsoft Bing news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form like Google\n",
    "########################################################\n",
    "\n",
    "def scrape_bing_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://www.bing.com/news/search?q={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='news-card')\n",
    "    date_elements = soup.find_all('span', tabindex=\"0\")\n",
    "    \n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 20)):\n",
    "        title = search_results[i].find('a', class_ = 'title').text\n",
    "        link = search_results[i].find('a', class_ = 'title')['href']\n",
    "        date = date_elements[i]['aria-label']\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_bing_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    print(scrape_content(result['link']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653e3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c962efc",
   "metadata": {},
   "source": [
    "### 2-4 Maritime Executive search\n",
    "The content scraping is not authorized (403 error). Need to set up an API or header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Microsoft Bing news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form like Google\n",
    "########################################################\n",
    "\n",
    "def scrape_maritime_executive(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://www.maritime-executive.com/search?key={query}\"\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(soup)\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='desc body no-padding-xs')\n",
    "#    date_elements = soup.find_all('span', tabindex=\"0\")\n",
    "    \n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 20)):\n",
    "        title = search_results[i].find('a', class_ = \"font-firasans\").text\n",
    "        title = title.strip()\n",
    "        link = search_results[i].find('a', class_ = \"font-firasans\")['href']\n",
    "        date = 0\n",
    " #       date = search_results[i].find('p', class_ = \"summary\")\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_maritime_executive(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "#    print(result['date'])\n",
    "    print(scrape_content(result['link']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18304f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
