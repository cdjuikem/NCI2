{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a84bf1",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f13329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12a389",
   "metadata": {},
   "source": [
    "###  Set url to scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa16567",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://cimsec.org/lead-the-fight-against-climate-change-and-transnational-crime-in-the-indian-ocean/')\n",
    "soup = BeautifulSoup(url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02544f30",
   "metadata": {},
   "source": [
    "### Scrap the content and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a2952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def text_to_word(soup):\n",
    "    # this can be different for each link, because sites may use different html structure/classes\n",
    "    content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "    for paragraph in content:\n",
    "        text = paragraph.get_text(separator='\\n')\n",
    "        text = clean_text(text)\n",
    "        text_wordlist = text.split()\n",
    "    return text_wordlist\n",
    "\n",
    "text_wordlist = text_to_word(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ce708",
   "metadata": {},
   "source": [
    "### Count the frequency of each word appeared in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49d387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter(text_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81113836",
   "metadata": {},
   "source": [
    "### Using pandas to form a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9dd96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8317009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  count\n",
      "0     notes      1\n",
      "1        to     16\n",
      "2       the     32\n",
      "3       new      1\n",
      "4       cno      2\n",
      "..      ...    ...\n",
      "258    port      1\n",
      "259    city      1\n",
      "260   lanka      1\n",
      "261     via      1\n",
      "262  xinhua      1\n",
      "\n",
      "[263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(word_count.items(), columns = ['word', 'count'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437c313",
   "metadata": {},
   "source": [
    "####\n",
    "#### I searched for \"fish crime\", \"ocean crime\", \"fish crime ship\", etc on Google. \n",
    "#### Sometimes, the results contain some irrelevant articles such as \"stolen fish from restaurants\".\n",
    "#### My first thought on an algorithm detecting relevant information is to use the naive Bayes model like a spam-email detector.\n",
    "####\n",
    "#### A technical question: Can we automatically collect the top 20 results from a search engine, instead of doing it one-by-one like my code?\n",
    "#### If we can do, that can save us a lot of time!\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb0434b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sesarch engines...Here are some examples just to start with:\n",
    "\n",
    "# Google News: https://news.google.com/home?hl=en-CA&gl=CA&ceid=CA:en\n",
    "# Yahoo News: https://ca.news.yahoo.com/\n",
    "\n",
    "#### North America\n",
    "# Global News: https://globalnews.ca/\n",
    "# CNN: https://www.cnn.com/\n",
    "# CBC News: https://www.cbc.ca/news\n",
    "# NBC News: https://www.nbcnews.com/\n",
    "# Maritime Crimes: https://maritimescrimes.com/\n",
    "\n",
    "#### Europe\n",
    "# BBC News: https://www.bbc.com/news\n",
    "\n",
    "#### South Korea (These were easy to find, just because I'm Korean haha)\n",
    "# Korea Herald: https://www.koreaherald.com/\n",
    "# Korea Times: https://www.koreatimes.co.kr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56906f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
