{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4462c347",
   "metadata": {},
   "source": [
    "# Scrap functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8b434",
   "metadata": {},
   "source": [
    "## 1. Scraping content from the given url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e94e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the contents in the url\n",
    "#### input: url\n",
    "#### output: content (type: str)\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "###### The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 3)):\n",
    "### For the whole content:\n",
    "#        for i in range(len(paragraphs)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a50f83f",
   "metadata": {},
   "source": [
    "## 2. Auto scrap of the top 20 search results from different search engines\n",
    "What we scrap:\n",
    "* Title\n",
    "* Link\n",
    "* Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efd8d8",
   "metadata": {},
   "source": [
    "###  2-1. Google News (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "697b478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from google news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "    date_elements = soup.find_all('time', class_='hvbAAd')\n",
    "\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 5)):\n",
    "        title = search_results[i].find('a', class_ = 'JtKRv').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        link = 'https://news.google.com' + link[1:]\n",
    "        date = date_elements[i]['datetime'][:10]    \n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3715f7c",
   "metadata": {},
   "source": [
    "#### 2-1-1. Testing Google scrap algorithm\n",
    "#### Getting the queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f10f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])\n",
    "    \n",
    "queries = queries[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477367a",
   "metadata": {},
   "source": [
    "#### Testing with queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test with differnet queries\n",
    "query = input(\"Enter your search query: \")\n",
    "\n",
    "# When we want to use the queries from the excel file:\n",
    "#for query in queries:\n",
    "#    top_results = scrape_google_news(query)\n",
    "#    for index, result in enumerate(top_results, start=1):\n",
    "#        print(f\"{index}. {result['title']}\")\n",
    "#        link = \"https://news.google.com\" + result['link'][1:]\n",
    "#        print(link)\n",
    "#        print()\n",
    "#        print(scrape_content(link))\n",
    "\n",
    "\n",
    "top_results = scrape_google_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    link = result['link']\n",
    "#    link = \"https://news.google.com\" + result['link'][1:]\n",
    "    print(link)\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f22b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8775bf9f",
   "metadata": {},
   "source": [
    "### 2-2. Yahoo News (working)\n",
    "This is very similar to Google one. I think we can easily produce similar functions for other search engines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f86e8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Yahoo news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form\n",
    "########################################################\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "    date_elements = soup.find_all('span', class_='fc-2nd s-time mr-8')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 5)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445576b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_yahoo_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    print(scrape_content(link))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443eeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Google for query: Vessel caught misreporting catch amount\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Sea Shepherd Global', 'Link': 'https://news.google.com/articles/CBMiQWh0dHBzOi8vd3d3LnNlYXNoZXBoZXJkZ2xvYmFsLm9yZy9sYXRlc3QtbmV3cy9jb21iYXQtaXV1LWZpc2hpbmcv0gEA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Wednesday, 05 Jun, 2024 Sea Shepherd Global stands at the forefront of the fight against Illegal, Unreported, and Unregulated (IUU) fishing, deploying innovative strategies and international collaborations to protect marine biodiversity.\\xa0 Illegal, Unreported, and Unregulated (IUU) fishing refers to fishing activities that do not comply with national, regional, or international fisheries conservation and management laws and regulations. These activities are conducted by vessels in various ways, including:'}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'New rules tighten controls on EU…', 'Link': 'https://news.google.com/articles/CBMiZGh0dHBzOi8vZWpmb3VuZGF0aW9uLm9yZy9uZXdzLW1lZGlhL25ldy1ydWxlcy10aWdodGVuLWNvbnRyb2xzLW9uLWV1LXZlc3NlbHMtdG8tcHJldmVudC1taXNyZXBvcnRpbmfSAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Today, the Environmental Justice Foundation commends the European Commission for approving new rules that require stricter controls for landings by EU fishing vessels, providing new tools to prevent significant misreporting of unsorted catches when landing in selected ports, including those in third countries. It is welcome that the new rules require advanced and stricter control tools, such as CCTV to monitor landings, and set minimum benchmarks for the rates of inspection on trans-shipments. If properly implemented, this can increase transparency and accuracy in reporting by EU fleets that catch a large number of species, including those that have been overfished, such as yellowfin tuna in the Indian Ocean. Sean Parramore, Senior EU Advocacy Officer at the Environmental Justice Foundation, said: \"Stricter control measures on landings by EU vessels that have more leeway to misreport their catches are critical to prevent hidden overfishing. In the long run, everyone loses if we open the door to fish fraud. Fish stocks will collapse and the EU\\'s credibility in its efforts to promote sustainable fisheries on the world stage will be severely undermined. These new rules can help prevent that.“'}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'N.S. boat captain, 2 companies fined $125K for fisheries violations', 'Link': 'https://news.google.com/articles/CBMiYGh0dHBzOi8vd3d3LmNiYy5jYS9uZXdzL2NhbmFkYS9ub3ZhLXNjb3RpYS9uLXMtYm9hdC1jYXB0YWluLWZpbmVkLWZpc2hlcmllcy12aW9sYXRpb25zLTEuNjk2NTE5ONIBIGh0dHBzOi8vd3d3LmNiYy5jYS9hbXAvMS42OTY1MTk4?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' A boat captain from Sambro, N.S., with a history of fishery convictions has been fined $60,000 and banned from fishing for six months for five violations that included a secret, middle-of-the-night offload of halibut. The case involved misreporting of halibut, hake and cod catch from trips on board the fishing boat Ivy Lew between May 2019 and June 2020. Casey Henneberry, 40, and ALS Fisheries and Law Fisheries were found guilty last October by Halifax provincial court\\xa0Judge\\xa0Elizabeth Buckle.'}\n",
      "Failed to retrieve content. Status code: 403\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Move the IUU Fight Up the Food Chain | Proceedings - November 2023 Vol. 149/11/1,449', 'Link': 'https://news.google.com/articles/CBMiUmh0dHBzOi8vd3d3LnVzbmkub3JnL21hZ2F6aW5lcy9wcm9jZWVkaW5ncy8yMDIzL25vdmVtYmVyL21vdmUtaXV1LWZpZ2h0LWZvb2QtY2hhaW7SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': None}\n",
      "{'Query': 'Vessel caught misreporting catch amount', 'Engine': 'Google', 'Title': 'Surprise! Media is misreporting the source of a Dutch cargo ship fire', 'Link': 'https://news.google.com/articles/CBMiZGh0dHBzOi8vZWxlY3RyZWsuY28vMjAyMy8wNy8yNi9zdXJwcmlzZS1tZWRpYS1pcy1taXNyZXBvcnRpbmctdGhlLXNvdXJjZS1vZi1hLWR1dGNoLWNhcmdvLXNoaXAtZmlyZS_SAQA?hl=en-CA&gl=CA&ceid=CA%3Aen', 'Content': ' Early this morning, the Fremantle Highway, a vehicle carrying cargo ship, caught fire in the North Sea, off the coast of Ameland in the Netherlands. The fire has killed one person on board and injured several more, though all 23 crew members have at this point been evacuated from the ship. Currently, the fire is still burning and the cause of the fire is unknown, according to the Dutch Coast Guard, which is carrying out the firefighting operation. But media reports, seeming to all crib from the same misquote, would have you think otherwise. The Fremantle Highway is a “roll-on/roll-off,” or “RoRo,” ship, a vehicle carrier designed for cars to drive on and off of it in loading and unloading. According to early reports, it was carrying 2,832 gas-powered cars, complete with a large amount of volatile energy stored in their collective gas tanks (some is needed to drive on and off of the ship) and in the tank of the ship itself, and 25 electric cars, from Germany to Egypt (those numbers have now been updated to 3,783 cars, including 498 EVs – which shows the peril of reporting on things like this with incomplete information).'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m             data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     25\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuery\u001b[39m\u001b[38;5;124m'\u001b[39m: query,\n\u001b[0;32m     26\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngine\u001b[39m\u001b[38;5;124m'\u001b[39m: engine_name,\n\u001b[0;32m     27\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo results\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m             })\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;28mlen\u001b[39m(data)])\n\u001b[1;32m---> 32\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adding a delay to be respectful of website policies\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#### Data frame:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "search_engines = {\n",
    "    'Google': scrape_google_news,\n",
    "    'Yahoo': scrape_yahoo_news,\n",
    "}\n",
    "\n",
    "data = []\n",
    "for query in queries:\n",
    "    for engine_name, scrape_function in search_engines.items():\n",
    "        print(f\"Scraping {engine_name} for query: {query}\")\n",
    "        results = scrape_function(query)\n",
    "        \n",
    "        if results:\n",
    "            for result in results[:len(results)]:  # Limiting to the top 20 results per engine\n",
    "                data.append({\n",
    "                    'Query': query,\n",
    "                    'Engine': engine_name,\n",
    "                    'Title': result['title'],\n",
    "                    'Link': result['link'],\n",
    "                    'Content': scrape_content(result['link'])\n",
    "                })\n",
    "                print(data[len(data)-1])\n",
    "\n",
    "        else:\n",
    "            data.append({\n",
    "                'Query': query,\n",
    "                'Engine': engine_name,\n",
    "                'Title': 'No results'\n",
    "            })\n",
    "            print(data[len(data)])\n",
    "\n",
    "        \n",
    "        time.sleep(1)  # Adding a delay to be respectful of website policies\n",
    "\n",
    "#### Data frame:\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b4064",
   "metadata": {},
   "source": [
    "### 2-3. Microsoft Bing News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49840bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Microsoft Bing news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form like Google\n",
    "########################################################\n",
    "\n",
    "def scrape_bing_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://www.bing.com/news/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='news-card')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 20)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter your search query: \")\n",
    "top_results = scrape_bing_news(query)\n",
    "for index, result in enumerate(top_results, start=1):\n",
    "    print(f\"{index}. {result['title']}\")\n",
    "    print(result['link'])\n",
    "    print(result['date'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653e3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895f351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18304f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
