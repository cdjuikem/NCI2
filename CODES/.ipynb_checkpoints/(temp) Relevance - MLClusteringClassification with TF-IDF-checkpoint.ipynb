{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dc4d9e",
   "metadata": {},
   "source": [
    "### Necessary scrap functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d5b3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all functions in functions.py\n",
    "#from functions import rep_word_text\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a3762",
   "metadata": {},
   "source": [
    "### Read preselected articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55db1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "url_data = pd.read_excel('sample_url.xlsx')\n",
    "\n",
    "\n",
    "relevant_list = []\n",
    "irrelevant_list = []\n",
    "for index, row in url_data.iterrows():\n",
    "    if row['relevance'] == 1:\n",
    "        relevant_list.append(row['url'])\n",
    "    else:\n",
    "        irrelevant_list.append(row['url'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cdf9e",
   "metadata": {},
   "source": [
    "### List of related terminologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe3c72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms_groups and word_forms together\n",
    "\n",
    "word_groups = [\n",
    "    # About documents\n",
    "    ['unreport', 'unreported', 'unreporting', 'unreports'],\n",
    "    ['underreport', 'underreports', 'underreported', 'underreporting'],\n",
    "    ['undocumented', 'undocument', 'undocumenting', 'undocuments'],\n",
    "    ['misrepresent', 'misrepresented', 'misrepresenting', 'misreports'],\n",
    "    ['register', 'registers', 'registered', 'registering', 'unregister', 'unregisters', 'unregistered', 'unregistering'],\n",
    "    ['logbook', 'logged'],\n",
    "    ['declaration', 'declare', 'declares', 'declared', 'declaring', 'underdeclaration', 'under-declaration'],\n",
    "    ['label', 'mislabel', 'labeling', 'labeled', 'labels', 'mislabeling', 'mislabled', 'mislables'],\n",
    "    # About being false\n",
    "    ['false', 'falsify', 'falsifies', 'falsifying', 'falsifies', 'falsified', 'falsification', 'fake', 'manipulated', 'manipulate', 'manipulates'],\n",
    "    ['fraud', 'fraudulence', 'hoax'],\n",
    "    # About amounts\n",
    "    ['volume', 'quota', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'inflation', 'weight'],\n",
    "    # Fish, Ocean, port \n",
    "    ['catch', 'bycatch', 'harvest', 'juvenile', 'invasive'],\n",
    "    ['farmed', 'farming'], \n",
    "    ['ocean', 'sea', 'seas'],\n",
    "    ['port'],\n",
    "    ['transshipment'],\n",
    "    ['landing'],\n",
    "    ['selective'],    \n",
    "#    ['commercial', 'industrial'],\n",
    "    # Crime\n",
    "    ['offend', 'offended', 'offends', 'violate', 'violates', 'violated', 'violation'],\n",
    "    ['regulation', 'regulate', 'regulated', 'regulates', 'regulating', 'regulations', 'regulates'],\n",
    "    ['alter', 'altering', 'alters'],\n",
    "    ['exploit', 'exploited', 'exploiting', 'exploits'],\n",
    "    ['impose', 'imposed', 'imposes', 'imposing'],\n",
    "    ['inspector', 'inspectors', 'inspect', 'inspects', 'inspected', 'inspecting','investigate', 'investigates', 'investigated','investigating','investigator'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2faacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_word(text, group):\n",
    "    if len(group) == 1:\n",
    "        return text\n",
    "    elif len(group) != 1:\n",
    "        updated_text = text\n",
    "        for i in range(1,len(group)):\n",
    "            pattern = r'\\b{}\\b'.format(re.escape(group[i]))\n",
    "            updated_text = re.sub(pattern, group[0], updated_text)\n",
    "        return updated_text\n",
    "    else:\n",
    "        print(\"The word group is empty\")\n",
    "        return None\n",
    "\n",
    "def rep_word_text(text, word_group_list):\n",
    "    if len(word_group_list) != 0:\n",
    "        new_text = text\n",
    "        for i in range(len(word_group_list)):\n",
    "            new_text = clean_word(new_text, word_group_list[i])\n",
    "        return new_text\n",
    "    else:\n",
    "        print(\"the word group list is invalid\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baeb07a",
   "metadata": {},
   "source": [
    "### Can we cluster the articles with these words and TF-IDF?\n",
    "* It was not successfuly before, when we did not use the specific keyword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57fba9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 443)\t0.01545298990375343\n",
      "  (0, 1711)\t0.01545298990375343\n",
      "  (0, 931)\t0.012338232803801781\n",
      "  (0, 333)\t0.01545298990375343\n",
      "  (0, 1015)\t0.01545298990375343\n",
      "  (0, 1862)\t0.01545298990375343\n",
      "  (0, 388)\t0.01545298990375343\n",
      "  (0, 276)\t0.01545298990375343\n",
      "  (0, 1910)\t0.011154057113087992\n",
      "  (0, 643)\t0.013738815374637236\n",
      "  (0, 1836)\t0.01545298990375343\n",
      "  (0, 363)\t0.01545298990375343\n",
      "  (0, 291)\t0.01545298990375343\n",
      "  (0, 381)\t0.01545298990375343\n",
      "  (0, 1023)\t0.01545298990375343\n",
      "  (0, 1719)\t0.01545298990375343\n",
      "  (0, 828)\t0.01545298990375343\n",
      "  (0, 897)\t0.01545298990375343\n",
      "  (0, 1011)\t0.012338232803801781\n",
      "  (0, 1026)\t0.01545298990375343\n",
      "  (0, 1289)\t0.01545298990375343\n",
      "  (0, 19)\t0.01545298990375343\n",
      "  (0, 1691)\t0.01545298990375343\n",
      "  (0, 661)\t0.01545298990375343\n",
      "  (0, 1672)\t0.01545298990375343\n",
      "  :\t:\n",
      "  (9, 867)\t0.011682107099703836\n",
      "  (9, 985)\t0.052874636049512105\n",
      "  (9, 1826)\t0.061687075391097454\n",
      "  (9, 926)\t0.16895782545027854\n",
      "  (9, 1957)\t0.057960774224902485\n",
      "  (9, 326)\t0.1287297717716408\n",
      "  (9, 779)\t0.03876703039272518\n",
      "  (9, 1970)\t0.009660129037483749\n",
      "  (9, 751)\t0.05303882746823663\n",
      "  (9, 1148)\t0.012922343464241726\n",
      "  (9, 283)\t0.011682107099703836\n",
      "  (9, 1292)\t0.09693683275743885\n",
      "  (9, 354)\t0.061687075391097454\n",
      "  (9, 409)\t0.0739965434463764\n",
      "  (9, 1815)\t0.49078225487938054\n",
      "  (9, 1814)\t0.1287297717716408\n",
      "  (9, 774)\t0.08850171809300304\n",
      "  (9, 756)\t0.057960774224902485\n",
      "  (9, 179)\t0.2252771006003714\n",
      "  (9, 273)\t0.04022805367863775\n",
      "  (9, 864)\t0.028980387112451243\n",
      "  (9, 755)\t0.011682107099703836\n",
      "  (9, 1270)\t0.20918587912891629\n",
      "  (9, 1990)\t0.04022805367863775\n",
      "  (9, 789)\t0.04830064518741874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "articles = [ ]\n",
    "\n",
    "for i in range(5):\n",
    "    tmp = scrape_content(relevant_list[i])\n",
    "    tmp = rep_word_text(tmp, word_groups)\n",
    "    articles.append(tmp)\n",
    "    \n",
    "    tmp = scrape_content(irrelevant_list[i])\n",
    "    tmp = rep_word_text(tmp, word_groups)\n",
    "    articles.append(rep_word_text(tmp, word_groups))\n",
    "    \n",
    "\n",
    "\n",
    "#print(articles)\n",
    "#print(dataset)\n",
    "#data = scrape_content(relevant_list[0]).split('.')\n",
    "#data = [scrape_content(relevant_list[0])]\n",
    "#print(data)\n",
    "\n",
    "# fit and tranform your data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_data = vectorizer.fit_transform(articles)\n",
    "\n",
    "print(vectorized_data)\n",
    "\n",
    "\n",
    "# Convert the TF-IDF matrix into a dense representation (if needed)\n",
    "tfidf_matrix = vectorized_data.todense()\n",
    "#print(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c1b3f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "#print(feature_names, type(feature_names))\n",
    "#feature_names_np = np.array(feature_names)\n",
    "for name in feature_names:\n",
    "    if name == 'document':\n",
    "        print(\"yes\")\n",
    "        \n",
    "\n",
    "def keyword_matrix(vectorized_data, word):\n",
    "    if np.where(feature_names == word)[0] is not []:\n",
    "        print(np.where(feature_names == word)[0][0])\n",
    "        word_index = np.where(feature_names == word)[0][0]\n",
    "        return vectorized_data[:,word_index]\n",
    "    else:\n",
    "        word_index = None\n",
    "        return None\n",
    "\n",
    "#print(np.where(feature_names == 'port'))\n",
    "#print(np.where(feature_names == 'document'))\n",
    "\n",
    "#word_index = feature_names.index('port')\n",
    "vectorized_data[:,word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b508d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c19936e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1110441  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04209085 ... 0.         0.         0.        ]\n",
      " [0.02339436 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.0108676  0.         0.         ... 0.         0.         0.01830114]\n",
      " [0.0305778  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.02176135 0.         0.        ]]\n",
      "<class 'numpy.matrix'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(tfidf_matrix)\n",
    "print(type(tfidf_matrix))\n",
    "tfidf_matrix = np.asarray(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Example TF-IDF matrix (tfidf_matrix) obtained from vectorizing your articles\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 2\n",
    "#np.random.seed(42)\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Print cluster assignments\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Document {i + 1} assigned to cluster {cluster + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332587d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ac9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1974b35c",
   "metadata": {},
   "source": [
    "#### What we want to do:\n",
    "We want to train a model with the preselected articles so we can evaluate the level of information of a new article. For evaluation, we want information:\n",
    "\n",
    "* What was the violation\n",
    "    - What kind of fish\n",
    "    - misreported / underreported\n",
    "* Where the incident happend\n",
    "* When the incident happend\n",
    "* Names of people involved\n",
    "* Type of vessels involved\n",
    "* \n",
    "* \n",
    "* (anything else?)\n",
    "\n",
    "I'll start with the first three.\n",
    "\n",
    "* How can a model answer this question?\n",
    "* How should we score each question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1f27e",
   "metadata": {},
   "source": [
    "#### What we want to do is to evaluate how much information the article have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bd160",
   "metadata": {},
   "source": [
    "### NLTK sample (To extract the information we want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9653b5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'you', 'know', 'where', 'a', 'fisherman', 'misreported', 'the', 'amount', 'of', 'salmon', 'catch', '?', 'I', 'want', 'to', 'know', 'about', 'where', ',', 'who', ',', 'how', ',', 'why', ',', 'what', '..']\n",
      "[('Do', 'VB'), ('you', 'PRP'), ('know', 'VB'), ('where', 'WRB'), ('a', 'DT'), ('fisherman', 'NN'), ('misreported', 'VBD'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('salmon', 'NN'), ('catch', 'NN'), ('?', '.'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('where', 'WRB'), (',', ','), ('who', 'WP'), (',', ','), ('how', 'WRB'), (',', ','), ('why', 'WRB'), (',', ','), ('what', 'WP'), ('..', 'VBD')]\n",
      "(S\n",
      "  Do/VB\n",
      "  you/PRP\n",
      "  know/VB\n",
      "  where/WRB\n",
      "  a/DT\n",
      "  fisherman/NN\n",
      "  misreported/VBD\n",
      "  the/DT\n",
      "  amount/NN\n",
      "  of/IN\n",
      "  salmon/NN\n",
      "  catch/NN\n",
      "  ?/.\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  know/VB\n",
      "  about/IN\n",
      "  where/WRB\n",
      "  ,/,\n",
      "  who/WP\n",
      "  ,/,\n",
      "  how/WRB\n",
      "  ,/,\n",
      "  why/WRB\n",
      "  ,/,\n",
      "  what/WP\n",
      "  ../VBD)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "#nltk.download('all')\n",
    "\n",
    "\n",
    "# Sample text\n",
    "text = \"Do you know where a fisherman misreported the amount of salmon catch? I want to know about where, who, how, why, what..\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# Perform NER using ne_chunk\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)\n",
    "# Extract named entities and their labels\n",
    "named_entities = []\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label())\n",
    "        print(\"chunk is :\", chunk)\n",
    "        named_entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))\n",
    "\n",
    "# Print named entities and their labels\n",
    "for entity in named_entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e00b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
