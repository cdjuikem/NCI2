{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580ff700",
   "metadata": {},
   "source": [
    "### Information evaluation:\n",
    "Searching for \n",
    "1. IMO: 7 digit number\n",
    "2. MMSI: 9 digit number\n",
    "3. Ship, Company, or Peopleâ€™s name : information about who is involved (crew/captain/police)\n",
    "4. country's name: it can be offence location / flag / where the offender from etc\n",
    "5. fish names: species involved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354415d",
   "metadata": {},
   "source": [
    "#### 1. Searching IMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4b4097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMO found:\n",
      "1231244\n",
      "2384822\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Here are some numbers: 1234, 12312, 1423124125, 1231241232, 1231244, 2384822, 210392892\"\n",
    "# Regular expression for numbers of exactly 7 digits\n",
    "imo_pattern = r'\\b\\d{7}\\b'\n",
    "\n",
    "# Find all matches in the text\n",
    "imo_matches = re.findall(imo_pattern, text)\n",
    "\n",
    "# Print the matched 7-digit numbers\n",
    "print(\"IMO found:\")\n",
    "if imo_matches:\n",
    "    for match in imo_matches:\n",
    "        print(match)\n",
    "else:\n",
    "    print(\"no IMO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaf90c",
   "metadata": {},
   "source": [
    "#### 2. Searching MMSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c716fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSI found:\n",
      "210392892\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expression for numbers of exactly 7 digits\n",
    "mmsi_pattern = r'\\b\\d{9}\\b'\n",
    "\n",
    "# Find all matches in the text\n",
    "mmsi_matches = re.findall(mmsi_pattern, text)\n",
    "\n",
    "# Print the matched 9-digit numbers\n",
    "print(\"MMSI found:\")\n",
    "if mmsi_matches:\n",
    "    for match in mmsi_matches:\n",
    "        print(match)\n",
    "else:\n",
    "    print(\"no MMSI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e502d48",
   "metadata": {},
   "source": [
    "#### 3. Ship, company, people's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94a80520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names found (using SpaCy): ['Xing Tang', 'John Doe', 'Jane Smith']\n",
      "Company names found: ['Apple Inc.', 'Microsoft Corporation']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def find_names_spacy(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            names.append(ent.text)\n",
    "\n",
    "    return names\n",
    "\n",
    "# Example usage:\n",
    "text = \"Hello there! Xing Tang, My name is John Doe and I work with Jane Smith.\"\n",
    "found_names_spacy = find_names_spacy(text)\n",
    "print(\"Names found (using SpaCy):\", found_names_spacy)\n",
    "\n",
    "\n",
    "def find_company_names(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    company_names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'ORG':  # 'ORG' is the label for organizations\n",
    "            company_names.append(ent.text)\n",
    "\n",
    "    return company_names\n",
    "\n",
    "# Example usage:\n",
    "text = \"Apple Inc. is a leading technology company. Microsoft Corporation is another major player in the industry. We also have a connection with Asian Marine Reefer.\"\n",
    "found_companies = find_company_names(text)\n",
    "print(\"Company names found:\", found_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7903476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON \n",
      "love VERB \n",
      "Italian ADJ NORP\n",
      "food NOUN \n",
      ". PUNCT \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93270c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099080cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1cb819c",
   "metadata": {},
   "source": [
    "#### 4. Country's name / location (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22d89aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am AUX\n",
      "from ADP\n",
      "France PROPN\n",
      ", PUNCT\n",
      "and CCONJ\n",
      "my PRON\n",
      "friend NOUN\n",
      "is AUX\n",
      "from ADP\n",
      "Germany PROPN\n",
      ". PUNCT\n",
      "We PRON\n",
      "love VERB\n",
      "Italian ADJ\n",
      "cuisine NOUN\n",
      ". PUNCT\n",
      "It PRON\n",
      "is AUX\n",
      "delicious ADJ\n",
      ". PUNCT\n",
      "You PRON\n",
      "are AUX\n",
      "French ADJ\n",
      "Country related words found: ['France', 'Germany']\n",
      "I PRON \n",
      "love VERB \n",
      "Italian ADJ NORP\n",
      "food NOUN \n",
      ". PUNCT \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "def find_country_related_words(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    country_related_words = []\n",
    "\n",
    "    for token in doc:\n",
    "        print(token, token.pos_)\n",
    "        if (token.pos_ == 'PROPN' or token.pos == 'NORP') and token.dep_ == 'pobj':  # Check for proper nouns that are objects\n",
    "            country_related_words.append(token.text)\n",
    "        elif token.ent_type_ == 'GPE':  # Check for geopolitical entities (countries, cities, etc.)\n",
    "            country_related_words.append(token.text)\n",
    "\n",
    "    return country_related_words\n",
    "\n",
    "# Example usage:\n",
    "text = \"I am from France, and my friend is from Germany. We love Italian cuisine. It is delicious. You are French\"\n",
    "found_country_words = find_country_related_words(text)\n",
    "print(\"Country related words found:\", found_country_words)\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example text containing \"Italian\"\n",
    "text = \"I love Italian food.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print tokens and their POS tags\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.ent_type_)\n",
    "\n",
    "# Example output:\n",
    "# I PRON \n",
    "# love VERB \n",
    "# Italian ADJ NORP\n",
    "# food NOUN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19bdf99",
   "metadata": {},
   "source": [
    "#### 5. Searching fish names (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e59fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "fish_words = {'salmon', 'tuna', 'shark', 'whale', 'crab', 'lobster', 'shrimp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a3c2668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('fish.n.02')\n",
      "Fish names found in WordNet:\n",
      "fish\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def search_fish_names():\n",
    "    fish_names = []\n",
    "    # Search for synsets related to 'fish'\n",
    "    synsets = wn.synsets('fish', pos=wn.NOUN)\n",
    "    synset = synsets[1]\n",
    "    print(synset)\n",
    "    for lemma in synset.lemmas():\n",
    "        fish_names.append(lemma.name().replace('_', ' '))\n",
    "    return fish_names\n",
    "\n",
    "# Example usage:\n",
    "fish_names = search_fish_names()\n",
    "print(\"Fish names found in WordNet:\")\n",
    "for name in fish_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9eed7b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish names found in WordNet:\n",
      "bony fish\n",
      "bottom-feeder\n",
      "bottom-dweller\n",
      "bottom lurkers\n",
      "cartilaginous fish\n",
      "chondrichthian\n",
      "climbing perch\n",
      "Anabas testudineus\n",
      "A. testudineus\n",
      "fingerling\n",
      "food fish\n",
      "game fish\n",
      "sport fish\n",
      "mouthbreeder\n",
      "northern snakehead\n",
      "rough fish\n",
      "spawner\n",
      "young fish\n",
      "alewife\n",
      "anchovy\n",
      "eel\n",
      "haddock\n",
      "hake\n",
      "mullet\n",
      "grey mullet\n",
      "gray mullet\n",
      "panfish\n",
      "rock salmon\n",
      "salmon\n",
      "schrod\n",
      "scrod\n",
      "shad\n",
      "smelt\n",
      "stockfish\n",
      "trout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sumin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def search_fish_names():\n",
    "    fish_names = []\n",
    "    # Search for synsets directly related to 'fish'\n",
    "    synsets = wn.synsets('fish', pos=wn.NOUN)\n",
    "\n",
    "    for synset in synsets:\n",
    "        # Retrieve hyponyms (more specific terms) of each 'fish' synset\n",
    "        hyponyms = synset.hyponyms()\n",
    "        for hyponym in hyponyms:\n",
    "            # Retrieve lemmas (actual words) of each hyponym\n",
    "            for lemma in hyponym.lemmas():\n",
    "                fish_names.append(lemma.name().replace('_', ' '))\n",
    "\n",
    "    return fish_names\n",
    "\n",
    "# Example usage:\n",
    "fish_names = search_fish_names()\n",
    "print(\"Fish names found in WordNet:\")\n",
    "for name in fish_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e38ee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish names found in WordNet:\n",
      "bony fish\n",
      "crossopterygian\n",
      "lobefin\n",
      "lobe-finned fish\n",
      "lungfish\n",
      "teleost fish\n",
      "teleost\n",
      "teleostan\n",
      "bottom-feeder\n",
      "bottom-dweller\n",
      "mullet\n",
      "bottom lurkers\n",
      "cartilaginous fish\n",
      "chondrichthian\n",
      "elasmobranch\n",
      "selachian\n",
      "holocephalan\n",
      "holocephalian\n",
      "climbing perch\n",
      "Anabas testudineus\n",
      "A. testudineus\n",
      "fingerling\n",
      "food fish\n",
      "barracouta\n",
      "snoek\n",
      "groundfish\n",
      "bottom fish\n",
      "herring\n",
      "Clupea harangus\n",
      "salmon\n",
      "sardine\n",
      "sea bass\n",
      "shad\n",
      "snapper\n",
      "sole\n",
      "trout\n",
      "tuna\n",
      "tunny\n",
      "whitefish\n",
      "game fish\n",
      "sport fish\n",
      "mouthbreeder\n",
      "northern snakehead\n",
      "rough fish\n",
      "spawner\n",
      "young fish\n",
      "brit\n",
      "britt\n",
      "parr\n",
      "parr\n",
      "whitebait\n",
      "alewife\n",
      "anchovy\n",
      "eel\n",
      "elver\n",
      "smoked eel\n",
      "haddock\n",
      "finnan haddie\n",
      "finnan haddock\n",
      "finnan\n",
      "smoked haddock\n",
      "hake\n",
      "mullet\n",
      "grey mullet\n",
      "gray mullet\n",
      "panfish\n",
      "rock salmon\n",
      "salmon\n",
      "Atlantic salmon\n",
      "chinook salmon\n",
      "chinook\n",
      "king salmon\n",
      "kippered salmon\n",
      "red salmon\n",
      "sockeye\n",
      "sockeye salmon\n",
      "silver salmon\n",
      "coho salmon\n",
      "coho\n",
      "cohoe\n",
      "smoked salmon\n",
      "schrod\n",
      "scrod\n",
      "shad\n",
      "smelt\n",
      "American smelt\n",
      "rainbow smelt\n",
      "European smelt\n",
      "sparling\n",
      "stockfish\n",
      "trout\n",
      "rainbow trout\n",
      "sea trout\n",
      "salmon trout\n",
      "angle\n",
      "fly-fish\n",
      "flyfish\n",
      "troll\n",
      "brail\n",
      "crab\n",
      "net fish\n",
      "prawn\n",
      "rail\n",
      "scallop\n",
      "scollop\n",
      "seine\n",
      "shark\n",
      "shrimp\n",
      "still-fish\n",
      "trawl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sumin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def search_fish_names():\n",
    "    fish_names = []\n",
    "    # Search for synsets related to 'fish'\n",
    "    fish_synsets = wn.synsets('fish')\n",
    "\n",
    "    for synset in fish_synsets:\n",
    "        # Retrieve all hyponyms (more specific terms) recursively\n",
    "        hyponyms = synset.hyponyms()\n",
    "        for hyponym in hyponyms:\n",
    "            # Iterate through all lemmas (actual words) of each hyponym\n",
    "            for lemma in hyponym.lemmas():\n",
    "                fish_names.append(lemma.name().replace('_', ' '))\n",
    "\n",
    "            # Additionally, retrieve all hyponyms of each hyponym recursively\n",
    "            nested_hyponyms = hyponym.hyponyms()\n",
    "            for nested_hyponym in nested_hyponyms:\n",
    "                for lemma in nested_hyponym.lemmas():\n",
    "                    fish_names.append(lemma.name().replace('_', ' '))\n",
    "\n",
    "    return fish_names\n",
    "\n",
    "# Example usage:\n",
    "fish_names = search_fish_names()\n",
    "print(\"Fish names found in WordNet:\")\n",
    "for name in fish_names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96dbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
