{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d65d98",
   "metadata": {},
   "source": [
    "# Meeting Notes\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7194f",
   "metadata": {},
   "source": [
    "### <font color = 'blue'> June 14 2024, Friday </font>\n",
    "### What we did:\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f5d38",
   "metadata": {},
   "source": [
    "### <font color = 'blue'> June 13, 2024, Thursday </font>\n",
    "### What we did:\n",
    "#### Variables:\n",
    "Details can be found here in this [link](https://docs.google.com/document/d/1HDImwWFsQENAnyOMTcSLCgJAPG17NqrU1se9HIWstDM/edit?usp=sharing)\n",
    "1. Relevance of Text (NLP)\n",
    "    * We can use Precision to mearsure the relevance\n",
    "        - see Clotilde's note for reference and description\n",
    "        - number of relevant articles / total number of articles\n",
    "                \n",
    "    * WordNet / Merriam-Webster dictionary can identify synonyms and similar words in queries and contents\n",
    "        - Princeton WordNet web (http://wordnetweb.princeton.edu/perl/webwn)\n",
    "            \n",
    "2. SEO - Need a tool such as HRS (Hiva's note has more details)\n",
    "    * How to measure?\n",
    "        \n",
    "3. Date\n",
    "\n",
    "* If use TF-IDF, we can do Clustering / Classification / Machine Learning to let python learn what's relevant\n",
    "\n",
    "\n",
    "### What to do:\n",
    "text analysis (NLP)\n",
    "1. Learn more about TF-IDF\n",
    "2. WordNet / Merriam-Webster dictionary\n",
    "\n",
    "Presentation (Mon/Tue 10 min check-in)\n",
    "    What we did\n",
    "    What we will do\n",
    "    \n",
    "Sumin: integrate date fetching step in the scraping algorithm (done)\n",
    "Make a list of variables and document what's been done for each variable (done):\n",
    "    https://docs.google.com/document/d/1HDImwWFsQENAnyOMTcSLCgJAPG17NqrU1se9HIWstDM/edit?usp=sharing\n",
    "\n",
    "\n",
    "Meeting with Sogol/Sana\n",
    "1. Ask about SEO how to measure it!\n",
    "2. Date as a variable\n",
    "3. Checking in\n",
    "\n",
    "Q. What if we only have less than 20 search results?\n",
    "\n",
    "A: We need to come up with a solution. It can be because of (1) query being too specific, (2) location information\n",
    "##### \n",
    "\n",
    "Q: Do we actually want north American information?\n",
    "\n",
    "A: No. We want more general information, not only for north America\n",
    "##### \n",
    "\n",
    "Q: Can we change query with words in similar meaning word group\n",
    "\n",
    "A: Yes we can, but it's a bit more of group 1's problem.\n",
    "##### \n",
    "\n",
    "Q: Any available NLP library to recognize synonyms/similar words, other than WordNet and Merriam-Webster?\n",
    "\n",
    "A: Sana can help!\n",
    "##### \n",
    "\n",
    "Q: Can stability measurement be a variable?\n",
    "\n",
    "A: Yes! It needs some \n",
    "\n",
    "##### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cb624",
   "metadata": {},
   "source": [
    "### <font color = 'blue'> June 12, 2024 </font>\n",
    "### What to do:\n",
    "Sumin: exploring clustering / how to identify keywords\n",
    "\n",
    "Clotilde: searching for previous works\n",
    "\n",
    "Irushi: reviewing the code / think about variables\n",
    "\n",
    "Hiva: searching about optimization\n",
    "\n",
    "Common:\n",
    "1. Subvariables for relevance of the top 20 results with the Sogol's queries (no need to use all 100 of them for now)\n",
    "    * how to measure how much broad / specific information the articles have.\n",
    "2. How to choose keywords (we need to collect some data from articles) - Sogol has some data?\n",
    "\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209d9cc",
   "metadata": {},
   "source": [
    "Our thoughts on evaluating the search engine results:\n",
    "\n",
    "* I think we need some data to train our algorithm, especially to identify keywords and features of relevant/irrelevant articles. We can easily collect information from news search! (or Sogol and Sana may be able to provide us some information)\n",
    "\n",
    "### Variables (to determine the best/worst search engine):\n",
    "1. The number of relevant articles among top 20 results\n",
    "    * Give weights for the position of the article? (1st result has more weight and 20th result has less)\n",
    "2. how much specific information the relevant articles contain (general info only or specific info included)\n",
    "    * how well the search engines do for Broad / Specific prompts\n",
    "    * misreporting / underreporting any difference in results?\n",
    "3. dates of the articles (how old the articles are)\n",
    "4. how many different incidents show up (not every article is about the same incident)\n",
    "\n",
    "\n",
    "### Techniques for preliminary screen (removing irrelevent articles):\n",
    "1. keyword evaluation\n",
    "    * Frequency of the keywords OR how many keywords included (Clotilde's function) in the title and content (maybe not the whole content if it's long, we can sample some part for efficiency) - Sana's doc also mentioned Term Frequency-Inverse Document Frequency Algorithm (https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/)\n",
    "    * Naive Bayes model (similar to spam email detection, we can detect irrelevant articles) - but this needs the keyword statistics (probability) information / training data\n",
    "    \n",
    "2. Clustering\n",
    "    * Cluster the articles with similarity and sort out irrelevant articles - also need some training data\n",
    "    * Non-numerical data need to be carefully pre-processed to obtain the clusters we want. For example, usually the words should be mapped to numerical values (maybe there's some python library does the job?)\n",
    "    * From Sana's file, it seems like some engines (if not all) make clusters for their results so they can show different results. We can make our own clusters to gather relevant information.\n",
    "\n",
    "\n",
    "### Next thing to think about / questions:\n",
    "1. How to screen 'biased' information\n",
    "    * Penalize certain keywords such as politics, (we need to consider different things)\n",
    "2. Specific information (Vessel, captain's name, where it happend, what happened)\n",
    "    * How to identify them in the articles\n",
    "    * How to measure/compare the amount of information in an article numerically\n",
    "3. About regional biases, are we actually focus on Vancouver/North West America? Or anywhere in the world?\n",
    "4. Can we get certain information like click rate / scroll speed / time on page? (not sure if we would need them though)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b07f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aa460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
