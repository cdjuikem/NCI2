{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dc4d9e",
   "metadata": {},
   "source": [
    "### Necessary scrap functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9d5b3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all functions in functions.py\n",
    "#from functions import rep_word_text\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "528e8d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 3]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2, 3] + [1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a3762",
   "metadata": {},
   "source": [
    "### Read preselected articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "55db1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "url_data = pd.read_excel('sample_url.xlsx')\n",
    "\n",
    "\n",
    "relevant_list = []\n",
    "irrelevant_list = []\n",
    "labels = []\n",
    "for index, row in url_data.iterrows():\n",
    "    if row['relevance'] == 1:\n",
    "        relevant_list.append(row['url'])\n",
    "        labels.append('relevant')\n",
    "    else:\n",
    "        irrelevant_list.append(row['url'])\n",
    "        labels.append('irrelevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "51958a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word list\n",
    "# Terms_groups and word_forms together\n",
    "\n",
    "word_groups = [\n",
    "    # About documents\n",
    "    ['unreport', 'unreported', 'unreporting', 'unreports'],\n",
    "    ['underreport', 'underreports', 'underreported', 'underreporting'],\n",
    "    ['undocumented', 'undocument', 'undocumenting', 'undocuments'],\n",
    "    ['misrepresent', 'misrepresented', 'misrepresenting', 'misreports'],\n",
    "    ['register', 'registers', 'registered', 'registering', 'unregister', 'unregisters', 'unregistered', 'unregistering'],\n",
    "    ['logbook', 'logged'],\n",
    "    ['declaration', 'declare', 'declares', 'declared', 'declaring', 'underdeclaration', 'under-declaration'],\n",
    "    ['label', 'mislabel', 'labeling', 'labeled', 'labels', 'mislabeling', 'mislabled', 'mislables'],\n",
    "    # About being false\n",
    "    ['false', 'falsify', 'falsifies', 'falsifying', 'falsifies', 'falsified', 'falsification', 'fake', 'manipulated', 'manipulate', 'manipulates'],\n",
    "    ['fraud', 'fraudulence', 'hoax'],\n",
    "    # About amounts\n",
    "    ['volume', 'quota', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'inflation', 'weight'],\n",
    "    # Fish, Ocean, port \n",
    "    ['catch', 'bycatch', 'harvest', 'juvenile', 'invasive'],\n",
    "    ['farmed', 'farming'], \n",
    "    ['ocean', 'sea', 'seas'],\n",
    "    ['port'],\n",
    "    ['transshipment'],\n",
    "    ['landing'],\n",
    "    ['selective'],    \n",
    "#    ['commercial', 'industrial'],\n",
    "    # Crime\n",
    "    ['offend', 'offended', 'offends', 'violate', 'violates', 'violated', 'violation'],\n",
    "    ['regulation', 'regulate', 'regulated', 'regulates', 'regulating', 'regulations', 'regulates'],\n",
    "    ['alter', 'altering', 'alters'],\n",
    "    ['exploit', 'exploited', 'exploiting', 'exploits'],\n",
    "    ['impose', 'imposed', 'imposes', 'imposing'],\n",
    "    ['inspector', 'inspectors', 'inspect', 'inspects', 'inspected', 'inspecting','investigate', 'investigates', 'investigated','investigating','investigator'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "918bdc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re \n",
    "\n",
    "def clean_word(text, group):\n",
    "    if len(group) == 1:\n",
    "        return text\n",
    "    elif len(group) != 1:\n",
    "        updated_text = text\n",
    "        for i in range(1,len(group)):\n",
    "            pattern = r'\\b{}\\b'.format(re.escape(group[i]))\n",
    "            updated_text = re.sub(pattern, group[0], updated_text)\n",
    "        return updated_text\n",
    "    else:\n",
    "        print(\"The word group is empty\")\n",
    "        return None\n",
    "\n",
    "def rep_word_text(text, word_group_list):\n",
    "    if len(word_group_list) != 0:\n",
    "        new_text = text\n",
    "        for i in range(len(word_group_list)):\n",
    "            new_text = clean_word(new_text, word_group_list[i])\n",
    "        return new_text\n",
    "    else:\n",
    "        print(\"the word group list is invalid\")\n",
    "        return None\n",
    "\n",
    "#### Article scrap\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "articles = [ ]\n",
    "for i in range(5):\n",
    "    tmp = scrape_content(relevant_list[i])\n",
    "    tmp = rep_word_text(tmp, word_groups)\n",
    "    articles.append(tmp)\n",
    "\n",
    "    tmp = scrape_content(irrelevant_list[i])\n",
    "    tmp = rep_word_text(tmp, word_groups)\n",
    "    articles.append(tmp)\n",
    "\n",
    "print(len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b0af79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text1 = \"\"\"\n",
    "The former director of a major Australian fishing company has faced court in Sydney after being charged over Australia's biggest cocaine haul.\n",
    "\n",
    "Late on Wednesday, the Australian Federal Police announced it had arrested 15 men and seized 1.1 tonnes of cocaine worth an estimated $360 million.\n",
    "\n",
    "The syndicate was allegedly using a fishing trawler to meet a \"mother ship\" from South America to bring the cocaine into Australia.\n",
    "\n",
    "Joseph Pirrello, 63, was arrested in Sydney on Christmas Day.\n",
    "\n",
    "Pirello was formerly a managing director of Seafish Tasmania when the company brought the controversial super trawler Abel Tasman (later named Magiris) to Australia in 2012.\n",
    "\n",
    "He faced court on December 27, was refused bail and his next court date is March 29.\n",
    "\n",
    "Fellow Tasmanian James Colin Collins, 63, was arrested in Rokeby, a suburb of Hobart, on December 27 and extradited to Sydney the next day.\n",
    "\n",
    "Collins is a director of fishing company Lorjona Pty Ltd, which has an office in Hobart.\n",
    "\n",
    "He faced court in Sydney on December 29, was refused bail and will appear in court again on January 11.\n",
    "\n",
    "Police had been monitoring the drug ring for three years.\n",
    "\"\"\"\n",
    "\n",
    "text1 = rep_word_text(text1, word_groups)\n",
    "articles.append(text1)\n",
    "labels.append('relevant')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text2 = \"\"\"\n",
    "Illegal, unreported and unregulated (IUU) fishing is a major contributor to declining fish stocks and marine habitat destruction. Globally, IUU fishing takes many forms both within nationally-controlled waters and on the high seas. While it is not known for sure how much IUU fishing is taking place, it is estimated that IUU fishing accounts for about 30 per cent of all fishing activity worldwide.\n",
    "\n",
    "Plane and CCG boat\n",
    "According to the Food and Agriculture Organization of the United Nations, IUU fishing represents up to 26 million tonnes of fish caught annually, valued at between $10 to $23 billion USD. IUU fishing occurs both on the high seas and within the 200 mile limits of coastal states, especially effecting coastal rural populations in vulnerable areas. Illegal fishing is most prevalent where governance measures to manage fisheries are the weakest, which explains why developing countries are the hardest hit by IUU fishing. An estimated $1 billion in IUU fishing is happening in the coastal waters of sub-Saharan Africa each year.\n",
    "\n",
    "Strong governance of the high seas through regional fisheries management organizations (RFMOs) is integral to reducing illegal fishing activities. An increasing number of RFMOs are using port and trade measures to discourage IUU fishing activity. Measures include not allowing vessels suspected of fishing illegally to dock or unload in a country's port, developing IUU fishing lists of vessels taking part in illegal fishing activities, and scrapping vessels found guilty of multiple illegal fishing offences.\n",
    "\n",
    "Drivers of IUU Fishing\n",
    "United Nations Food and Agriculture Organization's Port State Measures\n",
    "Illegal fishing refers to:\n",
    "\n",
    "Fishing by national or foreign vessels within a country's Exclusive Economic Zone without permission, or, undertaking fishing activities that contravene that country's laws or regulations.\n",
    "Fishing by a vessel flying the flag of a State party to a relevant Regional Fisheries Management Organization (RFMO) that contravenes conservation or management measures adopted by that organization or part of international law.\n",
    "Fishing that violates national laws or international obligations.\n",
    "Unreported fishing refers to:\n",
    "\n",
    "Fishing that has not been reported, or has been misreported, to the relevant national authority or RFMO.\n",
    "Unregulated fishing refers to:\n",
    "\n",
    "Fishing within the regulatory zone of a RFMO of a vessel without a nationality, or by a vessel flying the flag of a state not party to the organization (Flag of convenience), which contravenes the conservation and management measure set out by the RFMO.\n",
    "Fishing outside of regulated zones, which is inconsistent with efforts under international law to conserve living marine resources.\n",
    "\"\"\"\n",
    "\n",
    "text2 = rep_word_text(text2, word_groups)\n",
    "articles.append(text2)\n",
    "labels.append('irrelevant')\n",
    "\n",
    "\n",
    "text3 = \"\"\"\n",
    "The US is one of the largest seafood markets for imported seafood—importing more than 5.3 billion pounds of seafood per year—worth almost $18 billion annually. Unfortunately, there is a problem with the seafood that hits our plates. Right now we simply cannot tell if the fish we eat was legally caught because our current laws are not strong enough to trace from bait to plate.\n",
    "\n",
    "But there is great momentum for change. In December 2016, the US government officially established the Seafood Import Monitoring Program to address illegal, unreported and unregulated (IUU) fishing products entering the market. The new rule, in effect as of Jan. 1, 2018, sets up reporting and record-keeping requirements for certain seafood products to prevent IUU-caught and/or misrepresented seafood from entering US commerce. This first phase of the program applies to an initial list of imported fish and fish products identified as particularly vulnerable to illegal fishing and fraud.\n",
    "\n",
    "With its influence in the market place the US can lead the global IUU fishing challenge by requiring the entire supply chain is fully traceable to legal sources. This will improve practices around the globe by those who hope to access the US market. \n",
    "\"\"\"\n",
    "\n",
    "text3 = rep_word_text(text3, word_groups)\n",
    "articles.append(text3)\n",
    "labels.append('irrelevant')\n",
    "\n",
    "\n",
    "text4 = \"\"\"\n",
    "Twenty-two crewmembers are currently being held in the Maldives awaiting repatriation to Thailand, following the detention of the Thai-owned trawlers CHOTPATTANA 51 and CHOTPATTANA 55 by authorities in the Maldives.\n",
    "\n",
    "The vessels were part of a group of Thai-owned, Djibouti-flagged trawlers labelled the ‘Somali Seven’ by New York Times investigative reporter, Ian Urbina, and were investigated for operating illegally in Somali waters from March to May 2017. The CHOTPATTANA 55, CHOTCHAINAVEE 35, CHOTPATTNA 51, CHAINAVEE 54, CHAINAVEE 55, SUPPHERMNAVEE 21 and CHAICHANACHOKE 8 were all reflagged from Thailand to Djibouti during 2017, most likely in response to stronger regulations and controls put in place by Thai authorities in respect to their flagged fishing vessels.\n",
    "\n",
    "The fleet arrived in the Indian Ocean in January 2017 and operated close to the Puntland coastline with vessel and track analysis indicating they were engaged in trawl fisheries. Under Somali law, trawling is prohibited within 24 nautical miles of the coastline, while the Somali and Puntland fisheries laws also ban the use of trawl gear.\n",
    "\n",
    "After six weeks of operations in the region the fishing vessels were joined by the Honduran flagged WISDOM SEA REEFER, a refrigerated cargo vessel that is known to have the capability to conduct at sea transhipment. A few days after the WISDOM SEA REEFER arrived in the area, four of the fishing vessels, CHAINAVEE 55, CHAINAVEE 54, SUPPHERMNAVEE 21 and CHAICHANACHOKE 8 went “dark” so their positions were no longer transmitted by their AIS units, a known strategy when conducting transhipment that is considered unauthorised or controversial.\n",
    "\n",
    "The WISDOM SEA REEFER has been linked to human trafficking through its operational association with the BLISSFUL REEFER, which was exposed in the 2015 Associated Press coverage of labour abuses in the fisheries sector. In December 2017, the WISDOM SEA REEFER was renamed RENOWN REEFER and reflagged to the landlocked, ‘flag of convenience’ country Bolivia, behaviour often associated with vessels looking to avoid sanctions and oversight.\n",
    "\n",
    "Concern for the welfare of the largely Thai and Cambodian crew of the seven trawlers operating in Somali waters led to intervention by the Thai authorities, and apparent de-flagging by flag State Djibouti.\n",
    "\n",
    "By May 2017, the seven trawlers had largely dispersed; four of the vessels were identified in the port of Suza, Iran. The CHOTCHAINAVEE 35 returned to Samut Sakhon, Thailand, on 6 May 2017 where it was subsequently detained and inspected, due to concern that the vessels were in violation of the national fisheries laws of the Federal Government of Somalia. Charges have now been made against several Thai individuals linked to the vessel through the direct and beneficial ownership companies.\n",
    "\n",
    "January 2018 saw the detention of a further two of the ‘Somali Seven’ by authorities in the Maldives. The CHOTPATTANA 51 and CHOTPATTANA 55, both thought to be stateless, had a total of 400 tons of fish and 22 Thai nationals on board.Wanted by Thai authorities, and monitored by INTERPOL, the vessels were linked through ownership and crew to other vessels from the Somali Seven fleet. Some members of the original CHOTPATTANA 55 crew had been interviewed by Thai officials on the CHOTCHAINAVEE 35 and reported that they had changed vessels to escape the living and working conditions they had experienced and to return home to Thailand.\n",
    "\n",
    "Per Erik Bergh of Stop Illegal Fishing commented, “It is very concerning to see that the strengthening of policy and enforcement in one region prompts operators, who are determined to violate human rights and fishing regulations, to seek out regions and countries where there is less control. Foreign fishing vessels continue to take advantage of the situation in Somalia, and as an international community, we must make every effort to stop this from happening.”\n",
    "\n",
    "FISH-i Africa continues to monitor the activity of the remaining ‘Somali Seven’ vessels and calls for support from all port States in ensuring that any labour abuses or human trafficking is identified and the crew are fully supported. Nicholas Ntheketha, Chair of FISH-i stated, “In the Western Indian Ocean we are very concerned to see the deliberate and illegal targeting of our fisheries by operators who clearly make no effort to respect our rules, regulations or laws. The use of trawl gear so close to the Somali coastline may have devastating impacts on the environment and also on the Somali people.”\n",
    "\"\"\"\n",
    "\n",
    "text4 = rep_word_text(text4, word_groups)\n",
    "articles.append(text4)\n",
    "labels.append('relevant')\n",
    "\n",
    "\n",
    "\n",
    "# fit and tranform your data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_data = vectorizer.fit_transform(articles)\n",
    "\n",
    "\n",
    "# Convert the TF-IDF matrix into a dense representation (if needed)\n",
    "tfidf_matrix = vectorized_data.todense()\n",
    "#print(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5df0ab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['relevant', 'relevant', 'relevant', 'relevant', 'relevant', 'irrelevant', 'irrelevant', 'irrelevant', 'irrelevant', 'irrelevant', 'relevant', 'irrelevant', 'relevant', 'irrelevant', 'relevant', 'irrelevant', 'irrelevant', 'relevant', 'irrelevant', 'irrelevant', 'relevant', 'relevant', 'irrelevant', 'irrelevant', 'relevant']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example data\n",
    "#articles = [\"This is article 1 text\", \"Another article with different text\", ...]\n",
    "\n",
    "print(len(labels), labels)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=100)  # Limiting to top 1000 features\n",
    "\n",
    "# Fit and transform the vectorizer on the text data\n",
    "X = vectorizer.fit_transform(articles)\n",
    "\n",
    "# Y contains the labels (categories)\n",
    "Y = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fc5b2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e91e8cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "37ea8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "29769271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       1.00      0.75      0.86         4\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.75      0.88      0.76         5\n",
      "weighted avg       0.90      0.80      0.82         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e1516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c30cdf9e",
   "metadata": {},
   "source": [
    "### List of related terminologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe3c72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms_groups and word_forms together\n",
    "\n",
    "word_groups = [\n",
    "    # About documents\n",
    "    ['unreport', 'unreported', 'unreporting', 'unreports'],\n",
    "    ['underreport', 'underreports', 'underreported', 'underreporting'],\n",
    "    ['undocumented', 'undocument', 'undocumenting', 'undocuments'],\n",
    "    ['misrepresent', 'misrepresented', 'misrepresenting', 'misreports'],\n",
    "    ['register', 'registers', 'registered', 'registering', 'unregister', 'unregisters', 'unregistered', 'unregistering'],\n",
    "    ['logbook', 'logged'],\n",
    "    ['declaration', 'declare', 'declares', 'declared', 'declaring', 'underdeclaration', 'under-declaration'],\n",
    "    ['label', 'mislabel', 'labeling', 'labeled', 'labels', 'mislabeling', 'mislabled', 'mislables'],\n",
    "    # About being false\n",
    "    ['false', 'falsify', 'falsifies', 'falsifying', 'falsifies', 'falsified', 'falsification', 'fake', 'manipulated', 'manipulate', 'manipulates'],\n",
    "    ['fraud', 'fraudulence', 'hoax'],\n",
    "    # About amounts\n",
    "    ['volume', 'quota', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'inflation', 'weight'],\n",
    "    # Fish, Ocean, port \n",
    "    ['catch', 'bycatch', 'harvest', 'juvenile', 'invasive'],\n",
    "    ['farmed', 'farming'], \n",
    "    ['ocean', 'sea', 'seas'],\n",
    "    ['port'],\n",
    "    ['transshipment'],\n",
    "    ['landing'],\n",
    "    ['selective'],    \n",
    "#    ['commercial', 'industrial'],\n",
    "    # Crime\n",
    "    ['offend', 'offended', 'offends', 'violate', 'violates', 'violated', 'violation'],\n",
    "    ['regulation', 'regulate', 'regulated', 'regulates', 'regulating', 'regulations', 'regulates'],\n",
    "    ['alter', 'altering', 'alters'],\n",
    "    ['exploit', 'exploited', 'exploiting', 'exploits'],\n",
    "    ['impose', 'imposed', 'imposes', 'imposing'],\n",
    "    ['inspector', 'inspectors', 'inspect', 'inspects', 'inspected', 'inspecting','investigate', 'investigates', 'investigated','investigating','investigator'],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a1082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_word(text, group):\n",
    "    if len(group) == 1:\n",
    "        return text\n",
    "    elif len(group) != 1:\n",
    "        updated_text = text\n",
    "        for i in range(1,len(group)):\n",
    "            pattern = r'\\b{}\\b'.format(re.escape(group[i]))\n",
    "            updated_text = re.sub(pattern, group[0], updated_text)\n",
    "        return updated_text\n",
    "    else:\n",
    "        print(\"The word group is empty\")\n",
    "        return None\n",
    "\n",
    "def rep_word_text(text, word_group_list):\n",
    "    if len(word_group_list) != 0:\n",
    "        new_text = text\n",
    "        for i in range(len(word_group_list)):\n",
    "            new_text = clean_word(new_text, word_group_list[i])\n",
    "        return new_text\n",
    "    else:\n",
    "        print(\"the word group list is invalid\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baeb07a",
   "metadata": {},
   "source": [
    "### Can we cluster the articles with these words and TF-IDF?\n",
    "* It was not successfuly before, when we did not use the specific keyword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "57fba9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\sumin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers:\n",
      "[[ 0.5236717   0.80375288]\n",
      " [ 0.95128587 -0.1778818 ]]\n",
      "Document 1 - Predicted Cluster: 0\n",
      "Document 2 - Predicted Cluster: 1\n",
      "Document 3 - Predicted Cluster: 1\n",
      "Document 4 - Predicted Cluster: 1\n",
      "Document 5 - Predicted Cluster: 1\n",
      "Document 6 - Predicted Cluster: 0\n",
      "Document 7 - Predicted Cluster: 0\n",
      "Document 8 - Predicted Cluster: 1\n",
      "Document 9 - Predicted Cluster: 1\n",
      "Document 10 - Predicted Cluster: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Example text data (replace this with your own dataset)\n",
    "documents = articles\n",
    "\n",
    "# Example set of features (replace this with your own set of features)\n",
    "selected_features = [word_list[0] for word_list in word_groups]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with selected features\n",
    "vectorizer = TfidfVectorizer(vocabulary=selected_features)\n",
    "\n",
    "# Fit and transform the TF-IDF vectorizer\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Reduce dimensionality for clustering (optional, using TruncatedSVD for example)\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X_reduced = lsa.fit_transform(X)\n",
    "\n",
    "# Perform clustering (K-means clustering for example)\n",
    "num_clusters = 2  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X_reduced)\n",
    "\n",
    "# Print cluster centers (optional)\n",
    "print(\"Cluster centers:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Print predicted clusters for each document\n",
    "labels = kmeans.labels_\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i+1} - Predicted Cluster: {labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0be264f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 443)\t0.01545298990375343\n",
      "  (0, 1711)\t0.01545298990375343\n",
      "  (0, 931)\t0.012338232803801781\n",
      "  (0, 333)\t0.01545298990375343\n",
      "  (0, 1015)\t0.01545298990375343\n",
      "  (0, 1862)\t0.01545298990375343\n",
      "  (0, 388)\t0.01545298990375343\n",
      "  (0, 276)\t0.01545298990375343\n",
      "  (0, 1910)\t0.011154057113087992\n",
      "  (0, 643)\t0.013738815374637236\n",
      "  (0, 1836)\t0.01545298990375343\n",
      "  (0, 363)\t0.01545298990375343\n",
      "  (0, 291)\t0.01545298990375343\n",
      "  (0, 381)\t0.01545298990375343\n",
      "  (0, 1023)\t0.01545298990375343\n",
      "  (0, 1719)\t0.01545298990375343\n",
      "  (0, 828)\t0.01545298990375343\n",
      "  (0, 897)\t0.01545298990375343\n",
      "  (0, 1011)\t0.012338232803801781\n",
      "  (0, 1026)\t0.01545298990375343\n",
      "  (0, 1289)\t0.01545298990375343\n",
      "  (0, 19)\t0.01545298990375343\n",
      "  (0, 1691)\t0.01545298990375343\n",
      "  (0, 661)\t0.01545298990375343\n",
      "  (0, 1672)\t0.01545298990375343\n",
      "  :\t:\n",
      "  (9, 867)\t0.011682107099703836\n",
      "  (9, 985)\t0.052874636049512105\n",
      "  (9, 1826)\t0.061687075391097454\n",
      "  (9, 926)\t0.16895782545027854\n",
      "  (9, 1957)\t0.057960774224902485\n",
      "  (9, 326)\t0.1287297717716408\n",
      "  (9, 779)\t0.03876703039272518\n",
      "  (9, 1970)\t0.009660129037483749\n",
      "  (9, 751)\t0.05303882746823663\n",
      "  (9, 1148)\t0.012922343464241726\n",
      "  (9, 283)\t0.011682107099703836\n",
      "  (9, 1292)\t0.09693683275743885\n",
      "  (9, 354)\t0.061687075391097454\n",
      "  (9, 409)\t0.0739965434463764\n",
      "  (9, 1815)\t0.49078225487938054\n",
      "  (9, 1814)\t0.1287297717716408\n",
      "  (9, 774)\t0.08850171809300304\n",
      "  (9, 756)\t0.057960774224902485\n",
      "  (9, 179)\t0.2252771006003714\n",
      "  (9, 273)\t0.04022805367863775\n",
      "  (9, 864)\t0.028980387112451243\n",
      "  (9, 755)\t0.011682107099703836\n",
      "  (9, 1270)\t0.20918587912891629\n",
      "  (9, 1990)\t0.04022805367863775\n",
      "  (9, 789)\t0.04830064518741874\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m vectorized_data \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(articles) \n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorized_data)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vectorized_data))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#word_index = feature_names.index('port')\u001b[39;00m\n\u001b[0;32m     46\u001b[0m keyword_matrix(vectorizer, vectorized_data, word_groups)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:340\u001b[0m, in \u001b[0;36m_spbase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse array length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "#print(feature_names, type(feature_names))\n",
    "#feature_names_np = np.array(feature_names)\n",
    "\n",
    "\n",
    "\n",
    "def word_vector(vectorizer, vectorized_data, word):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_search = np.where(feature_names == word)[0]\n",
    "    if len(word_search) != 0:\n",
    "#        print(word_search)\n",
    "        word_index = word_search[0]\n",
    "        word_vector = vectorized_data[:,word_index]\n",
    "#        print(word_vector.todense())\n",
    "#        print(type(word_vector.todense()))\n",
    "        print(word_vector.todense())\n",
    "        return word_vector.todense()\n",
    "    else:\n",
    "        word_index = None\n",
    "        return None\n",
    "\n",
    "def keyword_matrix(vectorizer, vectorized_data, word_group_list):\n",
    "    list_length = len(word_group_list)\n",
    "    if list_length != 0:\n",
    "        vector_list = []\n",
    "        for i in range(list_length):\n",
    "            word = word_group_list[i][0]\n",
    "            print(\"This time, the word is : \", word)\n",
    "            word_vec = word_vector(vectorizer, vectorized_data, word)\n",
    "            #print(word_vec.todense())\n",
    "            vector_list.append(word_vector)\n",
    "        keyword_matrix = np.array(vector_list)\n",
    "    else:\n",
    "        return None #np.zeros((3, 3))\n",
    "    \n",
    "#print(np.where(feature_names == 'port'))\n",
    "#print(np.where(feature_names == 'document'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_data = vectorizer.fit_transform(articles) \n",
    "\n",
    "#word_index = feature_names.index('port')\n",
    "keyword_matrix(vectorizer, vectorized_data, word_groups)\n",
    "\n",
    "print(keyword_matrix(vectorizer, vectorized_data, word_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fb622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c19936e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1110441  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04209085 ... 0.         0.         0.        ]\n",
      " [0.02339436 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.0108676  0.         0.         ... 0.         0.         0.01830114]\n",
      " [0.0305778  0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.02176135 0.         0.        ]]\n",
      "<class 'numpy.matrix'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(tfidf_matrix)\n",
    "print(type(tfidf_matrix))\n",
    "tfidf_matrix = np.asarray(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Example TF-IDF matrix (tfidf_matrix) obtained from vectorizing your articles\n",
    "\n",
    "# Number of clusters\n",
    "num_clusters = 2\n",
    "#np.random.seed(42)\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Print cluster assignments\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Document {i + 1} assigned to cluster {cluster + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad990d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332587d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ac9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1974b35c",
   "metadata": {},
   "source": [
    "#### What we want to do:\n",
    "We want to train a model with the preselected articles so we can evaluate the level of information of a new article. For evaluation, we want information:\n",
    "\n",
    "* What was the violation\n",
    "    - What kind of fish\n",
    "    - misreported / underreported\n",
    "* Where the incident happend\n",
    "* When the incident happend\n",
    "* Names of people involved\n",
    "* Type of vessels involved\n",
    "* \n",
    "* \n",
    "* (anything else?)\n",
    "\n",
    "I'll start with the first three.\n",
    "\n",
    "* How can a model answer this question?\n",
    "* How should we score each question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1f27e",
   "metadata": {},
   "source": [
    "#### What we want to do is to evaluate how much information the article have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bd160",
   "metadata": {},
   "source": [
    "### NLTK sample (To extract the information we want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9653b5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'you', 'know', 'where', 'a', 'fisherman', 'misreported', 'the', 'amount', 'of', 'salmon', 'catch', '?', 'I', 'want', 'to', 'know', 'about', 'where', ',', 'who', ',', 'how', ',', 'why', ',', 'what', '..']\n",
      "[('Do', 'VB'), ('you', 'PRP'), ('know', 'VB'), ('where', 'WRB'), ('a', 'DT'), ('fisherman', 'NN'), ('misreported', 'VBD'), ('the', 'DT'), ('amount', 'NN'), ('of', 'IN'), ('salmon', 'NN'), ('catch', 'NN'), ('?', '.'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('where', 'WRB'), (',', ','), ('who', 'WP'), (',', ','), ('how', 'WRB'), (',', ','), ('why', 'WRB'), (',', ','), ('what', 'WP'), ('..', 'VBD')]\n",
      "(S\n",
      "  Do/VB\n",
      "  you/PRP\n",
      "  know/VB\n",
      "  where/WRB\n",
      "  a/DT\n",
      "  fisherman/NN\n",
      "  misreported/VBD\n",
      "  the/DT\n",
      "  amount/NN\n",
      "  of/IN\n",
      "  salmon/NN\n",
      "  catch/NN\n",
      "  ?/.\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  know/VB\n",
      "  about/IN\n",
      "  where/WRB\n",
      "  ,/,\n",
      "  who/WP\n",
      "  ,/,\n",
      "  how/WRB\n",
      "  ,/,\n",
      "  why/WRB\n",
      "  ,/,\n",
      "  what/WP\n",
      "  ../VBD)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "#nltk.download('all')\n",
    "\n",
    "\n",
    "# Sample text\n",
    "text = \"Do you know where a fisherman misreported the amount of salmon catch? I want to know about where, who, how, why, what..\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)\n",
    "\n",
    "# Perform NER using ne_chunk\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)\n",
    "# Extract named entities and their labels\n",
    "named_entities = []\n",
    "for chunk in ner_tags:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label())\n",
    "        print(\"chunk is :\", chunk)\n",
    "        named_entities.append((chunk.label(), ' '.join(c[0] for c in chunk)))\n",
    "\n",
    "# Print named entities and their labels\n",
    "for entity in named_entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e00b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
