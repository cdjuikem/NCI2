{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dc4d9e",
   "metadata": {},
   "source": [
    "### Necessary scrap functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the contents in the url\n",
    "#### input: url\n",
    "#### output: content (type: str)\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_content(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the desired information\n",
    "        # Example: Extracting all paragraphs from the webpage\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        content = \"\"\n",
    "        # Print or process the extracted information\n",
    "#        for paragraph in paragraphs:\n",
    "#            content += \" \" + paragraph.text\n",
    "\n",
    "###### The below is to only return a shorter content. For the full content, use the commented commands above\n",
    "        for i in range(min(len(paragraphs), 3)):\n",
    "### For the whole content:\n",
    "#        for i in range(len(paragraphs)):\n",
    "            content += \" \" + paragraphs[i].text\n",
    "        return content\n",
    "    else:\n",
    "        print(\"Failed to retrieve content. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from google news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "########################################################\n",
    "\n",
    "def scrape_google_news(query):\n",
    "    # Construct the Google News URL with the query\n",
    "    url = f\"https://news.google.com/search?q={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='IL9Cne')\n",
    "    date_elements = soup.find_all('time', class_='hvbAAd')\n",
    "\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 20)):\n",
    "        title = search_results[i].find('a', class_ = 'JtKRv').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i]['datetime'][:10]    \n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "########################################################\n",
    "#### This function scrap the titles, links, and dates of the first 20 articles from Yahoo news with the given query\n",
    "#### input: query\n",
    "#### output: the list of the title and link\n",
    "####\n",
    "#### required libraries:\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "#\n",
    "# Note: The dates in Yahoo news is in different form.\n",
    "# It only gives info in the form of \"1 month ago\", \"3 hours ago\", not YYYY-MM-DD form\n",
    "########################################################\n",
    "\n",
    "def scrape_yahoo_news(query):\n",
    "    # Construct the Yahoo News URL with the query\n",
    "    url = f\"https://news.search.yahoo.com/search?p={query}\"\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the search result elements\n",
    "    search_results = soup.find_all('div', class_='NewsArticle')\n",
    "    date_elements = soup.find_all('span', class_='fc-2nd s-time mr-8')\n",
    "\n",
    "    # Extract the title and link of each search result\n",
    "    scrap = []\n",
    "    for i in range(min(len(search_results), 20)):\n",
    "        title = search_results[i].find('h4').text\n",
    "        link = search_results[i].find('a')['href']\n",
    "        date = date_elements[i].text[2:]\n",
    "        scrap.append({'title': title, 'link': link, 'date': date})   \n",
    "    return scrap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61319288",
   "metadata": {},
   "source": [
    "### Read queries from the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a3762",
   "metadata": {},
   "source": [
    "### Read preselected articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "url_data = pd.read_excel('sample_url.xlsx')\n",
    "\n",
    "\n",
    "relevant_list = []\n",
    "irrelevant_list = []\n",
    "for index, row in url_data.iterrows():\n",
    "    if row['relevance'] == 1:\n",
    "        relevant_list.append(row['url'])\n",
    "    else:\n",
    "        irrelevant_list.append(row['url'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1974b35c",
   "metadata": {},
   "source": [
    "#### What we want to do:\n",
    "We want to train a model with the preselected articles so we can evaluate the level of information of a new article. For evaluation, we want information:\n",
    "\n",
    "* What was the violation\n",
    "    - What kind of fish\n",
    "    - misreported / underreported\n",
    "* Where the incident happend\n",
    "* When the incident happend\n",
    "* Names of people involved\n",
    "* Type of vessels involved\n",
    "* \n",
    "* \n",
    "* (anything else?)\n",
    "\n",
    "I'll start with the first three.\n",
    "\n",
    "* How can a model answer this question?\n",
    "* How should we score each question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1f27e",
   "metadata": {},
   "source": [
    "#### What we want to do is to evaluate how much information the article have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653b5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e00b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
