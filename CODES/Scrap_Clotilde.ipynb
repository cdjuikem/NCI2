{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d038e90",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['ocean', 'sea', 'crime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7592a4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fc983",
   "metadata": {},
   "source": [
    "## Search Engine Prompts for \"Underreporting/Misreporting of Catch\"\n",
    "\n",
    "1. \"underreporting catch maritime incidents\"\n",
    "2. \"misreporting catch incidents at sea\"\n",
    "3. \"cases of underreporting catch in the maritime industry\"\n",
    "4. \"catch reporting issues in fishing\"\n",
    "5. \"scandals of misreporting catch in the maritime industry\"\n",
    "6. \"investigations into underreporting catch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ba38698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search query: etudes sur la desinformations des noirs au Canada \n",
      "1. Une étude sur l'incidence de la désinformation chez les ...\n",
      "   Link: https://ici.radio-canada.ca/info/videos/1-8896282/une-etude-sur-incidence-desinformation-chez-jeunes-noirs\n",
      "\n",
      "2. Expériences de discrimination chez les Noirs et les ...\n",
      "   Link: https://www150.statcan.gc.ca/n1/pub/85-002-x/2022001/article/00002-fra.htm\n",
      "\n",
      "3. Événements marquants de l'histoire des personnes noires ...\n",
      "   Link: https://www.canada.ca/fr/patrimoine-canadien/campagnes/mois-histoire-des-noirs/communautes-historiques-noires.html\n",
      "\n",
      "4. Contexte – Les jeunes provenant des communautés noires ...\n",
      "   Link: https://www.justice.gc.ca/fra/pr-rp/jr/yncjs-bycjs/contexte-background.html\n",
      "\n",
      "5. Lutter contre la désinformation étrangère et la manipulation ...\n",
      "   Link: https://www.international.gc.ca/world-monde/issues_development-enjeux_developpement/peace_security-paix_securite/combatt-disinformation-desinformation.aspx?lang=fra\n",
      "\n",
      "6. Ségrégation raciale des Noirs au Canada\n",
      "   Link: https://www.thecanadianencyclopedia.ca/fr/article/segregation-raciale-des-noirs-au-canada\n",
      "\n",
      "7. Mois de l'histoire des Noirs : Affronter les préjugés et faire ...\n",
      "   Link: https://www.chrc-ccdp.gc.ca/fr/ressources/mois-de-lhistoire-des-noirs-affronter-les-prejuges-et-faire-place-a-la-diversite\n",
      "\n",
      "8. Le Mois de l'histoire des Noirs 2023... en chiffres\n",
      "   Link: https://www.statcan.gc.ca/fr/dai/smr08/2023/smr08_270\n",
      "\n",
      "9. Alberta - Mois de l'histoire des Noirs (février)\n",
      "   Link: https://teachers-ab.libguides.com/histoiredesnoirs/alberta\n",
      "\n",
      "10. Rapport de veille – Édition Février 2023\n",
      "   Link: https://icea.qc.ca/sites/icea.qc.ca/files/Fevrier2023_Veille-informationnelle-education-adultes_ICEA.pdf\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 2, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }  # User-Agent header to mimic a browser\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            search_results.append({'title': title, 'link': link})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "##Clotilde's function for relevance evaluation\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convertir le titre en minuscule pour une comparaison insensible à la casse\n",
    "        score = sum(1 for word in keywords if word in title)  # Compter combien de mots-clés apparaissent dans le titre\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "####\n",
    "\n",
    "# Example usage:\n",
    "query = input(\"Enter your search query: \")\n",
    "results = scrape_search_results(query)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results, start=1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Link: {result['link']}\")\n",
    "        print()\n",
    "\n",
    "evaluate_relevance(results, ['desinformations', 'noirs', 'canada'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89c67ce4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\cdjuk\\\\OneDrive\\\\Desktop\\\\Cours UoM\\\\PROJET CTMC\\\\NCI2\\\\CODESCODES\\\\PIMS_Sample_Prompts.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m queries_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcdjuk\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCours UoM\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPROJET CTMC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNCI2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCODESCODES\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPIMS_Sample_Prompts.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\cdjuk\\\\OneDrive\\\\Desktop\\\\Cours UoM\\\\PROJET CTMC\\\\NCI2\\\\CODESCODES\\\\PIMS_Sample_Prompts.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries_file = r'C:\\Users\\cdjuk\\OneDrive\\Desktop\\Cours UoM\\PROJET CTMC\\NCI2\\CODESCODES\\PIMS_Sample_Prompts.xlsx'\n",
    "queries = pd.read_excel(queries_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "897dc437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Title</th>\n",
       "      <th>Relevance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Northwest Atlantic Fisheries Organization Noti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>How Sea Shepherd Combats Illegal Fishing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Saithe misreporting prosecution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>N.S. boat captain, 2 companies fined $125K for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Major halibut fishing investigation leads to d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Illegal fishing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Pacific-licensed vessels top IUU fishing list ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Pelagic trawlers report false catch figures an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Estimating Illegal and Unreported Catches from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Catch Me If You Can: The Global Pursuit of a F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Coast Guard seizes Chinese fishing boat over f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>The rise and fall of the Codfather, North Amer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Pelagic trawlers report false catch figures an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Chinese boats using fake certificates to fish ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Maine fishermen take plea deal for herring fraud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Catch Me If You Can: The Global Pursuit of a F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Indonesia catches illegal Philippine fishing v...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Alaska Fisherman Sentenced to Federal Prison a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>'The Codfather' Carlos Rafael ordered to pay $...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vessel caught with incorrect catch reports</td>\n",
       "      <td>Superior fisherman fined for bad paperwork</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Query  \\\n",
       "0      Vessel caught misreporting catch amount   \n",
       "1      Vessel caught misreporting catch amount   \n",
       "2      Vessel caught misreporting catch amount   \n",
       "3      Vessel caught misreporting catch amount   \n",
       "4      Vessel caught misreporting catch amount   \n",
       "5      Vessel caught misreporting catch amount   \n",
       "6      Vessel caught misreporting catch amount   \n",
       "7      Vessel caught misreporting catch amount   \n",
       "8      Vessel caught misreporting catch amount   \n",
       "9      Vessel caught misreporting catch amount   \n",
       "10       Vessel caught falsifying fishing logs   \n",
       "11       Vessel caught falsifying fishing logs   \n",
       "12       Vessel caught falsifying fishing logs   \n",
       "13       Vessel caught falsifying fishing logs   \n",
       "14       Vessel caught falsifying fishing logs   \n",
       "15       Vessel caught falsifying fishing logs   \n",
       "16       Vessel caught falsifying fishing logs   \n",
       "17       Vessel caught falsifying fishing logs   \n",
       "18       Vessel caught falsifying fishing logs   \n",
       "19  Vessel caught with incorrect catch reports   \n",
       "\n",
       "                                                Title  Relevance Score  \n",
       "0   Northwest Atlantic Fisheries Organization Noti...                0  \n",
       "1            How Sea Shepherd Combats Illegal Fishing                2  \n",
       "2                     Saithe misreporting prosecution                0  \n",
       "3   N.S. boat captain, 2 companies fined $125K for...                0  \n",
       "4   Major halibut fishing investigation leads to d...                1  \n",
       "5                                     Illegal fishing                2  \n",
       "6   Pacific-licensed vessels top IUU fishing list ...                1  \n",
       "7   Pelagic trawlers report false catch figures an...                0  \n",
       "8   Estimating Illegal and Unreported Catches from...                1  \n",
       "9   Catch Me If You Can: The Global Pursuit of a F...                0  \n",
       "10  Coast Guard seizes Chinese fishing boat over f...                1  \n",
       "11  The rise and fall of the Codfather, North Amer...                0  \n",
       "12  Pelagic trawlers report false catch figures an...                0  \n",
       "13  Chinese boats using fake certificates to fish ...                0  \n",
       "14   Maine fishermen take plea deal for herring fraud                1  \n",
       "15  Catch Me If You Can: The Global Pursuit of a F...                0  \n",
       "16  Indonesia catches illegal Philippine fishing v...                2  \n",
       "17  Alaska Fisherman Sentenced to Federal Prison a...                0  \n",
       "18  'The Codfather' Carlos Rafael ordered to pay $...                0  \n",
       "19         Superior fisherman fined for bad paperwork                0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape search results from Google\n",
    "def scrape_search_results(query):\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = []\n",
    "        for result in soup.find_all('div', class_='tF2Cxc'):\n",
    "            title = result.find('h3').text\n",
    "            link = result.find('a')['href']\n",
    "            # Attempt to find the publication date (if available)\n",
    "            date_span = result.find('span', class_='MUxGbd wuQ4Ob WZ8Tjf')\n",
    "            date = date_span.text if date_span else \"Date not found\"\n",
    "            search_results.append({'title': title, 'link': link, 'date': date})\n",
    "        return search_results\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "        return None\n",
    "\n",
    "# Function to evaluate relevance of results\n",
    "def evaluate_relevance(results, keywords):\n",
    "    relevance_scores = []\n",
    "    for result in results:\n",
    "        title = result[\"title\"].lower()  # Convert title to lowercase for case-insensitive comparison\n",
    "        score = sum(1 for word in keywords if word in title)  # Count how many keywords appear in the title\n",
    "        relevance_scores.append(score)\n",
    "    return relevance_scores\n",
    "\n",
    "# List of queries extracted from the image\n",
    "queries = [\n",
    "    \"Vessel caught misreporting catch amount\",\n",
    "    \"Vessel caught falsifying fishing logs\",\n",
    "    \"Vessel caught with incorrect catch reports\",\n",
    "    \"Vessel caught falsifying catch during transport\",\n",
    "    \"Vessel caught misreporting catch for fishing competition\",\n",
    "    \"Vessel caught misreporting catch for quota manipulation\"\n",
    "]\n",
    "\n",
    "# Read queries from Excel file\n",
    "#def read_queries_from_excel(file_path):\n",
    "   # df = pd.read_excel(file_path)\n",
    "  #  return df['Query'].tolist()\n",
    "\n",
    "# Excel file containing the queries\n",
    "#queries_file = 'PIMS_Sample_Prompts.xlsx'\n",
    "\n",
    "# Read queries from the Excel file\n",
    "#queries =pd.read_excel(\"CODES/PIMS_Sample_Prompts.xlsx\")\n",
    "\n",
    "# Keywords for relevance evaluation\n",
    "keywords = ['fishing', 'illegal', 'fraud', 'quota', 'inspection',\n",
    "            'violation', 'regulation', 'sustainability', 'penalty', 'monitoring',\n",
    "           'ocean', 'crime', ]\n",
    "\n",
    "# Data storage\n",
    "data = []\n",
    "\n",
    "# Loop through each query and process the search results\n",
    "for query in queries:\n",
    "    #print(f\"Search results for query: {query}\")\n",
    "    results = scrape_search_results(query)\n",
    "    \n",
    "    if results:\n",
    "        relevance_scores = evaluate_relevance(results, keywords)\n",
    "        for i, (result, score) in enumerate(zip(results, relevance_scores), start=1):\n",
    "            if i > 20:  \n",
    "                break\n",
    "            data.append({\n",
    "                'Query': query,\n",
    "                'Title': result['title'],\n",
    "                #'Link': result['link'],\n",
    "                #'Date': result['date'],\n",
    "                'Relevance Score': score\n",
    "            })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the table\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed1f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a1cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1bd870",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "- `engine`: The search engine used.\n",
    "  - Done\n",
    "  \n",
    "- `query`: The search query.\n",
    "  - Done\n",
    "  \n",
    "- `title`: The title of the article.\n",
    "  - Done\n",
    "  \n",
    "- `content`: The content of the article (for computation purposes).\n",
    "  - Done\n",
    "  \n",
    "- `domain`: The domain of the article.\n",
    "\n",
    "- `Precision`:  measures the ability of Search Engine to produce only relevant results. Precision is the ratio between the number of relevant documents retrieved by the system and the total number of documents retrieved.\n",
    "       Performance Evaluation of Selected Search Engines (Ajayi, Olusola Olajide, Elegbeleye, Damilola Matthew, 2014)\n",
    "\n",
    "- `relevance_score`: The computed relevance score of the article to the query using NLP (TF-IDF vectorization and cosine similarity).\n",
    "  - **Computation of this score**:\n",
    "    - Collect the queries and the content of the articles.\n",
    "    - Remove punctuation and other noise.\n",
    "    - Convert the text data (queries and content) into numerical vectors using TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "    - Compute the cosine similarity between the query vectors and the article content vectors. Cosine similarity measures the cosine of the angle between two non-zero vectors, which helps in determining how similar the two vectors are. A higher cosine similarity score indicates higher relevance.\n",
    "\n",
    "- `topical_cluster`: The cluster to which the article belongs.\n",
    "\n",
    "- `domain_authority`: The authority score of the article's domain.\n",
    "\n",
    "- `source_novelty`: Indicates if the source domain is unique (0) or repeated (1).\n",
    "\n",
    "- `content_uniqueness`: The uniqueness score of the article's content.\n",
    "\n",
    "- `publication_date`: The publication date of the article.\n",
    "\n",
    "- `volatility_indicator`: A score indicating the volatility of the search results.\n",
    "\n",
    "- `Stability measurements`: are to examine the stability of search engine performance over time. The measurements are: \n",
    "(1) The stability of the number of pages retrieved;\n",
    "\n",
    "(2) the number of pages among the top 20 retrieved pages that remain the same in two consecutive tests over a short time period (e.g. a week apart); and\n",
    "\n",
    "(3) The number of pages among the top 20 retrieved pages that remain in the same ranking order in two consecutive tests over a short time period (e.g. a week apart). Essentially, a series of searches needs to be performed on a search engine over a period of time (e.g. one search a week over a 10-week period). The number of pages retrieved needs to be recorded. The top 10 pages retrieved is compared with those from the previous search to determine\n",
    "\n",
    "(a) If the pages retrieved are the same;\n",
    "\n",
    "(b) If the ranking of the pages is the same.\n",
    "\n",
    "The examination of the top 10 pages is carried out because it will be extremely time consuming to compare each\n",
    "page in the entire retrieved set, which is typically very large. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c12d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe7931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff8962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'engine': ['Google', 'Bing', 'Google', 'Bing'],\n",
    "    'query': ['underreporting catch', 'underreporting catch', 'misreporting catch', 'misreporting catch'],\n",
    "    'title': ['Title 1', 'Title 2', 'Title 3', 'Title 4'],\n",
    "    'content': ['Content of article 1 about underreporting.', \n",
    "                'Different content of article 2 about catch.', \n",
    "                'Another article 3 on misreporting catch.', \n",
    "                'Misreporting discussed in article 4.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239b6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Combine all text data (queries and content) for consistent vectorization\n",
    "all_texts = df['query'].tolist() + df['content'].tolist()\n",
    "\n",
    "# Fit the vectorizer on all texts\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "# Transform queries and content separately\n",
    "query_vectors = vectorizer.transform(df['query'])\n",
    "content_vectors = vectorizer.transform(df['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1e652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   engine                 query    title  \\\n",
      "0  Google  underreporting catch  Title 1   \n",
      "1    Bing  underreporting catch  Title 2   \n",
      "2  Google    misreporting catch  Title 3   \n",
      "3    Bing    misreporting catch  Title 4   \n",
      "\n",
      "                                       content  relevance_score  \n",
      "0   Content of article 1 about underreporting.         0.341680  \n",
      "1  Different content of article 2 about catch.         0.146520  \n",
      "2     Another article 3 on misreporting catch.         0.461982  \n",
      "3         Misreporting discussed in article 4.         0.297402  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity\n",
    "relevance_scores = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    query_vector = query_vectors[i]\n",
    "    content_vector = content_vectors[i]\n",
    "    relevance_score = cosine_similarity(query_vector, content_vector)[0][0]\n",
    "    relevance_scores.append(relevance_score)\n",
    "\n",
    "# Add relevance scores to the DataFrame\n",
    "df['relevance_score'] = relevance_scores\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8de6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23299bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Authority not found for https://www.google.com/search\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_domain_authority(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Example: If domain authority is in a <div> with class 'domain-authority'\n",
    "            da_element = soup.find('div', class_='domain-authority')\n",
    "            if da_element:\n",
    "                domain_authority = da_element.text.strip()\n",
    "                return domain_authority\n",
    "            else:\n",
    "                # Check for <meta> tag with domain authority information\n",
    "                meta_tags = soup.find_all('meta')\n",
    "                for tag in meta_tags:\n",
    "                    if 'domainAuthority' in tag.get('name', '').lower():\n",
    "                        domain_authority = tag.get('content', '').strip()\n",
    "                        return domain_authority\n",
    "                # Add more specific checks as per the website's structure\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "#url = 'https://www.clotilde-djuikem.com/'\n",
    "url = 'https://www.google.com/search'\n",
    "domain_authority = get_domain_authority(url)\n",
    "if domain_authority:\n",
    "    print(f\"Domain Authority for {url}: {domain_authority}\")\n",
    "else:\n",
    "    print(f\"Domain Authority not found for {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0fa9c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Date not found for https://ici.radio-canada.ca/info/videos/1-8896282/une-etude-sur-incidence-desinformation-chez-jeunes-noirs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parse\n",
    "\n",
    "def get_publication_date(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Example: Find publication date in <meta> tag with property='article:published_time'\n",
    "            meta_tags = soup.find_all('meta', property='article:published_time')\n",
    "            if meta_tags:\n",
    "                publication_date = meta_tags[0]['content']\n",
    "                return publication_date\n",
    "            else:\n",
    "                # If meta tag not found, look for other elements containing publication date\n",
    "                # Example: Find publication date in <time> tag with itemprop='datePublished'\n",
    "                time_tags = soup.find_all('time', itemprop='datePublished')\n",
    "                if time_tags:\n",
    "                    publication_date = time_tags[0].text.strip()\n",
    "                    return publication_date\n",
    "                else:\n",
    "                    # Add more specific checks as per the website's structure\n",
    "                    return None\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = 'https://ici.radio-canada.ca/info/videos/1-8896282/une-etude-sur-incidence-desinformation-chez-jeunes-noirs'\n",
    "\n",
    "publication_date = get_publication_date(url)\n",
    "\n",
    "if publication_date:\n",
    "    formatted_date = date_parse(publication_date)\n",
    "    print(f\"Publication Date for {url}: {formatted_date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(f\"Publication Date not found for {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784850ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Date not found for https://ici.radio-canada.ca/info/videos/1-8896282/une-etude-sur-incidence-desinformation-chez-jeunes-noirs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse as date_parse\n",
    "\n",
    "def get_publication_date(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Example: Extract publication date using regex from specific text patterns\n",
    "            # Adjust the regex pattern based on the structure of the webpage\n",
    "            regex_patterns = [\n",
    "                r'Published: (\\d{4}-\\d{2}-\\d{2})',  # Example pattern for date in 'Published: YYYY-MM-DD' format\n",
    "                r'Published on (\\w+ \\d{1,2}, \\d{4})'  # Another example pattern for date in 'Month Day, YYYY' format\n",
    "            ]\n",
    "            \n",
    "            for pattern in regex_patterns:\n",
    "                match = re.search(pattern, soup.get_text())\n",
    "                if match:\n",
    "                    publication_date_str = match.group(1)\n",
    "                    return publication_date_str\n",
    "            \n",
    "            # If no regex pattern matches, return None\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = 'https://ici.radio-canada.ca/info/videos/1-8896282/une-etude-sur-incidence-desinformation-chez-jeunes-noirs'\n",
    "publication_date = get_publication_date(url)\n",
    "\n",
    "if publication_date:\n",
    "    formatted_date = date_parse(publication_date)\n",
    "    print(f\"Publication Date for {url}: {formatted_date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(f\"Publication Date not found for {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48d644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
