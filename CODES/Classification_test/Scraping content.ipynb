{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de77f5a",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed98e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87214b69",
   "metadata": {},
   "source": [
    "### Importing the list of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908b59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('search_results_articles.xlsx')\n",
    "\n",
    "#link_list = excel_data['Link'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221df5b0",
   "metadata": {},
   "source": [
    "### Scrap one link at a time and form a new Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5c48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#excel_data = excel_data.iloc[:15]\n",
    "new_data = excel_data.copy()\n",
    "\n",
    "for ind, row in new_data.iloc[:15].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fba3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in new_data.iloc[15:30].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4edfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in new_data.iloc[30:33].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79b98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ind, row in new_data.iloc[30:50].iterrows():\n",
    "for ind, row in new_data.iloc[34:50].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "\n",
    "# 33 has some connection problem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95eab02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Engine</th>\n",
       "      <th>Title</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Bing</td>\n",
       "      <td>Two vessels catch fire after missile strikes o...</td>\n",
       "      <td>12d</td>\n",
       "      <td>https://www.yahoo.com/news/ambrey-says-cargo-s...</td>\n",
       "      <td>CAIRO (Reuters) -Two ships caught on fire aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Bing</td>\n",
       "      <td>Two vessels catch fire after missile strikes o...</td>\n",
       "      <td>12d</td>\n",
       "      <td>https://japantoday.com/category/world/two-vess...</td>\n",
       "      <td>\\nJapanToday\\nSotokanda S Bldg. 4F\\n\\t\\t\\t\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Bing</td>\n",
       "      <td>Two vessels catch fire after missile strikes o...</td>\n",
       "      <td>12d</td>\n",
       "      <td>https://www.jpost.com/breaking-news/article-80...</td>\n",
       "      <td>Two ships caught on fire after being hit by p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Bing</td>\n",
       "      <td>Two Vessels Catch Fire After Missile Strikes o...</td>\n",
       "      <td>13d</td>\n",
       "      <td>https://www.usnews.com/news/world/articles/202...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>CDFW News | CDFW Busts Poaching Ring in Unlawf...</td>\n",
       "      <td>Mar 20, 2024</td>\n",
       "      <td>https://wildlife.ca.gov/News/Archive/cdfw-bust...</td>\n",
       "      <td>Contact an Information Officer\\n Subscribe to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Jury Convicts Long Island Fishing Captain of C...</td>\n",
       "      <td>Oct 4, 2023</td>\n",
       "      <td>https://www.justice.gov/opa/pr/jury-convicts-l...</td>\n",
       "      <td>An official website of the United States gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Florida fishing: Yahoo for wahoo! Live blue ru...</td>\n",
       "      <td>Feb 22, 2024</td>\n",
       "      <td>https://www.tcpalm.com/story/sports/outdoors/f...</td>\n",
       "      <td>Did you enjoy your false spring? You know, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Montauk Fisherman on Trial for Conspiracy, Fra...</td>\n",
       "      <td>Sep 27, 2023</td>\n",
       "      <td>https://www.27east.com/east-hampton-press/mont...</td>\n",
       "      <td>The captain of the Montauk commercial fishing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Terrifying Titan communication logs that 'deta...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-13515...</td>\n",
       "      <td>By James Gordon For Dailymail.com    Publishe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Cape Cod Fishing Report- August 31, 2023</td>\n",
       "      <td>Aug 31, 2023</td>\n",
       "      <td>https://onthewater.com/fishing-reports/2023/08...</td>\n",
       "      <td>More albies fill in Vineyard and Nantucket S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Investigation reveals Chinese seafood caught a...</td>\n",
       "      <td>Dec 12, 2023</td>\n",
       "      <td>https://www.pbs.org/newshour/show/investigatio...</td>\n",
       "      <td>Donate to PBS News Hour by June 30! \\n\\nJohn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>FACT CHECK: No reports of Taiwan attacking Chi...</td>\n",
       "      <td>Nov 27, 2023</td>\n",
       "      <td>https://www.rappler.com/newsbreak/fact-check/n...</td>\n",
       "      <td>SUMMARY This is AI generated summarization, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>False albacore hit inshore, more could be on t...</td>\n",
       "      <td>Sep 1, 2023</td>\n",
       "      <td>https://www.app.com/story/sports/outdoors/fish...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Massachusetts Fishing Report- August 17, 2023</td>\n",
       "      <td>Aug 17, 2023</td>\n",
       "      <td>https://onthewater.com/fishing-reports/2023/08...</td>\n",
       "      <td>While hardly a banner year for bunker, in bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Reasons Why Deadliest Catch Is Fake</td>\n",
       "      <td>Sep 18, 2023</td>\n",
       "      <td>https://www.grunge.com/30744/reasons-deadliest...</td>\n",
       "      <td>\\n\"Deadliest Catch\" has been lauded for being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Run &amp; Gun: False Albacore Froth</td>\n",
       "      <td>Sep 1, 2020</td>\n",
       "      <td>https://www.thefisherman.com/article/run-gun-f...</td>\n",
       "      <td>The Fisherman Advanced Search By beach or boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Alaska Fisherman Sentenced to Federal Prison a...</td>\n",
       "      <td>Aug 23, 2021</td>\n",
       "      <td>https://www.fisheries.noaa.gov/feature-story/a...</td>\n",
       "      <td>Internet Explorer lacks support for the featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>How Hackers Are Helping Illegal Fishing Boats ...</td>\n",
       "      <td>Mar 6, 2024</td>\n",
       "      <td>https://nautil.us/how-illegal-fishing-ships-hi...</td>\n",
       "      <td>Art+Science Biology + Beyond Cosmos Culture E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Global Fishing Watch, SkyTruth warn of vessels...</td>\n",
       "      <td>15 hours ago</td>\n",
       "      <td>https://www.seafoodsource.com/news/environment...</td>\n",
       "      <td>Search Share    Nonprofit organizations Globa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Vessel caught falsifying fishing logs</td>\n",
       "      <td>Google</td>\n",
       "      <td>Northeast Offshore Fishing Report - August 25,...</td>\n",
       "      <td>Aug 25, 2023</td>\n",
       "      <td>https://onthewater.com/fishing-reports/2023/08...</td>\n",
       "      <td>At some point back in July, we reported abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Query  Engine  \\\n",
       "30  Vessel caught misreporting catch amount    Bing   \n",
       "31  Vessel caught misreporting catch amount    Bing   \n",
       "32  Vessel caught misreporting catch amount    Bing   \n",
       "33  Vessel caught misreporting catch amount    Bing   \n",
       "34    Vessel caught falsifying fishing logs  Google   \n",
       "35    Vessel caught falsifying fishing logs  Google   \n",
       "36    Vessel caught falsifying fishing logs  Google   \n",
       "37    Vessel caught falsifying fishing logs  Google   \n",
       "38    Vessel caught falsifying fishing logs  Google   \n",
       "39    Vessel caught falsifying fishing logs  Google   \n",
       "40    Vessel caught falsifying fishing logs  Google   \n",
       "41    Vessel caught falsifying fishing logs  Google   \n",
       "42    Vessel caught falsifying fishing logs  Google   \n",
       "43    Vessel caught falsifying fishing logs  Google   \n",
       "44    Vessel caught falsifying fishing logs  Google   \n",
       "45    Vessel caught falsifying fishing logs  Google   \n",
       "46    Vessel caught falsifying fishing logs  Google   \n",
       "47    Vessel caught falsifying fishing logs  Google   \n",
       "48    Vessel caught falsifying fishing logs  Google   \n",
       "49    Vessel caught falsifying fishing logs  Google   \n",
       "\n",
       "                                                Title         Dates  \\\n",
       "30  Two vessels catch fire after missile strikes o...           12d   \n",
       "31  Two vessels catch fire after missile strikes o...           12d   \n",
       "32  Two vessels catch fire after missile strikes o...           12d   \n",
       "33  Two Vessels Catch Fire After Missile Strikes o...           13d   \n",
       "34  CDFW News | CDFW Busts Poaching Ring in Unlawf...  Mar 20, 2024   \n",
       "35  Jury Convicts Long Island Fishing Captain of C...   Oct 4, 2023   \n",
       "36  Florida fishing: Yahoo for wahoo! Live blue ru...  Feb 22, 2024   \n",
       "37  Montauk Fisherman on Trial for Conspiracy, Fra...  Sep 27, 2023   \n",
       "38  Terrifying Titan communication logs that 'deta...    1 week ago   \n",
       "39           Cape Cod Fishing Report- August 31, 2023  Aug 31, 2023   \n",
       "40  Investigation reveals Chinese seafood caught a...  Dec 12, 2023   \n",
       "41  FACT CHECK: No reports of Taiwan attacking Chi...  Nov 27, 2023   \n",
       "42  False albacore hit inshore, more could be on t...   Sep 1, 2023   \n",
       "43      Massachusetts Fishing Report- August 17, 2023  Aug 17, 2023   \n",
       "44                Reasons Why Deadliest Catch Is Fake  Sep 18, 2023   \n",
       "45                    Run & Gun: False Albacore Froth   Sep 1, 2020   \n",
       "46  Alaska Fisherman Sentenced to Federal Prison a...  Aug 23, 2021   \n",
       "47  How Hackers Are Helping Illegal Fishing Boats ...   Mar 6, 2024   \n",
       "48  Global Fishing Watch, SkyTruth warn of vessels...  15 hours ago   \n",
       "49  Northeast Offshore Fishing Report - August 25,...  Aug 25, 2023   \n",
       "\n",
       "                                                 Link  \\\n",
       "30  https://www.yahoo.com/news/ambrey-says-cargo-s...   \n",
       "31  https://japantoday.com/category/world/two-vess...   \n",
       "32  https://www.jpost.com/breaking-news/article-80...   \n",
       "33  https://www.usnews.com/news/world/articles/202...   \n",
       "34  https://wildlife.ca.gov/News/Archive/cdfw-bust...   \n",
       "35  https://www.justice.gov/opa/pr/jury-convicts-l...   \n",
       "36  https://www.tcpalm.com/story/sports/outdoors/f...   \n",
       "37  https://www.27east.com/east-hampton-press/mont...   \n",
       "38  https://www.dailymail.co.uk/news/article-13515...   \n",
       "39  https://onthewater.com/fishing-reports/2023/08...   \n",
       "40  https://www.pbs.org/newshour/show/investigatio...   \n",
       "41  https://www.rappler.com/newsbreak/fact-check/n...   \n",
       "42  https://www.app.com/story/sports/outdoors/fish...   \n",
       "43  https://onthewater.com/fishing-reports/2023/08...   \n",
       "44  https://www.grunge.com/30744/reasons-deadliest...   \n",
       "45  https://www.thefisherman.com/article/run-gun-f...   \n",
       "46  https://www.fisheries.noaa.gov/feature-story/a...   \n",
       "47  https://nautil.us/how-illegal-fishing-ships-hi...   \n",
       "48  https://www.seafoodsource.com/news/environment...   \n",
       "49  https://onthewater.com/fishing-reports/2023/08...   \n",
       "\n",
       "                                              Content  \n",
       "30   CAIRO (Reuters) -Two ships caught on fire aft...  \n",
       "31   \\nJapanToday\\nSotokanda S Bldg. 4F\\n\\t\\t\\t\\t\\...  \n",
       "32   Two ships caught on fire after being hit by p...  \n",
       "33                                                nan  \n",
       "34   Contact an Information Officer\\n Subscribe to...  \n",
       "35   An official website of the United States gove...  \n",
       "36   Did you enjoy your false spring? You know, wh...  \n",
       "37   The captain of the Montauk commercial fishing...  \n",
       "38   By James Gordon For Dailymail.com    Publishe...  \n",
       "39    More albies fill in Vineyard and Nantucket S...  \n",
       "40   Donate to PBS News Hour by June 30! \\n\\nJohn ...  \n",
       "41    SUMMARY This is AI generated summarization, ...  \n",
       "42                                                     \n",
       "43    While hardly a banner year for bunker, in bo...  \n",
       "44   \\n\"Deadliest Catch\" has been lauded for being...  \n",
       "45   The Fisherman Advanced Search By beach or boa...  \n",
       "46   Internet Explorer lacks support for the featu...  \n",
       "47   Art+Science Biology + Beyond Cosmos Culture E...  \n",
       "48   Search Share    Nonprofit organizations Globa...  \n",
       "49     At some point back in July, we reported abo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.iloc[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7eea090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 50\n",
      "====== Done 51\n",
      "====== Done 52\n",
      "====== Done 53\n",
      "====== Done 54\n",
      "====== Done 55\n",
      "====== Done 56\n",
      "====== Done 57\n",
      "====== Done 58\n",
      "====== Done 59\n",
      "====== Done 60\n",
      "====== Done 61\n",
      "====== Done 62\n",
      "====== Done 63\n",
      "====== Done 64\n",
      "====== Done 65\n",
      "====== Done 66\n",
      "====== Done 67\n",
      "====== Done 68\n",
      "====== Done 69\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[50:70].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6693de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 70\n",
      "====== Done 71\n",
      "====== Done 72\n",
      "====== Done 73\n",
      "====== Done 74\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 75\n",
      "====== Done 76\n",
      "====== Done 77\n",
      "====== Done 78\n",
      "====== Done 79\n",
      "====== Done 80\n",
      "====== Done 81\n",
      "====== Done 82\n",
      "====== Done 83\n",
      "====== Done 84\n",
      "====== Done 85\n",
      "====== Done 86\n",
      "====== Done 87\n",
      "====== Done 88\n",
      "====== Done 89\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[70:90].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b470ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 90\n",
      "====== Done 91\n",
      "====== Done 92\n",
      "====== Done 93\n",
      "====== Done 94\n",
      "====== Done 95\n",
      "====== Done 96\n",
      "====== Done 97\n",
      "====== Done 98\n",
      "====== Done 99\n",
      "====== Done 100\n",
      "====== Done 101\n",
      "====== Done 102\n",
      "====== Done 103\n",
      "====== Done 104\n",
      "====== Done 105\n",
      "====== Done 106\n",
      "====== Done 107\n",
      "====== Done 108\n",
      "====== Done 109\n",
      "====== Done 110\n",
      "====== Done 111\n",
      "====== Done 112\n",
      "====== Done 113\n",
      "====== Done 114\n",
      "====== Done 115\n",
      "====== Done 116\n",
      "====== Done 117\n",
      "====== Done 118\n",
      "====== Done 119\n",
      "====== Done 120\n",
      "====== Done 121\n",
      "====== Done 122\n",
      "Failed to retrieve content. Status code: 401\n",
      "====== Done 123\n",
      "====== Done 124\n",
      "====== Done 125\n",
      "====== Done 126\n",
      "====== Done 127\n",
      "====== Done 128\n",
      "====== Done 129\n",
      "====== Done 130\n",
      "====== Done 131\n",
      "====== Done 132\n",
      "====== Done 133\n",
      "====== Done 134\n",
      "====== Done 135\n",
      "====== Done 136\n",
      "====== Done 137\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 138\n",
      "====== Done 139\n",
      "====== Done 140\n",
      "====== Done 141\n",
      "====== Done 142\n",
      "====== Done 143\n",
      "====== Done 144\n",
      "====== Done 145\n",
      "====== Done 146\n",
      "====== Done 147\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[90:148].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ca8bb",
   "metadata": {},
   "source": [
    "#### Save!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936d62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "new_data.to_excel('df_content.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0784051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 150\n",
      "====== Done 151\n",
      "====== Done 152\n",
      "====== Done 153\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 154\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 155\n",
      "====== Done 156\n",
      "====== Done 157\n",
      "====== Done 158\n",
      "====== Done 159\n",
      "====== Done 160\n",
      "====== Done 161\n",
      "====== Done 162\n",
      "====== Done 163\n",
      "====== Done 164\n",
      "====== Done 165\n",
      "====== Done 166\n",
      "====== Done 167\n",
      "====== Done 168\n",
      "====== Done 169\n",
      "====== Done 170\n",
      "====== Done 171\n",
      "====== Done 172\n",
      "====== Done 173\n",
      "====== Done 174\n",
      "====== Done 175\n",
      "====== Done 176\n",
      "====== Done 177\n",
      "====== Done 178\n",
      "====== Done 179\n",
      "====== Done 180\n",
      "====== Done 181\n",
      "====== Done 182\n",
      "====== Done 183\n",
      "====== Done 184\n",
      "====== Done 185\n",
      "====== Done 186\n",
      "====== Done 187\n",
      "====== Done 188\n",
      "====== Done 189\n",
      "====== Done 190\n",
      "====== Done 191\n",
      "====== Done 192\n",
      "====== Done 193\n",
      "====== Done 194\n",
      "====== Done 195\n",
      "====== Done 196\n",
      "====== Done 197\n",
      "====== Done 198\n",
      "====== Done 199\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[150:200].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b95ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "new_data.to_excel('df_content.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050c22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 200\n",
      "====== Done 201\n",
      "====== Done 202\n",
      "====== Done 203\n",
      "====== Done 204\n",
      "====== Done 205\n",
      "====== Done 206\n",
      "====== Done 207\n",
      "====== Done 208\n",
      "====== Done 209\n",
      "====== Done 210\n",
      "====== Done 211\n",
      "====== Done 212\n",
      "====== Done 213\n",
      "====== Done 214\n",
      "====== Done 215\n",
      "====== Done 216\n",
      "====== Done 217\n",
      "====== Done 218\n",
      "====== Done 219\n",
      "====== Done 220\n",
      "====== Done 221\n",
      "====== Done 222\n",
      "====== Done 223\n",
      "====== Done 224\n",
      "====== Done 225\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 226\n",
      "====== Done 227\n",
      "====== Done 228\n",
      "====== Done 229\n",
      "====== Done 230\n",
      "====== Done 231\n",
      "====== Done 232\n",
      "====== Done 233\n",
      "====== Done 234\n",
      "====== Done 235\n",
      "====== Done 236\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 237\n",
      "====== Done 238\n",
      "====== Done 239\n",
      "====== Done 240\n",
      "====== Done 241\n",
      "====== Done 242\n",
      "====== Done 243\n",
      "====== Done 244\n",
      "====== Done 245\n",
      "====== Done 246\n",
      "====== Done 247\n",
      "====== Done 248\n",
      "====== Done 249\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[200:250].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed0e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "new_data.to_excel('df_content.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec8248a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 250\n",
      "====== Done 251\n",
      "Failed to retrieve content. Status code: 403\n",
      "====== Done 252\n",
      "====== Done 253\n",
      "====== Done 254\n",
      "====== Done 255\n",
      "====== Done 256\n",
      "====== Done 257\n",
      "====== Done 258\n",
      "====== Done 259\n",
      "====== Done 260\n",
      "====== Done 261\n",
      "====== Done 262\n",
      "====== Done 263\n",
      "====== Done 264\n",
      "====== Done 265\n",
      "====== Done 266\n",
      "====== Done 267\n",
      "====== Done 268\n",
      "====== Done 269\n",
      "====== Done 270\n",
      "====== Done 271\n",
      "====== Done 272\n",
      "====== Done 273\n",
      "====== Done 274\n",
      "====== Done 275\n",
      "====== Done 276\n",
      "====== Done 277\n",
      "====== Done 278\n",
      "====== Done 279\n",
      "====== Done 280\n",
      "====== Done 281\n",
      "====== Done 282\n",
      "====== Done 283\n",
      "====== Done 284\n",
      "====== Done 285\n",
      "====== Done 286\n",
      "====== Done 287\n",
      "====== Done 288\n",
      "====== Done 289\n",
      "====== Done 290\n",
      "====== Done 291\n",
      "====== Done 292\n",
      "====== Done 293\n",
      "====== Done 294\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[250:297].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43938fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "new_data.to_excel('df_content.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d304d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Done 290\n",
      "====== Done 291\n",
      "====== Done 292\n",
      "====== Done 293\n",
      "====== Done 294\n"
     ]
    }
   ],
   "source": [
    "for ind, row in new_data.iloc[290:297].iterrows():\n",
    "    #print(row)\n",
    "    content = scrape_content(row['Link'])  # Example function call to get content from link\n",
    "    new_data.at[ind, 'Content'] = content \n",
    "    print(\"====== Done\", ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a77ede21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "new_data.to_excel('df_content.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1deb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f11b864",
   "metadata": {},
   "source": [
    "### Cleaning the title and text ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d33abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72426326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b4b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
