{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ead81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6dfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280285fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12 articles in relevant_articles.\n"
     ]
    }
   ],
   "source": [
    "# get pre-scraped text data\n",
    "# evaluate information score only on relative articles\n",
    "\n",
    "with open('rel_articles.txt', 'r') as file:\n",
    "    data_code = file.read()\n",
    "\n",
    "exec(data_code)  # Just to verify the content read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d981aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n",
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "for article in relevant_articles:\n",
    "    print(weighted_info_score(article))\n",
    "# 2  2   1   1  1     \n",
    "# IMO MMSI Names (people/location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7647e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================New Article\n",
      "['salmon', 'fish']\n",
      "=================New Article\n",
      "['fish', 'sablefish']\n",
      "=================New Article\n",
      "['fish']\n",
      "=================New Article\n",
      "['fish', 'mackerel']\n",
      "=================New Article\n",
      "['fish', 'lobster']\n",
      "=================New Article\n",
      "[]\n",
      "=================New Article\n",
      "['crab']\n",
      "=================New Article\n",
      "['fish']\n",
      "=================New Article\n",
      "['fish', 'albacore', 'tuna']\n",
      "=================New Article\n",
      "['yellowfin', 'fish', 'albacore', 'tuna', 'shark', 'bigeye']\n",
      "=================New Article\n",
      "[]\n",
      "=================New Article\n",
      "['fish', 'shark']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m     article \u001b[38;5;241m=\u001b[39m text_preprocess(article)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(find_words_in_list(article,fish_list))\n\u001b[1;32m---> 31\u001b[0m fish_list \u001b[38;5;241m=\u001b[39m get_related_words(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfish\u001b[39m\u001b[38;5;124m'\u001b[39m, limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m'\u001b[39m, topics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvessel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfishing\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfish\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfisheries\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfishing\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "some_species = ['salmon', 'tuna', 'shrimp', 'crab', 'shark', 'sablefish']\n",
    "\n",
    "fish_list = some_species\n",
    "for fish in some_species:\n",
    "    fish_list = list(set(fish_list).union(set(get_related_words(fish, limit = '100',topics = ['species', 'fishing']))))\n",
    "\n",
    "#print(fish_list)\n",
    "\n",
    "def find_words_in_list(text, word_list):\n",
    "#    nlp = spacy.load('en_core_web_sm')\n",
    "#    doc = nlp(text)\n",
    "    fish = []\n",
    "    text_words = text.split()\n",
    "    found_words = []\n",
    "\n",
    "    # Iterate over word_list and check if each word is in words_in_text\n",
    "    for word in word_list:\n",
    "        if word.lower() in text_words:\n",
    "            found_words.append(word)\n",
    "\n",
    "    return found_words\n",
    "\n",
    "\n",
    "\n",
    "for article in relevant_articles:\n",
    "    print(\"=================New Article\")\n",
    "    article = text_preprocess(article)\n",
    "    print(find_words_in_list(article,fish_list))\n",
    "\n",
    "\n",
    "fish_list = get_related_words('fish', limit = '100', topics = ['vessel', 'fishing']) - ['fish', 'fisheries','fishing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf8106",
   "metadata": {},
   "source": [
    "#### Thought: For a search engine, it might be a good idea to add the information score for all relevant articles from the search.\n",
    "\n",
    "#### Challenge: It is extremely rare an article directly contains IMO number or MMSI number. We need some more types of information to check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96466218",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#### find_imo: Find IMO (7-digit numbers)\n",
    "\n",
    "def find_imo(content):    \n",
    "    # regular expression for a 7-digit number (exactly 7-digit)\n",
    "    imo_pattern = r'\\b\\d{7}\\b'\n",
    "    \n",
    "    # find all 7-digit numbers in the content \n",
    "    imo_list = re.findall(imo_pattern, content)\n",
    "    \n",
    "    return imo_list\n",
    "\n",
    "########################################################\n",
    "#### find_mmsi: Find MMSI (9-digit numbers)\n",
    "\n",
    "def find_mmsi(content):    \n",
    "    # regular expression for a 9-digit number (exactly 9-digit)\n",
    "    mmsi_pattern = r'\\b\\d{9}\\b'\n",
    "    \n",
    "    # find all 9-digit numbers in the content \n",
    "    mmsi_list = re.findall(mmsi_pattern, content)\n",
    "    \n",
    "    return mmsi_list\n",
    "\n",
    "########################################################\n",
    "#### find names (company, people, etc)\n",
    "# These functions require spaCy\n",
    "########################################################\n",
    "\n",
    "import spacy\n",
    "\n",
    "########################################################\n",
    "# people's names\n",
    "def find_people(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON']: # NORP is about nationality/religious/political group\n",
    "            names.append(ent.text)\n",
    "    return names\n",
    "\n",
    "########################################################\n",
    "# company's names\n",
    "def find_company(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    names = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['ORG']: # NORP is about nationality/religious/political group\n",
    "            names.append(ent.text)\n",
    "    return names\n",
    "\n",
    "\n",
    "########################################################\n",
    "# country's name / location\n",
    "def find_location(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    locations = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['LOC', 'GPE', 'NORP']: # location labels\n",
    "            locations.append(ent.text)\n",
    "    return locations\n",
    "\n",
    "\n",
    "########################################################\n",
    "# fishing species\n",
    "def find_location(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    locations = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['LOC', 'GPE', 'NORP']: # location labels\n",
    "            locations.append(ent.text)\n",
    "    return locations\n",
    "\n",
    "########################################################\n",
    "# GRT\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Information score\n",
    "def weighted_info_score(text):\n",
    "    #give weigths to each information:\n",
    "    # IMO: 2\n",
    "    # MMSI: 2\n",
    "    # Involved parties or location: 1\n",
    "    # The score will be out of 10\n",
    "    score_list = [find_imo, find_mmsi, find_people, find_company, find_location]\n",
    "    weight_vector = np.array([2, 2, 1, 1, 1])\n",
    "    score = []\n",
    "    i=0\n",
    "    for item in score_list:\n",
    "        print(i, item(text))   \n",
    "        i += 1\n",
    "        if len(item(text)) > 0:\n",
    "            score.append(1)\n",
    "        else:\n",
    "            score.append(0)\n",
    "    score = np.array(score)\n",
    "    weighted_score = score * weight_vector\n",
    "    #np.dot(score,weight_vector)\n",
    "\n",
    "    return weighted_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075238d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "1 []\n",
      "2 ['Haida Gwaii', 'Miriam Maisonville', 'David Patterson', 'Adam Munro', 'Silas Levesque', 'Patterson', 'Haida Gwaii', 'Patterson', 'Levesque', 'Levesque', 'Patterson', 'Munro', 'Levesque', 'Patterson', 'Michael Andrew Bullock', 'Patterson', 'Munro', 'Patterson', 'Maisonville', 'Maisonville', 'Maisonville']\n",
      "3 ['B.C.’s Supreme Court', 'ATM Fishing Ltd.', 'Munro', 'ATM', 'the Wind Walker', 'the Wind Walker', 'Munro', 'B.C. Ltd.', '605463 B.C. Ltd.', 'the Winter Wind', 'Munro', 'B.C. Ltd.', 'the Winter Wind', 'the Winter Wind', 'the Winter Wind']\n",
      "4 ['Masset', 'Masset', 'B.C.', 'Masset', 'B.C.', 'Masset']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Kodiak', 'James Aaron Stevens', 'Stevens', 'Stevens', 'Stevens', 'Stevens', 'Stevens', 'Stevens', 'Kodiak', 'Stevens', 'Stevens', 'Stevens', 'Janiese Stevens', 'James', 'Stevens']\n",
      "3 ['EUR', 'Office', 'the F/V Southern Seas', 'International Pacific Halibut Commission', 'IPHC', 'Alaska Department', 'KMXT', 'IFQ', 'KMXT', 'NOAA', 'Office of Law Enforcement']\n",
      "4 ['Alaska', 'U.S.', 'Alaska District']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Yonhap', 'Yang Jin-moon']\n",
      "3 ['the oceans ministry', 'the Ministry of Oceans and Fisheries', 'COVID-19']\n",
      "4 ['SEOUL', 'Chinese', 'Seoul', 'Shinan', \"South Korea's\", 'the Yellow Sea', 'Chinese']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Groene Amsterdammer', 'Nils Courcy', 'Margiris', 'Van', 'Plas', 'Brian O’Riordan']\n",
      "3 ['ClientEarth', 'the Low Impact Fishers of Europe', 'NVWA', 'the European Union', 'NVWA', 'CFP', 'NVWA', 'the European Commission', 'the Court of Justice', 'the European Union', 'ClientEarth', 'NVWA', 'ClientEarth', 'E.U.']\n",
      "4 ['Dutch', 'The Netherlands', 'E.U.', 'Dutch', 'Dutch', 'Netherlands', 'E.U.', 'Dutch', 'European', 'Dutch', 'E.C.', 'Netherlands', 'E.C.', 'E.U.', 'Netherlands', 'Dutch', 'Dutch', 'Dutch', 'Parlevliet', 'MT', 'the Bay of Biscay', 'Netherlands', 'E.U.']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Glenn Robbins', 'Neil Herrick', 'Stephen Little of Warren', 'Ethan Chase', 'Jason Parent', 'Robbins']\n",
      "3 ['Western Seas Inc.', 'Commerce', 'Bangor Daily News', 'NOAA Fisheries', 'NOAA', 'Office of Law Enforcement', 'the Maine Marine Patrol']\n",
      "4 ['Maine', 'New Hampshire', 'Maine', 'Rockland', 'Maine', 'Maine', 'Portsmouth', 'N.H.', 'Maine', 'Western Sea', 'U.S.', 'Atlantic', 'U.S.', 'the Western Sea']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Casey Henneberry', 'Elizabeth Buckle', 'Henneberry', 'Sambro', 'Henneberry', 'Law Fisheries', 'Henneberry', 'Henneberry', 'Henneberry', 'Sambro', 'Buckle', 'Henneberry', 'Law Fisheries', 'Buckle']\n",
      "3 ['Sambro', 'ALS Fisheries', 'Halifax', 'ALS Fisheries', 'ALS Fisheries', 'the Department of Fisheries', 'ALS', 'ALS', 'ALS', 'Crown', 'DFO', 'ALS']\n",
      "4 ['N.S.', 'Oceans']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Jason Ahearn', 'James Gavin', 'Earl MacRae', 'Jeffery MacLeod', 'Casey Gavin', 'Nancy Orr', 'James Gavin', 'Gavin', 'Gavin', 'Leo Dorgan', 'Dorgan', 'Carter Hutt', 'Orr', 'Clifford Hood']\n",
      "3 ['Georgetown', 'Department of Fisheries', 'Court', 'the P.E.I. Snow Crab Association', 'CBC']\n",
      "4 ['P.E.I.', 'Georgetown', 'Oceans', 'Souris']\n",
      "[0 0 1 1 1]\n",
      "0 []\n",
      "1 []\n",
      "2 ['Glenn Robbins', 'Neil Herrick', 'Stephen Little of Warren', 'Ethan Chase', 'Jason Parent', 'James Landon']\n",
      "3 ['Portsmouth', 'Western Sea Inc.', 'the Penobscot Bay Pilot', 'the Courier-Gazette', 'the Portland Press Herald', 'Commerce', 'NOAA Fisheries', 'Chase', 'Herrick, Little', 'NOAA Office of Law Enforcement', 'the Press Herald', 'EUR']\n",
      "4 ['Maine', 'Rockland', 'Maine', 'Maine', 'New Hampshire', 'Maine', 'Maine', 'the Western Sea', 'Maine', 'U.S.', 'Parent', 'U.S.']\n",
      "[0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for article in relevant_articles:\n",
    "    print(weighted_info_score(article))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5c5a1",
   "metadata": {},
   "source": [
    "### Testing with NLTK\n",
    "(Not really better/worse than spaCy..) Sometimes spaCy get things correctly, and sometimes not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f44b21cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Haida Gwaii', 'David Patterson', 'Adam Munro', 'Silas Levesque', 'Patterson', 'Haida Gwaii', 'Patterson', 'Munro', 'Masset', 'Levesque', 'Patterson', 'Munro', 'Salmon Area', 'Salmon Troll Fishery', 'Levesque', 'Patterson', 'Munro', 'Michael Andrew Bullock', 'Patterson', 'Munro', 'Munro', 'Patterson', 'Maisonville']\n",
      "3 ['Supreme Court', 'Justice Miriam Maisonville', 'ATM Fishing', 'ATM', 'Wind', 'Wind', 'Wind', 'Justice Maisonville']\n",
      "4 ['Masset', 'Levesque', 'Masset', 'Prince Rupert', 'Crown', 'Masset']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['James Aaron Stevens', 'Alaska District', 'Stevens', 'Stevens', 'Alaskan Star', 'Stevens', 'Alaska Department', 'Stevens', 'Stevens', 'Stevens', 'Stevens', 'Stevens', 'Janiese Stevens', 'James', 'Law Enforcement']\n",
      "3 ['EUR', 'International Pacific Halibut Commission', 'IPHC', 'USD', 'EUR', 'EUR', 'KMXT', 'IFQ', 'KMXT', 'NOAA']\n",
      "4 ['Alaska', 'Kodiak', 'U.S.', 'Fish', 'Kodiak', 'Stevens', 'Stevens']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Shinan', 'Yang Jin-moon']\n",
      "3 ['Yellow Sea', 'Ministry of Oceans']\n",
      "4 ['SEOUL', 'Yonhap', 'Chinese', 'Seoul', 'South Korea', 'Chinese']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Groene Amsterdammer', 'Nils Courcy', 'Margiris', 'Van', 'Plas', 'Brian O', 'Riordan']\n",
      "3 ['ClientEarth', 'Low Impact Fishers', 'LIFE', 'NVWA', 'European Union', 'NVWA', 'CFP', 'NVWA', 'European Commission', 'Justice', 'European Union', 'LIFE', 'ClientEarth', 'NVWA', 'Parlevliet', 'Bay', 'LIFE', 'Netherlands']\n",
      "4 ['Environmental', 'Europe', 'Dutch', 'Netherlands', 'Dutch', 'Dutch', 'Netherlands', 'Dutch', 'European', 'Dutch', 'Netherlands', 'Netherlands', 'Dutch', 'Dutch', 'Dutch', 'Biscay']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Glenn Robbins', 'Neil Herrick', 'Stephen Little', 'Ethan Chase', 'Jason Parent', 'Maine', 'Robbins', 'Bangor Daily News', 'Law Enforcement']\n",
      "3 ['Owls Head', 'Seas Inc.', 'Commerce', 'Atlantic', 'NOAA Fisheries', 'NOAA', 'Maine Marine Patrol']\n",
      "4 ['New Hampshire', 'Eliot', 'Maine', 'Rockland', 'Maine', 'Warren', 'Maine', 'Portsmouth', 'N.H.', 'Western', 'U.S.', 'U.S.']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Ivy Lew', 'Casey', 'Elizabeth Buckle', 'Mr. Henneberry', 'Buckle', 'Henneberry', 'Henneberry', 'Henneberry', 'Henneberry', 'Buckle', 'Buckle', 'Henneberry', 'Fisheries Act', 'Law Fisheries', 'Crown', 'Buckle', 'Buckle']\n",
      "3 ['Henneberry', 'ALS Fisheries', 'Halifax', 'DFO', 'Ivy Lew', 'Law Fisheries', 'ALS Fisheries', 'ALS', 'Ivy Lew', 'Department', 'Sambro', 'ALS', 'ALS', 'ALS', 'DFO', 'Ivy Lew', 'ALS']\n",
      "4 ['Sambro', 'N.S.', 'Sambro', 'Fisheries', 'Oceans', 'Captain', 'Crown', 'Crown']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Jason Ahearn', 'James Gavin', 'Earl MacRae', 'Jeffery MacLeod', 'Casey Gavin', 'Nancy Orr', 'James Gavin', 'Court', 'Gavin', 'Gavin', 'Court', 'Leo Dorgan', 'Operation Gannet', 'Carter Hutt', 'Snow', 'Crab Association', 'Orr', 'Operation Gannet', 'Clifford Hood', 'Hood']\n",
      "3 ['Department', 'Operation Gannet', 'CBC']\n",
      "4 ['Georgetown', 'Georgetown', 'Fisheries', 'Oceans', 'Souris', 'Dorgan', 'Defence']\n",
      "=================New Article\n",
      "0 []\n",
      "1 []\n",
      "2 ['Glenn Robbins', 'Neil Herrick', 'Stephen Little', 'Ethan Chase', 'Jason Parent', 'Herald', 'Robbins', 'Herrick', 'Law Enforcement Director', 'James Landon', 'Herald']\n",
      "3 ['Owls Head', 'Commerce', 'NOAA', 'Parent', 'USD', 'EUR']\n",
      "4 ['Eliot', 'Maine', 'Rockland', 'Maine', 'Warren', 'Maine', 'Portsmouth', 'New Hampshire', 'Maine', 'Robbins', 'Maine', 'Maine', 'Portland', 'U.S.', 'Chase', 'Little', 'U.S.']\n"
     ]
    }
   ],
   "source": [
    "#People\n",
    "import nltk\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_names(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if isinstance(chunk, Tree) and chunk.label() == 'PERSON':\n",
    "                names.append(' '.join([token for token, pos in chunk.leaves()]))\n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# location\n",
    "import nltk\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_locations(text):\n",
    "    locations = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if isinstance(chunk, Tree) and chunk.label() == 'GPE':\n",
    "                locations.append(' '.join([token for token, pos in chunk.leaves()]))\n",
    "    return locations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# organization\n",
    "import nltk\n",
    "from nltk import ne_chunk, word_tokenize, pos_tag\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_organizations(text):\n",
    "    organizations = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if isinstance(chunk, Tree) and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join([token for token, pos in chunk.leaves()]))\n",
    "    return organizations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def info_extraction(text):\n",
    "    #give weigths to each information:\n",
    "    # IMO: 2\n",
    "    # MMSI: 2\n",
    "    # Involved parties or location: 1\n",
    "    # The score will be out of 10\n",
    "    score_list = [find_imo, find_mmsi, extract_names, extract_organizations, extract_locations]\n",
    "    weight_vector = np.array([2, 2, 1, 1, 1])\n",
    "    score = []\n",
    "    i=0\n",
    "    for item in score_list:\n",
    "        print(i, item(text))\n",
    "        i += 1\n",
    "        if len(item(text)) > 0:\n",
    "            score.append(1)\n",
    "        else:\n",
    "            score.append(0)\n",
    "    score = np.array(score)\n",
    "    weighted_score = score * weight_vector\n",
    "    #np.dot(score,weight_vector)\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "for article in relevant_articles:\n",
    "    print(\"=================New Article\")\n",
    "    info_extraction(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7102e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795deb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add279f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95e683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f30f1b",
   "metadata": {},
   "source": [
    "#### Fish species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ab663ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fishing', 'fishery', 'aquaculture', 'shellfish', 'aquatic', 'seaweed', 'finfish', 'seabass', 'sealife', 'native', 'codfish', 'mariculture', 'fishmeal', 'seawater', 'fruit of the sea', 'fruits of the sea', 'seafare', 'sea vegetable', 'seafish', 'sea fish', 'seafood boil', 'shellfishery', 'food fish', 'seafoodie', 'sea stock', 'foodfish', 'chalk fish', 'sea-weed', 'green caviar', 'shellfishing', 'other fishes in the sea', 'sea fennel', 'sea onion', 'geoduck', 'saltfish', 'shorefish', 'fishfood', 'sea lettuce', 'rock samphire', 'seafolk', 'sea pumpkin', 'sea dace', 'wallfish', 'fish stock', 'edible crab', 'aquafeed', 'land shrimp', 'fishlife', 'sea snail', 'european sea bass', 'dodman', 'sea potato', 'seafooder', 'sea orb', 'bottomfish', 'tofish', 'fish food', 'seasnail', 'sea salmon', 'chicken of the sea']\n"
     ]
    }
   ],
   "source": [
    "word_list = get_related_words('seafood', limit=60, topics=['fishing', 'species'])\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8e930b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'shark' is not in the list.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_word_in_list('shark', word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e36ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redfish',\n",
       " 'rockfish',\n",
       " 'humpback',\n",
       " 'blackfish',\n",
       " 'trout',\n",
       " 'sockeye',\n",
       " 'salmonid',\n",
       " 'smolt',\n",
       " 'brown',\n",
       " 'fingerling',\n",
       " 'pink',\n",
       " 'kipper',\n",
       " 'grayling',\n",
       " 'springer',\n",
       " 'sparling',\n",
       " 'salmon river',\n",
       " 'salmonine',\n",
       " 'samoun',\n",
       " 'atlantic salmon',\n",
       " 'salmonoid',\n",
       " 'black salmon',\n",
       " 'salmoniform',\n",
       " 'salmon pink',\n",
       " 'sea trout',\n",
       " 'king-of-the-salmon',\n",
       " 'silver salmon',\n",
       " 'pink salmon',\n",
       " 'cockfish',\n",
       " 'salmon trout',\n",
       " 'spring salmon',\n",
       " 'rock salmon',\n",
       " 'henfish',\n",
       " 'salmoner',\n",
       " 'coho salmon',\n",
       " 'sea salmon',\n",
       " 'landlocked salmon',\n",
       " 'spawner',\n",
       " 'wananish',\n",
       " 'sockeye salmon',\n",
       " 'dog salmon',\n",
       " 'seatrout',\n",
       " 'king salmon',\n",
       " 'samlet',\n",
       " 'mousefish',\n",
       " 'jack salmon',\n",
       " 'salmon peel',\n",
       " 'land-locked salmon',\n",
       " 'bull trout',\n",
       " 'white salmon',\n",
       " 'half-fish',\n",
       " 'beaked salmon',\n",
       " 'colonial salmon',\n",
       " 'silver bream',\n",
       " 'quisutsch',\n",
       " 'foul fish',\n",
       " 'salmonet',\n",
       " 'mykiss',\n",
       " 'humpback salmon',\n",
       " 'chum salmon',\n",
       " 'brown trout']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_words('salmon', limit=60, topics='vessel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d797eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_species = ['salmon', 'tuna', 'shrimp', 'crab', 'shark', 'sablefish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adac09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================New Article\n",
      "['fish', 'salmon']\n",
      "=================New Article\n",
      "['fish', 'sablefish']\n",
      "=================New Article\n",
      "['fish']\n",
      "=================New Article\n",
      "['mackerel', 'fish']\n",
      "=================New Article\n",
      "['lobster', 'fish']\n",
      "=================New Article\n",
      "[]\n",
      "=================New Article\n",
      "['crab']\n",
      "=================New Article\n",
      "['fish']\n",
      "=================New Article\n",
      "['albacore', 'tuna', 'fish']\n",
      "=================New Article\n",
      "['albacore', 'yellowfin', 'bigeye', 'tuna', 'fish', 'shark']\n",
      "=================New Article\n",
      "[]\n",
      "=================New Article\n",
      "['fish', 'shark']\n"
     ]
    }
   ],
   "source": [
    "fish_list = some_species\n",
    "for fish in some_species:\n",
    "    fish_list = list(set(fish_list).union(set(get_related_words(fish, limit = '100',topics = ['species', 'fishing']))))\n",
    "\n",
    "#print(fish_list)\n",
    "\n",
    "def find_words_in_list(text, word_list):\n",
    "#    nlp = spacy.load('en_core_web_sm')\n",
    "#    doc = nlp(text)\n",
    "    fish = []\n",
    "    text_words = text.split()\n",
    "    found_words = []\n",
    "\n",
    "    # Iterate over word_list and check if each word is in words_in_text\n",
    "    for word in word_list:\n",
    "        if word.lower() in text_words:\n",
    "            found_words.append(word)\n",
    "\n",
    "    return found_words\n",
    "\n",
    "\n",
    "\n",
    "for article in relevant_articles:\n",
    "    print(\"=================New Article\")\n",
    "    article = text_preprocess(article)\n",
    "    print(find_words_in_list(article,fish_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abd303ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fishery',\n",
       " 'fisherman',\n",
       " 'fishermen',\n",
       " 'seafood',\n",
       " 'tuna',\n",
       " 'halibut',\n",
       " 'lobster',\n",
       " 'crab',\n",
       " 'shellfish',\n",
       " 'shark',\n",
       " 'salmon',\n",
       " 'mackerel',\n",
       " 'shrimp',\n",
       " 'saltwater',\n",
       " 'sharks',\n",
       " 'trout',\n",
       " 'catfish',\n",
       " 'squid',\n",
       " 'swordfish',\n",
       " 'freshwater',\n",
       " 'lobsters',\n",
       " 'gamefish',\n",
       " 'dogfish',\n",
       " 'bait',\n",
       " 'crabs',\n",
       " 'grouper',\n",
       " 'crayfish',\n",
       " 'meat',\n",
       " 'whitebait',\n",
       " 'crustaceans',\n",
       " 'rockfish',\n",
       " 'steelhead',\n",
       " 'whitefish',\n",
       " 'fillet',\n",
       " 'carp',\n",
       " 'sunfish',\n",
       " 'flatfish',\n",
       " 'seafoods',\n",
       " 'finfish',\n",
       " 'walleye',\n",
       " 'sportfish',\n",
       " 'shrimps',\n",
       " 'bream',\n",
       " 'goldfish',\n",
       " 'salmonid',\n",
       " 'sturgeon',\n",
       " 'angle',\n",
       " 'trouts',\n",
       " 'burbot',\n",
       " 'lizardfish',\n",
       " 'fillets',\n",
       " 'minnows',\n",
       " 'tilapia',\n",
       " 'mug',\n",
       " 'bullheads',\n",
       " 'blackfish',\n",
       " 'gull',\n",
       " 'sucker',\n",
       " 'filets',\n",
       " 'bluegill',\n",
       " 'bluegills',\n",
       " 'milkfish',\n",
       " 'eels',\n",
       " 'salmonids',\n",
       " 'sticklebacks',\n",
       " 'chump',\n",
       " 'fool',\n",
       " 'patsy',\n",
       " 'soft touch',\n",
       " 'steelheads',\n",
       " 'brewis',\n",
       " 'mark',\n",
       " 'shlemiel',\n",
       " 'schlemiel',\n",
       " 'fall guy',\n",
       " 'go fish',\n",
       " 'goosefish',\n",
       " 'garfish',\n",
       " 'goldeye',\n",
       " 'breams',\n",
       " 'bloodworm',\n",
       " 'tilapias',\n",
       " 'needlefish',\n",
       " 'rockling',\n",
       " 'saury',\n",
       " 'greenling',\n",
       " 'cyprinids',\n",
       " 'yellowtails',\n",
       " 'butterfish',\n",
       " 'lunkers',\n",
       " 'squawfish',\n",
       " 'whitings',\n",
       " 'shimp',\n",
       " 'wolffish',\n",
       " 'mackerels',\n",
       " 'sockeyes',\n",
       " 'icefish',\n",
       " 'sheephead',\n",
       " 'cyprinid',\n",
       " 'planktons']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_words('fish', limit = '100', topics = ['vessel', 'fishing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c165176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a288ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
