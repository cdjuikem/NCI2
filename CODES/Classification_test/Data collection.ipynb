{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed92d77",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf945c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581a663",
   "metadata": {},
   "source": [
    "### 1. Import queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf90adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Query\n",
      "0            Vessel caught misreporting catch amount\n",
      "1              Vessel caught falsifying fishing logs\n",
      "2         Vessel caught with incorrect catch reports\n",
      "3  Vessel caught underreporting catch in North At...\n",
      "4           Vessel caught misreporting haddock catch\n",
      "5    Vessel caught with inaccurate fish size records\n",
      "6  Vessel caught underreporting catch near protec...\n",
      "7   Vessel caught misreporting tuna catch quantities\n",
      "8         Vessel caught with unrecorded fish species\n",
      "9               Vessel caught bypassing quota system\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the excel file\n",
    "excel_data = pd.read_excel('PIMS Sample Prompts.xlsx')\n",
    "\n",
    "queries = []\n",
    "for index, row in excel_data.iterrows():\n",
    "    # Process each row\n",
    "    queries.append(row['Prompt'])\n",
    "    \n",
    "queries = queries[:10]\n",
    "queries = pd.DataFrame(queries, columns = ['Query'])\n",
    "print(queries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5945f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f5990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168fa669",
   "metadata": {},
   "source": [
    "### 2. Data frame initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "237529d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Query, Engine, results, dates, days]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "numbers_and_dates = pd.DataFrame(columns = [\n",
    "    'Query', 'Engine',\n",
    "    'results', 'dates', 'days'\n",
    "])\n",
    "\n",
    "search_results_articles = pd.DataFrame(columns = ['Query', 'Engine','Title', 'Dates', 'Link'])\n",
    "print(numbers_and_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472d69c",
   "metadata": {},
   "source": [
    "### 3. Google, Yahoo, Bing news search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4785520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "# API Scrap functions\n",
    "\n",
    "def google_news_search(query, location = 'Canada'):\n",
    "    params = {\n",
    "        \"api_key\": \"7020efc655aafd1998bfad82b8f96fee27be3cdb26c1bb67909ba7042c56ebd1\",\n",
    "        \"q\": query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"location\": location,\n",
    "        \"num\": \"20\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    news_results = results[\"news_results\"]\n",
    "    return news_results\n",
    "\n",
    "def bing_news_search(query, location = \"CA\"):\n",
    "    params = {\n",
    "      \"api_key\": \"7020efc655aafd1998bfad82b8f96fee27be3cdb26c1bb67909ba7042c56ebd1\",\n",
    "      \"engine\": \"bing_news\",\n",
    "      \"q\": query,\n",
    "      \"cc\": location,\n",
    "      \"count\": \"20\"\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    news_results = results[\"organic_results\"]\n",
    "    return news_results\n",
    "\n",
    "    \n",
    "def duckduckgo_news_search(query, location = \"ca-en\"):\n",
    "    params = {\n",
    "        \"engine\": \"duckduckgo\",\n",
    "        \"q\": query,\n",
    "        \"kl\": location,\n",
    "        \"tbm\": \"nws\",\n",
    "        #\"api_key\": \"7020efc655aafd1998bfad82b8f96fee27be3cdb26c1bb67909ba7042c56ebd1\"\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    print(results)\n",
    "#    news_results = results[\"news_results\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "#duckduckgo_news_search(\"vessel underreport\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8cd03",
   "metadata": {},
   "source": [
    "### 4. Search and scrap the results (title, dates, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f8120d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def convert_day(text):\n",
    "    if None:\n",
    "        return None\n",
    "    \n",
    "    pattern1 = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "    pattern2 = r'\\b\\d+[hdwmy]'\n",
    "    pattern3 = r'^\\d{2}/\\d{2}/\\d{4}$'\n",
    "    pattern4 = r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}'\n",
    "    pattern5 = r'\\b\\d+(?:hrs|day|mon|wks|yrs)\\b'\n",
    "\n",
    "    if re.match(pattern1, text):\n",
    "        date = datetime.strptime(text, \"%Y-%M-%d\").date()\n",
    "        today = datetime.combine(date.today(), datetime.min.time())\n",
    "        days = str(today - date).days\n",
    "        return int(days[0])\n",
    "    pattern2match = re.findall(pattern2, text)\n",
    "    pattern5match = re.findall(pattern5,text)\n",
    "    if pattern2match:\n",
    "        text = pattern2match[0]\n",
    "        if text[len(text)-1] == 'h':\n",
    "            return 1\n",
    "        if text[len(text)-1] == 'd':\n",
    "            return int(text[:len(text)-1])\n",
    "        if text[len(text)-1] == 'w':\n",
    "            return int(text[:len(text)-1])*7\n",
    "        if text[len(text)-1] == 'm':\n",
    "            return int(text[:len(text)-1])*30\n",
    "        if text[len(text)-1] == 'y':\n",
    "            return int(text[:len(text)-1])*365    \n",
    "        \n",
    "    pattern5match = re.findall(pattern5,text)\n",
    "    if pattern5match:\n",
    "        text = pattern5match[0]\n",
    "        if text[len(text)-1] == 'hrs':\n",
    "            return 1\n",
    "        if text[len(text)-1] == 'day':\n",
    "            return int(text[:len(text)-1])\n",
    "        if text[len(text)-1] == 'wks':\n",
    "            return int(text[:len(text)-1])*7\n",
    "        if text[len(text)-1] == 'mon':\n",
    "            return int(text[:len(text)-1])*30\n",
    "        if text[len(text)-1] == 'yrs':\n",
    "            return int(text[:len(text)-1])*365   \n",
    "        \n",
    "    pattern3match = re.match(pattern3, text)\n",
    "    if pattern3match:\n",
    "        date_format = \"%m/%d/%Y\"\n",
    "        try:\n",
    "            # Parse the input date string into a datetime object\n",
    "            given_date = datetime.strptime(text, date_format).date()\n",
    "            # Get today's date\n",
    "            today = date.today()\n",
    "            # Calculate the difference in days\n",
    "            days_difference = (today - given_date).days\n",
    "            return int(days_difference)\n",
    "        except ValueError:\n",
    "            # Handle invalid date format or other errors\n",
    "            return None\n",
    "        \n",
    "    pattern4match = re.match(pattern4, text)\n",
    "    if pattern4match:\n",
    "        date_format = '%b %d, %Y'\n",
    "        try:\n",
    "            # Parse the input date string into a datetime object\n",
    "            given_date = datetime.strptime(text, date_format).date()\n",
    "            # Get today's date\n",
    "            today = datetime.now().date()\n",
    "            # Calculate the difference in days\n",
    "            days_difference = (today - given_date).days\n",
    "            return int(days_difference)\n",
    "\n",
    "        except ValueError:\n",
    "            # Handle invalid date format or other errors\n",
    "            return None\n",
    "    txtlist = text.split()\n",
    "    if len(txtlist) <2:\n",
    "#        print('txtlist is too short', len(txtlist), text)\n",
    "        return None\n",
    "    if txtlist[1] in ['hour', 'hours']:\n",
    "        return 1\n",
    "    if txtlist[1] in ['day', 'days']:\n",
    "        return int(txtlist[0])\n",
    "    if txtlist[1] in ['week', 'weeks']:\n",
    "        return int(txtlist[0])*7\n",
    "    if txtlist[1] in ['month', 'months']:\n",
    "        return int(txtlist[0])*30\n",
    "    if txtlist[1] in ['year', 'years']:\n",
    "        return int(txtlist[0])*365\n",
    "    return None\n",
    "\n",
    "convert_day('12 months ago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73e33561",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_functions = [\n",
    "    google_news_search,\n",
    "    #scrape_yahoo_news\n",
    "    #yahoo_news_search, \n",
    "    bing_news_search,\n",
    "    #duckduckgo_news_search\n",
    "]\n",
    "search_engine = ['Google', \n",
    "                 #'Yahoo', \n",
    "                 'Bing',\n",
    "                 #'DuckDuckGo'\n",
    "                ]\n",
    "\n",
    "#Google\n",
    "#search_results = dict()\n",
    "#search_results = [{'position' : 1, 'title': 'title', 'date': '20240621', 'link': 'http://google.com'}]\n",
    "#queries = ['query1', 'query 2']\n",
    "for indq, query in queries.iterrows():\n",
    "    for i in range(len(search_functions)):\n",
    "        # Initialize numbers and dates data row with query and search engine\n",
    "        num_dat_row = [query['Query'], search_engine[i]]\n",
    "        \n",
    "        # Do the search and only get top 20 \n",
    "        search_results = search_functions[i](query['Query'])\n",
    "        search_results = search_results[:min(20, len(search_results))]\n",
    "        \n",
    "        # Append  the number of articles to the num_dat_row\n",
    "        num_dat_row.append(len(search_results))\n",
    "        dates = []\n",
    "        days = []\n",
    "    \n",
    "        for article in search_results:\n",
    "            # search_data is a new data for the data frame search_results_articles\n",
    "            # 'Engine', 'Position', 'Title', 'Dates', 'Link', 'Content'\n",
    "            search_data = [\n",
    "                query['Query'],\n",
    "                search_engine[i],\n",
    "                article['title'],\n",
    "                article['date'],\n",
    "                article['link']\n",
    "            ]\n",
    "            search_results_articles.loc[len(search_results_articles)] = search_data\n",
    "            dates.append(search_data[3].strip())\n",
    "            days.append(convert_day(search_data[3].strip()))\n",
    "        # Dates and days information for the num_dat_row\n",
    "        num_dat_row.append(dates)\n",
    "        num_dat_row.append(days)\n",
    "        \n",
    "        # Add the num_dat_row to the data frame numbers_and_dates\n",
    "        numbers_and_dates.loc[len(numbers_and_dates)] = num_dat_row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e18c42",
   "metadata": {},
   "source": [
    "### Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17232183",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### For yahoo..\n",
    "# To count the number of articles\n",
    "def count_result(search_results):\n",
    "    count = 0\n",
    "    for index, result in enumerate(search_results, start=1):\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c0474da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vessel caught misreporting catch amount 0\n",
      "Vessel caught falsifying fishing logs 0\n",
      "Vessel caught with incorrect catch reports 6\n",
      "Vessel caught underreporting catch in North Atlantic 0\n",
      "Vessel caught misreporting haddock catch 0\n",
      "Vessel caught with inaccurate fish size records 0\n",
      "Vessel caught underreporting catch near protected area 0\n",
      "Vessel caught misreporting tuna catch quantities 0\n",
      "Vessel caught with unrecorded fish species 0\n",
      "Vessel caught bypassing quota system 0\n"
     ]
    }
   ],
   "source": [
    "######### Yahoo \n",
    "for indq, query in queries.iterrows():\n",
    "        # Initialize numbers and dates data row with query and search engine\n",
    "        num_dat_row = [query['Query'], 'Yahoo']\n",
    "        \n",
    "        # Do the search and only get top 20 \n",
    "        search_results = scrape_yahoo_news(query['Query'])\n",
    "        \n",
    "        # Append  the number of articles to the num_dat_row\n",
    "        count = count_result(search_results)\n",
    "        print(query['Query'],count)\n",
    "        num_dat_row.append(count)\n",
    "        dates = []\n",
    "        days = []\n",
    "    \n",
    "        for index, article in enumerate(search_results, start=1):\n",
    "            # search_data is a new data for the data frame search_results_articles\n",
    "            # 'Engine', 'Position', 'Title', 'Dates', 'Link', 'Content'\n",
    "            search_data = [\n",
    "                query['Query'],\n",
    "                'Yahoo',\n",
    "                article['title'],\n",
    "                article['date'],\n",
    "                article['link']\n",
    "            ]\n",
    "            search_results_articles.loc[len(search_results_articles)] = search_data\n",
    "            dates.append(search_data[3].strip())\n",
    "            days.append(convert_day(search_data[3].strip()))\n",
    "        # Dates and days information for the num_dat_row\n",
    "        num_dat_row.append(dates)\n",
    "        num_dat_row.append(days)\n",
    "        \n",
    "        # Add the num_dat_row to the data frame numbers_and_dates\n",
    "        numbers_and_dates.loc[len(numbers_and_dates)] = num_dat_row\n",
    "\n",
    "\n",
    "    \n",
    "#print(search_results_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84af8ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Engine</th>\n",
       "      <th>Title</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sea Shepherd Global</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>https://www.seashepherdglobal.org/latest-news/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Move the IUU Fight Up the Food Chain | Proceed...</td>\n",
       "      <td>Oct 31, 2023</td>\n",
       "      <td>https://www.usni.org/magazines/proceedings/202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>New rules tighten controls on EU…</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>https://ejfoundation.org/news-media/new-rules-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Surprise! Media is misreporting the source of ...</td>\n",
       "      <td>Jul 26, 2023</td>\n",
       "      <td>https://electrek.co/2023/07/26/surprise-media-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sea Shepherd Global</td>\n",
       "      <td>Mar 25, 2024</td>\n",
       "      <td>https://www.seashepherdglobal.org/latest-news/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>N.S. boat captain, 2 companies fined $125K for...</td>\n",
       "      <td>Sep 13, 2023</td>\n",
       "      <td>https://www.cbc.ca/news/canada/nova-scotia/n-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Caught out: Cameras on boats reveal massive un...</td>\n",
       "      <td>Apr 13, 2024</td>\n",
       "      <td>https://www.thepost.co.nz/politics/350242260/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>NOAA, Coast Guard: More regs needed in wake of...</td>\n",
       "      <td>Jan 17, 2022</td>\n",
       "      <td>https://www.heraldnews.com/story/news/2019/12/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Fish and Overfishing</td>\n",
       "      <td>Oct 11, 2021</td>\n",
       "      <td>https://ourworldindata.org/fish-and-overfishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Alaska fisherman given USD 1 million fine, jai...</td>\n",
       "      <td>Aug 9, 2021</td>\n",
       "      <td>https://www.seafoodsource.com/news/business-fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Electronic monitoring long-awaited boon for Ca...</td>\n",
       "      <td>Oct 9, 2020</td>\n",
       "      <td>https://www.capecodtimes.com/story/news/2020/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>The Tuna Transparency Pledge</td>\n",
       "      <td>Jun 29, 2023</td>\n",
       "      <td>https://www.nature.org/en-us/what-we-do/our-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Fish crimes in the global oceans</td>\n",
       "      <td>Mar 23, 2022</td>\n",
       "      <td>https://www.science.org/doi/10.1126/sciadv.abj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Catch Me If You Can: The Global Pursuit of a F...</td>\n",
       "      <td>Mar 3, 2020</td>\n",
       "      <td>https://hakaimagazine.com/features/catch-me-if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Report Finds Transshipments in Western and Cen...</td>\n",
       "      <td>Sep 12, 2019</td>\n",
       "      <td>https://www.pewtrusts.org/en/research-and-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Plenty more fish in the sea? How the insurance...</td>\n",
       "      <td>Jan 30, 2022</td>\n",
       "      <td>https://www.axa.com/en/insights/plenty-more-fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Reporting the accuracy of small-scale fishing ...</td>\n",
       "      <td>Jul 26, 2022</td>\n",
       "      <td>https://www.frontiersin.org/articles/10.3389/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Inaccurate EU catch reporting…</td>\n",
       "      <td>Feb 3, 2022</td>\n",
       "      <td>https://ejfoundation.org/news-media/allowing-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Electronic Monitoring in Large-Scale Fisheries...</td>\n",
       "      <td>Jan 3, 2019</td>\n",
       "      <td>https://www.nature.org/en-us/what-we-do/our-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vessel caught misreporting catch amount</td>\n",
       "      <td>Google</td>\n",
       "      <td>Transforming the Last Tuna Stronghold</td>\n",
       "      <td>Oct 8, 2018</td>\n",
       "      <td>https://www.nature.org/en-us/about-us/where-we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Query  Engine  \\\n",
       "0   Vessel caught misreporting catch amount  Google   \n",
       "1   Vessel caught misreporting catch amount  Google   \n",
       "2   Vessel caught misreporting catch amount  Google   \n",
       "3   Vessel caught misreporting catch amount  Google   \n",
       "4   Vessel caught misreporting catch amount  Google   \n",
       "5   Vessel caught misreporting catch amount  Google   \n",
       "6   Vessel caught misreporting catch amount  Google   \n",
       "7   Vessel caught misreporting catch amount  Google   \n",
       "8   Vessel caught misreporting catch amount  Google   \n",
       "9   Vessel caught misreporting catch amount  Google   \n",
       "10  Vessel caught misreporting catch amount  Google   \n",
       "11  Vessel caught misreporting catch amount  Google   \n",
       "12  Vessel caught misreporting catch amount  Google   \n",
       "13  Vessel caught misreporting catch amount  Google   \n",
       "14  Vessel caught misreporting catch amount  Google   \n",
       "15  Vessel caught misreporting catch amount  Google   \n",
       "16  Vessel caught misreporting catch amount  Google   \n",
       "17  Vessel caught misreporting catch amount  Google   \n",
       "18  Vessel caught misreporting catch amount  Google   \n",
       "19  Vessel caught misreporting catch amount  Google   \n",
       "\n",
       "                                                Title         Dates  \\\n",
       "0                                 Sea Shepherd Global   2 weeks ago   \n",
       "1   Move the IUU Fight Up the Food Chain | Proceed...  Oct 31, 2023   \n",
       "2                   New rules tighten controls on EU…   1 month ago   \n",
       "3   Surprise! Media is misreporting the source of ...  Jul 26, 2023   \n",
       "4                                 Sea Shepherd Global  Mar 25, 2024   \n",
       "5   N.S. boat captain, 2 companies fined $125K for...  Sep 13, 2023   \n",
       "6   Caught out: Cameras on boats reveal massive un...  Apr 13, 2024   \n",
       "7   NOAA, Coast Guard: More regs needed in wake of...  Jan 17, 2022   \n",
       "8                                Fish and Overfishing  Oct 11, 2021   \n",
       "9   Alaska fisherman given USD 1 million fine, jai...   Aug 9, 2021   \n",
       "10  Electronic monitoring long-awaited boon for Ca...   Oct 9, 2020   \n",
       "11                       The Tuna Transparency Pledge  Jun 29, 2023   \n",
       "12                   Fish crimes in the global oceans  Mar 23, 2022   \n",
       "13  Catch Me If You Can: The Global Pursuit of a F...   Mar 3, 2020   \n",
       "14  Report Finds Transshipments in Western and Cen...  Sep 12, 2019   \n",
       "15  Plenty more fish in the sea? How the insurance...  Jan 30, 2022   \n",
       "16  Reporting the accuracy of small-scale fishing ...  Jul 26, 2022   \n",
       "17                     Inaccurate EU catch reporting…   Feb 3, 2022   \n",
       "18  Electronic Monitoring in Large-Scale Fisheries...   Jan 3, 2019   \n",
       "19              Transforming the Last Tuna Stronghold   Oct 8, 2018   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://www.seashepherdglobal.org/latest-news/...  \n",
       "1   https://www.usni.org/magazines/proceedings/202...  \n",
       "2   https://ejfoundation.org/news-media/new-rules-...  \n",
       "3   https://electrek.co/2023/07/26/surprise-media-...  \n",
       "4   https://www.seashepherdglobal.org/latest-news/...  \n",
       "5   https://www.cbc.ca/news/canada/nova-scotia/n-s...  \n",
       "6   https://www.thepost.co.nz/politics/350242260/c...  \n",
       "7   https://www.heraldnews.com/story/news/2019/12/...  \n",
       "8     https://ourworldindata.org/fish-and-overfishing  \n",
       "9   https://www.seafoodsource.com/news/business-fi...  \n",
       "10  https://www.capecodtimes.com/story/news/2020/1...  \n",
       "11  https://www.nature.org/en-us/what-we-do/our-pr...  \n",
       "12  https://www.science.org/doi/10.1126/sciadv.abj...  \n",
       "13  https://hakaimagazine.com/features/catch-me-if...  \n",
       "14  https://www.pewtrusts.org/en/research-and-anal...  \n",
       "15  https://www.axa.com/en/insights/plenty-more-fi...  \n",
       "16  https://www.frontiersin.org/articles/10.3389/f...  \n",
       "17  https://ejfoundation.org/news-media/allowing-i...  \n",
       "18  https://www.nature.org/en-us/what-we-do/our-pr...  \n",
       "19  https://www.nature.org/en-us/about-us/where-we...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_and_dates.head(20)\n",
    "search_results_articles.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c781eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Query  Engine  results  \\\n",
      "0             Vessel caught misreporting catch amount  Google       20   \n",
      "1             Vessel caught misreporting catch amount    Bing       14   \n",
      "2               Vessel caught falsifying fishing logs  Google       20   \n",
      "3               Vessel caught falsifying fishing logs    Bing       14   \n",
      "4          Vessel caught with incorrect catch reports  Google       20   \n",
      "5          Vessel caught with incorrect catch reports    Bing       12   \n",
      "6   Vessel caught underreporting catch in North At...  Google       14   \n",
      "7   Vessel caught underreporting catch in North At...    Bing       11   \n",
      "8            Vessel caught misreporting haddock catch  Google        2   \n",
      "9            Vessel caught misreporting haddock catch    Bing       14   \n",
      "10    Vessel caught with inaccurate fish size records  Google       20   \n",
      "11    Vessel caught with inaccurate fish size records    Bing       14   \n",
      "12  Vessel caught underreporting catch near protec...  Google       20   \n",
      "13  Vessel caught underreporting catch near protec...    Bing       14   \n",
      "14   Vessel caught misreporting tuna catch quantities  Google       13   \n",
      "15   Vessel caught misreporting tuna catch quantities    Bing       14   \n",
      "16         Vessel caught with unrecorded fish species  Google       20   \n",
      "17         Vessel caught with unrecorded fish species    Bing       14   \n",
      "18               Vessel caught bypassing quota system  Google        5   \n",
      "19               Vessel caught bypassing quota system    Bing       14   \n",
      "\n",
      "                                                dates  \\\n",
      "0   [2 weeks ago, Oct 31, 2023, 1 month ago, Jul 2...   \n",
      "1   [13d, 12d, 12d, 13d, 8d, 12d, 8d, 12d, 13d, 8d...   \n",
      "2   [Mar 20, 2024, Oct 4, 2023, Feb 22, 2024, Sep ...   \n",
      "3   [18d, 12d, 2d, 24d, 2mon, 3d, 1mon, 15d, 1mon,...   \n",
      "4   [3 days ago, Mar 23, 2024, Oct 9, 2023, Jan 2,...   \n",
      "5   [2d, 2d, 12d, 3d, 5d, 8d, 3d, 12d, 12d, 12d, 1...   \n",
      "6   [May 24, 2023, Sep 26, 2022, Jan 19, 2016, May...   \n",
      "7   [8h, 1mon, 3d, 12d, 9d, 8d, 22d, 15d, 3d, 12d,...   \n",
      "8                         [Oct 9, 2020, Sep 28, 2022]   \n",
      "9   [1mon, 3d, 23d, 4mon, 2mon, 4y, 7mon, 15d, 14y...   \n",
      "10  [3 days ago, Aug 23, 2023, Nov 6, 2023, Feb 3,...   \n",
      "11  [6y, 2y, 4d, 3d, 14d, 1y, 3y, 3d, 2y, 1mon, 4m...   \n",
      "12  [Mar 1, 2024, Apr 2, 2024, 1 month ago, Apr 13...   \n",
      "13  [4d, 1mon, 19d, 15d, 1mon, 2mon, 23d, 17d, 12d...   \n",
      "14  [Oct 11, 2021, Jan 3, 2019, Jun 14, 2021, Feb ...   \n",
      "15  [2mon, 1y, 6d, 4d, 6y, 3d, 2y, 3d, 2y, 2y, 7d,...   \n",
      "16  [1 week ago, Jul 24, 2023, Nov 6, 2023, Apr 10...   \n",
      "17  [2y, 2y, 2y, 3d, 3d, 12d, 12d, 27d, 1mon, 15d,...   \n",
      "18  [2 weeks ago, Jul 17, 2023, May 9, 2016, Jun 1...   \n",
      "19  [2y, 2d, 2y, 8d, 6d, 15d, 12d, 9d, 7d, 1d, 15d...   \n",
      "\n",
      "                                                 days  \n",
      "0   [14, 234, 30, 331, 88, 282, 69, 886, 984, 1047...  \n",
      "1   [13, 12, 12, 13, 8, 12, 8, 12, 13, 8, 12, 12, ...  \n",
      "2   [93, 261, 120, 268, 7, 295, 192, 207, 294, 309...  \n",
      "3   [18, 12, 2, 24, None, 3, None, 15, None, None,...  \n",
      "4   [3, 90, 256, 171, 227, 241, 331, 351, 288, 288...  \n",
      "5          [2, 2, 12, 3, 5, 8, 3, 12, 12, 12, 12, 12]  \n",
      "6   [394, 634, 3076, 408, 3075, 3791, 463, 2481, 1...  \n",
      "7        [None, None, 3, 12, 9, 8, 22, 15, 3, 12, 12]  \n",
      "8                                         [1351, 632]  \n",
      "9   [None, 3, 23, None, None, 120, None, 15, 420, ...  \n",
      "10  [3, 303, 228, 504, 984, 413, 441, 4412, 1138, ...  \n",
      "11  [180, 60, 4, 3, 14, 30, 90, 3, 60, None, None,...  \n",
      "12  [112, 80, 30, 69, 3642, 598, 3417, 1032, 3951,...  \n",
      "13  [4, None, 19, 15, None, None, 23, 17, 12, 15, ...  \n",
      "14  [984, 1996, 1103, 869, 4972, 2698, 1336, 4480,...  \n",
      "15  [None, 30, 6, 4, 180, 3, 60, 3, 60, 60, 7, 6, ...  \n",
      "16  [7, 333, 228, 72, 380, 366, 203, 1485, 253, 10...  \n",
      "17  [60, 60, 60, 3, 3, 12, 12, 27, None, 15, 8, 6,...  \n",
      "18                        [14, 340, 2965, 5122, 1676]  \n",
      "19  [60, 2, 60, 8, 6, 15, 12, 9, 7, 1, 15, 23, 12, 3]  \n"
     ]
    }
   ],
   "source": [
    "print(numbers_and_dates.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3ce8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e8f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a6bc4d",
   "metadata": {},
   "source": [
    "## Export the data (excel file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93560e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "####### Have to have numberes_and_dates as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "numbers_and_dates.to_excel('numbers_and_dates.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8f610ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been saved to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "######## Have to have search_results_articles as a pandas data frame\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "search_results_articles.to_excel('search_results_articles.xlsx', index=False)\n",
    "\n",
    "print(\"DataFrame has been saved to output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7c8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
