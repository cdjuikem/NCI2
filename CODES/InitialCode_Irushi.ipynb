{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45ae08d0-b6bb-42cb-811d-8424a9d281e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1ea1bf6-11ca-4c68-91d3-260c3eae246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://www.bbc.com/news/uk-53699511')\n",
    "soup = BeautifulSoup(url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c10908d9-c20a-48f6-a783-cc9eb66a03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find_all(\"div\", class_ = 'entry-content')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "for paragraph in content:\n",
    "    text = paragraph.get_text(separator='\\n')\n",
    "    text = clean_text(text)\n",
    "    text_word = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af02755d-699d-43e0-8e74-eacf1154e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter(text_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de687ae3-ff84-466a-a9a6-a4555881f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "417ab9ed-9377-498e-96e8-febc2907ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  count\n",
      "0     notes      1\n",
      "1        to     16\n",
      "2       the     32\n",
      "3       new      1\n",
      "4       cno      2\n",
      "..      ...    ...\n",
      "258    port      1\n",
      "259    city      1\n",
      "260   lanka      1\n",
      "261     via      1\n",
      "262  xinhua      1\n",
      "\n",
      "[263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(word_count.items(), columns = ['word', 'count'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bb46e-2af1-4a48-94d0-7adb7abccbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df744a25-ef1f-4032-aa8c-751bd36f775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the web page.\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.bbc.com/news/uk-53699511')\n",
    "if url.status_code == 200:\n",
    "    print(\"Successfully fetched the web page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {url.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "737ffe96-3615-4339-85ca-e57544e06bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(url.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18f2391e-187a-41df-b8d6-cfcaf70d9c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of content blocks found: 0\n"
     ]
    }
   ],
   "source": [
    "content = soup.find_all(\"div\", class_='entry-content')\n",
    "print(f\"Number of content blocks found: {len(content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7877342-63b2-4520-92d3-fc70b7a979e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paragraph in content:\n",
    "    text = paragraph.get_text(separator='\\n')\n",
    "    text = clean_text(text)\n",
    "    text_word = text.split()\n",
    "    word_counts.update(text_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e733d8bb-f3b2-4f86-aa73-fc6d7a5c1746",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(word_counts\u001b[38;5;241m.\u001b[39mitems(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_counts' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8ffd9-5c92-47b7-9a47-38e5c0e87545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f9a756b-1a09-4991-b9fa-040cb4ed2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the web page.\n",
      "Number of content blocks found: 43\n",
      "Word counts: Counter({'the': 57, 'in': 35, 'to': 27, 'of': 27, 'people': 21, 'uk': 14, 'and': 14, 'a': 13, 'were': 13, '2023': 12, 'asylum': 11, 'small': 9, 'for': 9, 'as': 8, 'than': 7, 'had': 7, '2024': 7, 'number': 7, 'that': 6, 'arrivals': 6, 'on': 6, 'more': 6, 'who': 6, 'from': 5, 'channel': 5, 'boats': 5, 'came': 5, 'this': 5, 'by': 5, 'boat': 5, 'about': 5, 'at': 5, 'including': 5, 'their': 5, 'is': 5, 'with': 5, 'period': 4, '2022': 4, 'since': 4, '2018': 4, 'come': 4, 'all': 4, 'not': 4, 'claims': 4, 'refugees': 4, 'home': 4, 'them': 4, 'granted': 4, 'returned': 4, 'europe': 4, 'government': 3, 'some': 3, 'june': 3, 'same': 3, 'years': 3, 'was': 3, 'which': 3, 'highest': 3, 'figures': 3, 'have': 3, 'year': 3, 'march': 3, 'fifth': 3, 'between': 3, 'those': 3, 'applications': 3, 'arrived': 3, 'country': 3, 'or': 3, 'made': 3, 'decisions': 3, 'seekers': 2, 'rwanda': 2, 'crossing': 2, 'english': 2, 'however': 2, 'pledged': 2, 'almost': 2, 'crossed': 2, 'numbers': 2, 'whole': 2, 'way': 2, 'be': 2, 'nearly': 2, 'top': 2, 'nationality': 2, 'up': 2, 'just': 2, 'under': 2, '12': 2, 'next': 2, 'two': 2, 'most': 2, 'months': 2, 'least': 2, 'migrants': 2, 'died': 2, 'april': 2, 'children': 2, 'after': 2, 'into': 2, 'dependants': 2, 'peaked': 2, 'fled': 2, 'afghanistan': 2, 'syria': 2, 'followed': 2, 'claiming': 2, 'these': 2, 'ukrainian': 2, 'are': 2, 'been': 2, 'legal': 2, 'there': 2, 'other': 2, 'such': 2, 'an': 2, 'end': 2, 'but': 2, 'still': 2, 'decision': 2, 'december': 2, 'initial': 2, 'office': 2, 'returns': 2, 'sea': 2, 'italy': 2, 'spain': 2, 'received': 2, 'applicants': 2, 'germany': 2, 'france': 2, 'bbc': 2, 'external': 2, 'wants': 1, 'send': 1, 'deter': 1, 'labour': 1, 'leader': 1, 'keir': 1, 'starmer': 1, 'has': 1, 'scrap': 1, 'scheme': 1, 'if': 1, 'he': 1, 'wins': 1, 'general': 1, 'election': 1, 'instead': 1, 'plans': 1, 'use': 1, 'counterterrorism': 1, 'powers': 1, 'against': 1, 'gangs': 1, 'smuggling': 1, 'across': 1, '5': 1, '10800': 1, 'above': 1, 'previous': 1, 'four': 1, '29437': 1, 'big': 1, 'drop': 1, 'total': 1, '45755': 1, 'began': 1, 'collected': 1, '120000': 1, 'route': 1, 'ending': 1, 'afghans': 1, 'making': 1, 'iranians': 1, 'turkish': 1, 'nationals': 1, '11': 1, 'common': 1, 'nationalities': 1, 'arrive': 1, '85': 1, 'male': 1, 'where': 1, 'age': 1, 'recorded': 1, '25': 1, '39': 1, 'old': 1, 'international': 1, 'organization': 1, 'migration': 1, 'estimates': 1, '159': 1, 'five': 1, 'sevenyearold': 1, 'girl': 1, '23': 1, '6': 1, 'around': 1, '80': 1, 'three': 1, 'rescued': 1, 'got': 1, 'trouble': 1, '84000': 1, 'claimed': 1, '100000': 1, 'requested': 1, 'accounted': 1, '45': 1, 'equivalent': 1, 'figure': 1, 'available': 1, 'annual': 1, '103000': 1, '2002': 1, 'conflicts': 1, 'somalia': 1, 'iraq': 1, 'then': 1, 'fell': 1, 'sharply': 1, 'dropping': 1, '20year': 1, 'low': 1, '22600': 1, '2010': 1, 'rose': 1, 'again': 1, 'throughout': 1, '2010s': 1, 'largest': 1, '9307': 1, 'biggest': 1, 'group': 1, '7400': 1, 'iran': 1, 'pakistan': 1, 'india': 1, 'bangladesh': 1, 'albanians': 1, '17300': 1, '73': 1, 'russias': 1, 'invasion': 1, 'included': 1, '3': 1, '258700': 1, 'visas': 1, 'issued': 1, 'through': 1, 'routes': 1, 'set': 1, 'separate': 1, 'arrangements': 1, 'few': 1, 'specific': 1, 'groups': 1, 'afghan': 1, 'hong': 1, 'kong': 1, 'citizens': 1, 'wait': 1, 'several': 1, 'even': 1, 'considered': 1, 'delays': 1, 'system': 1, 'created': 1, 'overall': 1, 'backlog': 1, '128000': 1, 'prime': 1, 'minister': 1, 'clear': 1, 'older': 1, '4500': 1, 'complex': 1, 'cases': 1, 'awaiting': 1, 'latest': 1, 'show': 1, '14': 1, '2377': 1, 'waiting': 1, 'can': 1, 'remove': 1, 'no': 1, 'right': 1, 'stay': 1, 'refuse': 1, 'let': 1, 'enter': 1, '6014': 1, 'double': 1, '2931': 1, 'mainly': 1, 'because': 1, 'rise': 1, 'albanian': 1, 'increased': 1, '988': 1, '3369': 1, '1889': 1, '2580': 1, '2': 1, 'revealed': 1, 'only': 1, '408': 1, 'nonalbanians': 1, 'countries': 1, '2020': 1, '263048': 1, 'receiving': 1, '60': 1, '157314': 1, '57071': 1, 'greece': 1, '41561': 1, '17': 1, '40000': 1, 'via': 1, 'start': 1, '2015': 1, 'when': 1, 'million': 1, 'its': 1, 'borders': 1, 'majority': 1, 'fleeing': 1, 'conflict': 1, '329035': 1, 'quarter': 1, 'firsttime': 1, 'within': 1, 'eu': 1, 'second': 1, '160460': 1, '145095': 1, '130565': 1, '93303': 1, 'attached': 1, 'dependents': 1, 'thirds': 1, '67': 1, '217430': 1, '62': 1, 'similarsized': 1, 'population': 1, '132695': 1, '31': 1, 'copyright': 1, 'rights': 1, 'reserved': 1, 'responsible': 1, 'content': 1, 'sites': 1, 'read': 1, 'our': 1, 'approach': 1, 'linking': 1})\n",
      "           word  count\n",
      "0           the     57\n",
      "1            uk     14\n",
      "2    government      3\n",
      "3         wants      1\n",
      "4            to     27\n",
      "..          ...    ...\n",
      "358       sites      1\n",
      "359        read      1\n",
      "360         our      1\n",
      "361    approach      1\n",
      "362     linking      1\n",
      "\n",
      "[363 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = requests.get('https://www.bbc.com/news/uk-53699511')    #(small boats illegal immigration)\n",
    "\n",
    "# Check if the request was successful\n",
    "if url.status_code == 200:\n",
    "    print(\"Successfully fetched the web page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {url.status_code}\")\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(url.text, \"html.parser\")\n",
    "\n",
    "# Find all <div> tags with class 'entry-content'\n",
    "content = soup.find_all(\"p\", class_='sc-eb7bd5f6-0 fYAfXe')\n",
    "print(f\"Number of content blocks found: {len(content)}\")\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Initialize a Counter object to hold the word counts\n",
    "word_counts = Counter()\n",
    "\n",
    "# Iterate through the content and process each paragraph\n",
    "for paragraph in content:\n",
    "    # Extract text from the paragraph\n",
    "    text = paragraph.get_text(separator='\\n')\n",
    "    # Clean the extracted text\n",
    "    text = clean_text(text)\n",
    "    # Split the cleaned text into words\n",
    "    text_word = text.split()\n",
    "    # Update the Counter with words from the current paragraph\n",
    "    word_counts.update(text_word)\n",
    "\n",
    "# Print the word counts\n",
    "print(\"Word counts:\", word_counts)\n",
    "\n",
    "# Convert the word counts to a DataFrame\n",
    "df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e995de30-898d-4f0c-9795-b9d13fc295f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 403\n",
      "Number of content blocks found: 37\n",
      "Word counts: Counter({'the': 66, 'of': 30, 'to': 29, 'and': 27, 'a': 24, 'trafficking': 24, 'for': 18, 'human': 17, 'in': 15, 'maritime': 13, 'can': 13, 'is': 11, 'it': 11, 'by': 10, 'executive': 10, 'at': 10, 'published': 9, 'that': 9, 'their': 8, 'ports': 8, 'port': 7, 'employees': 7, 'has': 7, 'seaports': 7, 'more': 7, 'as': 6, 'from': 6, 'are': 6, 'or': 6, 'sex': 6, 'report': 6, 'on': 6, 'pm': 5, 'have': 5, 'how': 5, 'they': 5, 'us': 5, 'training': 5, '10': 5, '2024': 5, 'cargo': 4, 'shipping': 4, 'victims': 4, 'be': 4, 'department': 4, 'this': 4, 'jun': 4, 'ship': 4, 'was': 4, 'pandemic': 3, 'not': 3, 'down': 3, 'its': 3, 'been': 3, 'key': 3, 'helping': 3, 'people': 3, 'labor': 3, 'traffickers': 3, 'work': 3, 'than': 3, 'first': 3, 'transportation': 3, 'new': 3, 'pledge': 3, 'recognize': 3, 'ending': 3, 'when': 3, 'best': 3, 'out': 3, 'monday': 3, 'fire': 3, 'haifa': 3, 'channel': 3, '2020': 2, 'katie': 2, 'amodei': 2, 'never': 2, 'during': 2, 'working': 2, 'theyve': 2, 'shut': 2, 'so': 2, 'industry': 2, 'turned': 2, 'covid19': 2, 'community': 2, 'who': 2, 'forced': 2, 'force': 2, 'into': 2, 'paid': 2, 'just': 2, 'happen': 2, 'well': 2, 'facilities': 2, 'exploited': 2, 'years': 2, 'dot': 2, 'security': 2, 'dhs': 2, 'national': 2, 'against': 2, 'includes': 2, 'signs': 2, 'authorities': 2, 'prevention': 2, 'know': 2, 'spot': 2, 'potential': 2, 'situation': 2, 'with': 2, 'local': 2, 'learn': 2, 'lives': 2, 'nonprofit': 2, 'businesses': 2, 'slavery': 2, 'freedom': 2, 'help': 2, 'about': 2, 'subscriptions': 2, 'subscribe': 2, 'broke': 2, 'june': 2, 'magellan': 2, 'danish': 2, 'finance': 2, 'financing': 2, 'rare': 2, 'move': 2, 'treasury': 2, 'two': 2, 'officers': 2, 'olympics': 2, 'federal': 2, 'fully': 2, 'restored': 2, 'scott': 2, 'oct': 1, '27': 1, '441': 1, 'seaport': 1, 'sleeps': 1, 'vital': 1, 'keeping': 1, 'supply': 1, 'chains': 1, 'moving': 1, 'country': 1, 'fed': 1, 'wellsupplied': 1, 'current': 1, 'continued': 1, 'throughout': 1, 'like': 1, 'many': 1, 'industries': 1, 'another': 1, 'fact': 1, 'much': 1, 'world': 1, 'attention': 1, 'crime': 1, 'able': 1, 'flourish': 1, 'although': 1, 'quietly': 1, 'hidden': 1, 'thrives': 1, 'times': 1, 'chaos': 1, 'vulnerability': 1, 'benefit': 1, 'an': 1, 'improved': 1, 'understanding': 1, 'learning': 1, 'play': 1, 'role': 1, 'vulnerable': 1, 'include': 1, 'children': 1, 'women': 1, 'men': 1, 'subjected': 1, 'sexual': 1, 'exploitation': 1, 'through': 1, 'use': 1, 'fraud': 1, 'coercion': 1, 'particular': 1, 'take': 1, 'advantage': 1, 'discrepancies': 1, 'law': 1, 'order': 1, 'exploit': 1, 'making': 1, 'tremendous': 1, 'profit': 1, 'easier': 1, 'operate': 1, 'sea': 1, 'especially': 1, 'era': 1, 'crewmembers': 1, 'long': 1, 'hours': 1, 'manipulated': 1, 'extending': 1, 'contracts': 1, 'less': 1, 'agreed': 1, 'see': 1, 'wages': 1, 'withheld': 1, 'entirely': 1, 'need': 1, 'watch': 1, 'survivors': 1, 'sailors': 1, 'buyers': 1, 'these': 1, 'instances': 1, 'buying': 1, 'onboard': 1, 'docked': 1, 'ships': 1, 'justice': 1, 'task': 1, 'reports': 1, '83': 1, 'percent': 1, 'citizens': 1, 'average': 1, 'age': 1, 'victim': 1, 'commercial': 1, 'between': 1, '1214': 1, 'old': 1, 'preventing': 1, 'becoming': 1, 'priority': 1, 'homeland': 1, 'year': 1, 'launched': 1, 'program': 1, 'where': 1, 'asked': 1, 'leaders': 1, 'commit': 1, 'end': 1, 'practice': 1, 'signing': 1, 'committing': 1, 'train': 1, 'far': 1, '200': 1, 'groupsincluding': 1, 'airports': 1, 'joined': 1, 'dots': 1, 'january': 1, 'released': 1, 'outlining': 1, 'commitment': 1, 'summarized': 1, 'priorities': 1, 'next': 1, 'five': 1, 'including': 1, 'plans': 1, 'protection': 1, 'prosecution': 1, 'partnershipwhich': 1, 'checkpoints': 1, 'critical': 1, 'checkpoint': 1, 'stopping': 1, 'advocates': 1, 'properly': 1, 'trained': 1, 'increasingly': 1, 'interdependence': 1, 'will': 1, 'serve': 1, 'valuable': 1, 'asset': 1, 'fight': 1, 'traffickingtheir': 1, 'workers': 1, 'receive': 1, 'awareness': 1, 'happening': 1, 'quickly': 1, 'safely': 1, 'allow': 1, 'chance': 1, 'escape': 1, 'rebuild': 1, 'organization': 1, 'created': 1, '30minute': 1, 'videosimulated': 1, 'specifically': 1, 'labeled': 1, 'front': 1, 'line': 1, 'managers': 1, 'indicators': 1, 'proper': 1, 'begin': 1, 'providing': 1, 'save': 1, 'generating': 1, 'eyes': 1, 'ears': 1, 'looking': 1, 'warning': 1, 'communications': 1, 'manager': 1, 'aims': 1, 'expand': 1, 'information': 1, 'visit': 1, 'wwwbestallianceorgmaritimehtml': 1, 'opinions': 1, 'expressed': 1, 'herein': 1, 'authors': 1, 'necessarily': 1, 'those': 1, 'tugs': 1, 'salvage': 1, '1151': 1, 'aboard': 1, 'turkishowned': 1, 'prompting': 1, 'largescale': 1, 'response': 1, 'general': 1, 'yaf': 1, 'horizon': 1, 'arrived': 1, 'russia': 1, '9': 1, 'carrying': 1, 'iron': 1, 'steel': 1, 'vessel': 1, 'moored': 1, 'alongside': 1, 'israel': 1, 'shipyards': 1, 'haifas': 1, 'kishon': 1, 'district': 1, 'board': 1, 'responders': 1, 'kiryot': 1, 'station': 1, 'good': 1, 'samaritan': 1, 'tug': 1, 'business': 1, '951': 1, 'uaebased': 1, 'investment': 1, 'firm': 1, 'group': 1, 'purchased': 1, 'majority': 1, 'stake': 1, 'landmark': 1, 'financial': 1, 'institution': 1, 'based': 1, 'copenhagen': 1, 'price': 1, '750': 1, 'million': 1, 'told': 1, 'bloomberg': 1, 'founded': 1, '1961': 1, 'provide': 1, 'mortgage': 1, 'backing': 1, 'denmarks': 1, 'bank': 1, 'consortium': 1, 'interests': 1, 'originally': 1, 'focused': 1, 'danishbuilt': 1, 'danishowned': 1, 'vessels': 1, 'but': 1, 'shipbuilding': 1, 'moved': 1, 'overseas': 1, 'broadened': 1, 'portfolio': 1, '822': 1, 'targeting': 1, 'mariners': 1, 'added': 1, 'captains': 1, 'list': 1, 'blacklisted': 1, 'iranianlinked': 1, 'oil': 1, 'instance': 1, 'sanctions': 1, 'designation': 1, 'targets': 1, 'licensed': 1, 'rather': 1, 'shipowners': 1, 'charterers': 1, 'capt': 1, 'vivek': 1, 'ashok': 1, 'pandey': 1, 'allegedly': 1, 'served': 1, 'master': 1, 'sanctioned': 1, 'tanker': 1, 'ex': 1, 'name': 1, 'lady': 1, 'sophia': 1, 'according': 1, '729': 1, 'navigation': 1, 'baltimore': 1, 'harbor': 1, 'late': 1, '76': 1, 'days': 1, 'after': 1, 'dali': 1, 'knocked': 1, 'francis': 1, 'bridge': 1, 'army': 1, 'corp': 1, 'engineers': 1, 'had': 1, 'waiting': 1, 'final': 1, 'results': 1, 'underwater': 1, 'surveys': 1, 'before': 1, 'confirming': 1, 'full': 1, '700foot': 1, 'wide': 1, '50footdeep': 1, 'we': 1, 'proud': 1, 'unified': 1, 'efforts': 1, 'reopened': 1, 'operations': 1, 'said': 1, 'lt': 1, 'gen': 1, 'spellmon': 1, 'copyright': 1, 'llc': 1, 'all': 1, 'rights': 1, 'reserved': 1})\n",
      "          word  count\n",
      "0    published      9\n",
      "1           by     10\n",
      "2          the     66\n",
      "3     maritime     13\n",
      "4    executive     10\n",
      "..         ...    ...\n",
      "474  copyright      1\n",
      "475        llc      1\n",
      "476        all      1\n",
      "477     rights      1\n",
      "478   reserved      1\n",
      "\n",
      "[479 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL\n",
    "url = requests.get('https://maritime-executive.com/editorials/preventing-human-trafficking-at-seaports')    \n",
    "#(human trafficking through sea)\n",
    "\n",
    "# Check if the request was successful\n",
    "if url.status_code == 200:\n",
    "    print(\"Successfully fetched the web page.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {url.status_code}\")\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(url.text, \"html.parser\")\n",
    "\n",
    "# Find all <div> tags with class 'entry-content'\n",
    "content = soup.find_all(\"p\")\n",
    "print(f\"Number of content blocks found: {len(content)}\")\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters using regular expression\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Initialize a Counter object to hold the word counts\n",
    "word_counts = Counter()\n",
    "\n",
    "# Iterate through the content and process each paragraph\n",
    "for paragraph in content:\n",
    "    # Extract text from the paragraph\n",
    "    text = paragraph.get_text(separator='\\n')\n",
    "    # Clean the extracted text\n",
    "    text = clean_text(text)\n",
    "    # Split the cleaned text into words\n",
    "    text_word = text.split()\n",
    "    # Update the Counter with words from the current paragraph\n",
    "    word_counts.update(text_word)\n",
    "\n",
    "# Print the word counts\n",
    "print(\"Word counts:\", word_counts)\n",
    "\n",
    "# Convert the word counts to a DataFrame\n",
    "df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3bf7a-1487-4a1a-929b-5ba67adbb212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bceaec97-0a25-4ba0-8d2c-96682ec8fe7d",
   "metadata": {},
   "source": [
    "-POSSIBLE QUERIES TO GET SOME IDEA-\n",
    "\n",
    "General Query: \"Nautical crimes\"\n",
    "\n",
    "Specific Offenses: \"Piracy at sea\", \"Maritime theft\", \"Smuggling on ships\"\n",
    "\n",
    "Historical Cases: \"Famous maritime crimes\", \"Notable pirate attacks\"\n",
    "\n",
    "Legal Perspectives: \"Laws against maritime crime\", \"International maritime law\"\n",
    "\n",
    "Prevention and Enforcement: \"Nautical crime prevention\", \"Coast Guard operations against maritime crime\"\n",
    "\n",
    "Impact and Consequences: \"Economic impact of maritime crime\", \"Environmental consequences of nautical pollution\"\n",
    "\n",
    "Recent News: \"Latest nautical crime incidents\", \"Recent maritime piracy cases\"\n",
    "\n",
    "Statistics and Reports: \"Global maritime crime statistics\", \"Reports on piracy incidents\"\n",
    "\n",
    "Case Studies: \"Case studies of maritime criminal investigations\", \"Court cases related to nautical crimes\"\n",
    "\n",
    "Safety and Security Measures: \"Safety protocols for avoiding nautical crimes\", \"Security measures for ships\"\n",
    "\n",
    "\n",
    "-SEARCH ENGINES-\n",
    "Google, \n",
    "\n",
    "Bing, \n",
    "\n",
    "Yahoo, \n",
    "\n",
    "Baidu (Chinese), \n",
    "\n",
    "Yandex (Russia), \n",
    "\n",
    "DuckDuckGo (prioritizes user privacy and does not track or personalize search results but may have less comprehensive search results and features compared to Google.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc135f-42c7-4301-bbb9-a7909f041407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3a42a-be1b-4c0b-83f3-7c2dc31bc5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
